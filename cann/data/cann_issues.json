[
  {
    "user_login": "xxcc12345",
    "uuid": "issue-gitee-20703157",
    "company": "个人贡献者",
    "company_type": "个人贡献者",
    "internal": "外部",
    "repo_path": "catlass",
    "sig_name": "No-SIG",
    "namespace": "ascend",
    "html_url": "https://gitee.com/ascend/catlass/issues/ICBQNP",
    "title": "在算子工程中使用模板库矩阵计算结果不对",
    "body": "一、问题现象（附报错日志上下文）：\n在 msopgen 算子工程中使用模板库，矩阵计算结果不对\n\n```\nextern \"C\" __global__ __aicore__ void test_catlass(GM_ADDR a, GM_ADDR b, GM_ADDR c, GM_ADDR workspace, GM_ADDR tiling) {\n    GET_TILING_DATA(tiling_data, tiling);\n\n    // printf(\"M: %d\\n\", tiling_data.M);\n    // printf(\"N: %d\\n\", tiling_data.N);\n    // printf(\"K: %d\\n\", tiling_data.K);\n\n    GemmCoord problemShape{tiling_data.M, tiling_data.N, tiling_data.K};\n    \n    using LayoutA = layout::RowMajor;\n    using LayoutB = layout::RowMajor;\n    using LayoutC = layout::RowMajor;\n    LayoutA layoutA{tiling_data.M, tiling_data.K};\n    LayoutB layoutB{tiling_data.K, tiling_data.N};\n    LayoutC layoutC{tiling_data.M, tiling_data.N};\n\n\n    using ArchTag = Arch::AtlasA2;\n    using DispatchPolicy = Gemm::MmadAtlasA2Pingpong<true>;\n    using L1TileShape = GemmShape<64, 64, 64>;\n\n    using L0TileShape = GemmShape<64, 64, 64>;\n\n    using AType = Gemm::GemmType<float, LayoutA>;\n    using BType = Gemm::GemmType<float, LayoutB>;\n    using CType = Gemm::GemmType<float, LayoutC>;\n\n    using BlockMmad = Gemm::Block::BlockMmad<DispatchPolicy, L1TileShape, L0TileShape, AType, BType, CType>;\n    using BlockEpilogue = void;\n\n    // Swizzle offset is 3 and direction is 0.\n    using BlockScheduler = typename Gemm::Block::GemmIdentityBlockSwizzle<3, 0>;\n\n    // kernel level\n    using MatmulKernel = Gemm::Kernel::BasicMatmul<BlockMmad, BlockEpilogue, BlockScheduler>;\n\n    MatmulKernel::Params params{problemShape, a, layoutA, b, layoutB, c, layoutC};\n    MatmulKernel matmulKernel;\n    matmulKernel(params);\n}\n```\n\n\n二、软件版本:\n-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  8.1.RC1\n--Tensorflow/Pytorch/MindSpore 版本:\n--Python 版本 (e.g., Python 3.7.5):  3.10.17\n-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):\n--操作系统版本 (e.g., Ubuntu 18.04):  Ubuntu 22.04.5 LTS\n\n三、测试步骤：\n1. 创建算子工程\n\n```\n[\n    {\n        \"op\": \"TestCatlass\",\n        \n        \"input_desc\": [\n            {\n                \"name\": \"a\",\n                \"param_type\": \"required\",\n                \"format\": [\n                    \"ND\"\n                ],\n                \"type\": [\n                    \"float\"\n                ]\n            },\n            {\n                \"name\": \"b\",\n                \"param_type\": \"required\",\n                \"format\": [\n                    \"ND\"\n                ],\n                \"type\": [\n                    \"float\"\n                ]\n            }\n        ],\n        \"output_desc\": [\n            {\n                \"name\": \"c\",\n                \"param_type\": \"required\",\n                \"format\": [\n                    \"ND\"\n                ],\n                \"type\": [\n                    \"float\"\n                ]\n            }\n        ]\n    }\n]\n\n```\n\n2. host 侧代码\n\n```\n\n#include \"test_catlass_tiling.h\"\n#include \"register/op_def_registry.h\"\n\n\nnamespace optiling {\nstatic ge::graphStatus TilingFunc(gert::TilingContext* context)\n{\n\n  TestCatlassTilingData tiling;\n  const gert::StorageShape* a_shape = context->GetInputShape(0);\n  const gert::StorageShape* b_shape = context->GetInputShape(1);\n    uint32_t M = a_shape->GetStorageShape().GetDim(0);\n    uint32_t K = a_shape->GetStorageShape().GetDim(1);\n    uint32_t N = b_shape->GetStorageShape().GetDim(1);\n\n  tiling.set_M(M);\n  tiling.set_K(K);\n  tiling.set_N(N);\n  context->SetBlockDim(1);\n  tiling.SaveToBuffer(context->GetRawTilingData()->GetData(), context->GetRawTilingData()->GetCapacity());\n  context->GetRawTilingData()->SetDataSize(tiling.GetDataSize());\n\n  return ge::GRAPH_SUCCESS;\n}\n}\n\n\nnamespace ge {\nstatic ge::graphStatus InferShape(gert::InferShapeContext* context)\n{\n    const gert::Shape* x1_shape = context->GetInputShape(0);\n    gert::Shape* y_shape = context->GetOutputShape(0);\n    *y_shape = *x1_shape;\n    return GRAPH_SUCCESS;\n}\nstatic ge::graphStatus InferDataType(gert::InferDataTypeContext *context)\n{\nconst auto inputDataType = context->GetInputDataType(0);\ncontext->SetOutputDataType(0, inputDataType);\nreturn ge::GRAPH_SUCCESS;\n}\n}\n\n\nnamespace ops {\nclass TestCatlass : public OpDef {\npublic:\n    explicit TestCatlass(const char* name) : OpDef(name)\n    {\n        this->Input(\"a\")\n            .ParamType(REQUIRED)\n            .DataType({ge::DT_FLOAT})\n            .Format({ge::FORMAT_ND})\n            .UnknownShapeFormat({ge::FORMAT_ND});\n        this->Input(\"b\")\n            .ParamType(REQUIRED)\n            .DataType({ge::DT_FLOAT})\n            .Format({ge::FORMAT_ND})\n            .UnknownShapeFormat({ge::FORMAT_ND});\n        this->Output(\"c\")\n            .ParamType(REQUIRED)\n            .DataType({ge::DT_FLOAT})\n            .Format({ge::FORMAT_ND})\n            .UnknownShapeFormat({ge::FORMAT_ND});\n\n        this->SetInferShape(ge::InferShape).SetInferDataType(ge::InferDataType);\n\n        this->AICore()\n            .SetTiling(optiling::TilingFunc);\n        this->AICore().AddConfig(\"ascend910b\");\n\n    }\n};\n\nOP_ADD(TestCatlass);\n}\n```\n\n\n3. kernel 侧代码\n\n```\n#include \"kernel_operator.h\"\n\n#include \"catlass/catlass.hpp\"\n#include \"catlass/arch/arch.hpp\"\n#include \"catlass/gemm/block/block_mmad.hpp\"\n#include \"catlass/gemm/block/block_swizzle.hpp\"\n#include \"catlass/gemm/dispatch_policy.hpp\"\n#include \"catlass/gemm/kernel/basic_matmul.hpp\"\n#include \"catlass/gemm/gemm_type.hpp\"\n#include \"catlass/layout/layout.hpp\"\n\n#include \"catlass/status.hpp\"\n\nusing namespace AscendC;\nusing namespace Catlass;\n\nextern \"C\" __global__ __aicore__ void test_catlass(GM_ADDR a, GM_ADDR b, GM_ADDR c, GM_ADDR workspace, GM_ADDR tiling) {\n    GET_TILING_DATA(tiling_data, tiling);\n\n    // printf(\"M: %d\\n\", tiling_data.M);\n    // printf(\"N: %d\\n\", tiling_data.N);\n    // printf(\"K: %d\\n\", tiling_data.K);\n\n    GemmCoord problemShape{tiling_data.M, tiling_data.N, tiling_data.K};\n    \n    using LayoutA = layout::RowMajor;\n    using LayoutB = layout::RowMajor;\n    using LayoutC = layout::RowMajor;\n    LayoutA layoutA{tiling_data.M, tiling_data.K};\n    LayoutB layoutB{tiling_data.K, tiling_data.N};\n    LayoutC layoutC{tiling_data.M, tiling_data.N};\n\n\n    using ArchTag = Arch::AtlasA2;\n    using DispatchPolicy = Gemm::MmadAtlasA2Pingpong<true>;\n    using L1TileShape = GemmShape<64, 64, 64>;\n\n    using L0TileShape = GemmShape<64, 64, 64>;\n\n    using AType = Gemm::GemmType<float, LayoutA>;\n    using BType = Gemm::GemmType<float, LayoutB>;\n    using CType = Gemm::GemmType<float, LayoutC>;\n\n    using BlockMmad = Gemm::Block::BlockMmad<DispatchPolicy, L1TileShape, L0TileShape, AType, BType, CType>;\n    using BlockEpilogue = void;\n\n    // Swizzle offset is 3 and direction is 0.\n    using BlockScheduler = typename Gemm::Block::GemmIdentityBlockSwizzle<3, 0>;\n\n    // kernel level\n    using MatmulKernel = Gemm::Kernel::BasicMatmul<BlockMmad, BlockEpilogue, BlockScheduler>;\n\n    MatmulKernel::Params params{problemShape, a, layoutA, b, layoutB, c, layoutC};\n    MatmulKernel matmulKernel;\n    matmulKernel(params);\n}\n```\n\n4. 通过设置 CPLUS_INCLUDE_PATH=/path/to/catlass 编译安装算子并测试\n\n\n\n四、日志信息:\nxxxx\n请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。\n\n日志提供方式:\n将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。\n\n获取方法请参考wiki：\nhttps://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825",
    "created_at": "2025-05-30 17:25:39",
    "updated_at": "2025-05-30 17:26:33",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "is_pr": null,
    "is_issue": 1,
    "is_comment": null,
    "add": null,
    "remove": null,
    "first_reply_time": 46175,
    "closed_time": null,
    "open_time": 301717,
    "comment_num": null,
    "contrib_type": "issue",
    "issue_type": "训练问题",
    "version": null,
    "role": null,
    "code_platform": "gitee",
    "user_id": "13957648",
    "comment_type": null,
    "branch": null,
    "is_removed": null,
    "tag_url": null,
    "name_zh": null,
    "name_en": null,
    "private": "false",
    "is_invalid_comment": null,
    "is_hide": null
  },
  {
    "user_login": "uniartisan2018",
    "uuid": "issue-gitee-20713964",
    "company": "个人贡献者",
    "company_type": "个人贡献者",
    "internal": "外部",
    "repo_path": "triton-ascend",
    "sig_name": "No-SIG",
    "namespace": "ascend",
    "html_url": "https://gitee.com/ascend/triton-ascend/issues/ICBYZW",
    "title": "请求支持 FLA 的持续集成或测试样例",
    "body": "尊敬的 Ascend/Triton-Ascend 团队：\n\n您好！\n\n我是 [flash-linear-attention (FLA)](https://github.com/fla-org/flash-linear-attention) 项目的成员。\nFLA 旨在为学术界和工业界提供一系列基于 Triton 的、针对目前主流的线性注意力和稀疏注意力模型的高效算子实现，涵盖了训练和推理过程。同时，我们还维护一个名为 [flame](https://github.com/fla-org/flame) 的分布式训练脚本库，以支持这些模型的高效训练。\n\n我们希望能为 FLA 项目申请在昇腾 (Ascend) 硬件上的 CI (持续集成) / 回归测试资源。或者希望能够得到 triton-ascend 在发布时测试 FLA 内核的开箱支持。\n\n一、需求场景&价值\n1.  **昇腾 Triton 编译器健壮性测试**：FLA 包含了大量复杂的 Triton Kernel 实现。通过在昇腾 CI 环境中运行我们的测试用例，可以帮助发现和定位昇腾 Triton 编译器在编译复杂算子时可能存在的 bug 或问题。\n2.  **兼容性验证**：确保 FLA 中实现的各种高效算子能够与昇腾 Triton 编译器良好兼容，从而使广大昇腾 NPU 用户能够无缝使用这些先进的注意力机制。\n3.  **生态贡献**：FLA 的目标是支持更广泛的硬件后端。获得昇腾的官方 CI 支持，将极大地推动 FLA 在昇腾平台上的适配和优化，丰富昇腾 AI 生态。\n**目前遇到的具体问题：**\n\n在尝试将 FLA 适配到昇腾硬件的过程中，我们遇到了关于设备属性查询的问题。具体来说，当我们调用 `triton.runtime.driver.active.utils.get_device_properties(0)` 时，返回的信息如下：\n\n```python\n{'max_shared_mem': 1, 'num_aicore': 24, 'num_vectorcore': 48}\n```\nFLA 中的部分 Kernel 实现依赖于 triton.runtime.driver.active.utils.get_device_properties(tensor_idx)['multiprocessor_count'] 和 max_shared_mem 这两个参数来动态判断和配置 MMA (Matrix Multiply-Accumulate) 操作的矩阵大小以及其他并行化策略。\n\n然而，昇腾 NPU 返回的 max_shared_mem 值为 1，这与我们通常在其他 GPGPU 平台上（如 NVIDIA, AMD, Intel GPU）获取到的实际共享内存大小（通常以 B 为单位）的语义不符。此外，我们未能找到 multiprocessor_count （通常指 SM 流处理器数量）直接对应且行为一致的接口。我个人猜测在大部分时刻可以等价于 num_vectorcore 来实现类似的访问，但我更希望直接有一个键值映射。\n如以下是 Intel N100 核显返回的内容：\n```\ntriton.runtime.driver.active.utils.get_device_properties(0)\n{'max_shared_mem': 65536, 'multiprocessor_count': 2, 'sm_clock_rate': 750, 'mem_clock_rate': 0, 'mem_bus_width': 64, 'max_work_group_size': 512, 'sub_group_sizes': (8, 16, 32)}\n```\n\n我们期望昇腾 Triton 能够提供更接近于通用 GPGPU 硬件特性描述的接口，或者提供明确的指导，说明如何在昇腾硬件上获取用于 Kernel 优化的等效参数。这对于保证 Kernel 的可移植性和性能至关重要。\n\n二、需求建议实现的规格\n\n1. 获得在昇腾硬件上的 CI 资源，以便我们可以定期运行测试，确保 FLA 的兼容性和稳定性。\n2. 如果可能，希望昇腾 Triton 团队能考虑提供与主流 GPGPU 更为一致的设备属性查询接口，或者提供关于如何利用现有接口参数进行高效 Kernel 设计的指导。\n\n三、竞品比较（选填）\n\n再次感谢昇腾编译器团队提供如此好的工具。希望能够得到相关的支持。",
    "created_at": "2025-06-02 22:30:22",
    "updated_at": "2025-06-02 22:42:37",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "is_pr": null,
    "is_issue": 1,
    "is_comment": null,
    "add": null,
    "remove": null,
    "first_reply_time": 24258,
    "closed_time": null,
    "open_time": 24234,
    "comment_num": null,
    "contrib_type": "issue",
    "issue_type": "需求",
    "version": null,
    "role": null,
    "code_platform": "gitee",
    "user_id": "5304420",
    "comment_type": null,
    "branch": null,
    "is_removed": null,
    "tag_url": null,
    "name_zh": null,
    "name_en": null,
    "private": "false",
    "is_invalid_comment": null,
    "is_hide": null
  }
]