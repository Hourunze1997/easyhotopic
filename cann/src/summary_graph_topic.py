import random

import requests
import json
import pandas as pd
from tqdm import tqdm
from openai import OpenAI
import time


def llm_summary(content):
    system_prompt = f"""
    - Role: ä¸“ä¸šçš„æ˜‡è…¾CANNä¸“å®¶å’ŒæŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶
    - Background: ç”¨æˆ·éœ€è¦ä»æä¾›çš„æ–‡æ¡£å†…å®¹ä¸­æç‚¼å‡ºæ˜‡è…¾CANNé¢†åŸŸå¼€å‘è€…å…³å¿ƒæˆ–äºŸéœ€è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼Œè¿™è¡¨æ˜ç”¨æˆ·å¯èƒ½æ­£åœ¨å¤„ç†ä¸æ˜‡è…¾CANNç›¸å…³çš„æŠ€æœ¯æ–‡æ¡£ï¼Œéœ€è¦å¿«é€Ÿå‡†ç¡®åœ°æå–å…³é”®ä¿¡æ¯ä»¥æŒ‡å¯¼å¼€å‘å·¥ä½œã€‚
    - Profile: ä½ æ˜¯ä¸€ä½åœ¨æ˜‡è…¾CANNï¼ˆCompute Architecture for Neural Networksï¼Œç¥ç»ç½‘ç»œè®¡ç®—æ¶æ„ï¼‰é¢†åŸŸæ‹¥æœ‰æ·±åšä¸“ä¸šçŸ¥è¯†çš„ä¸“å®¶ï¼ŒåŒæ—¶ä¹Ÿæ“…é•¿æŠ€æœ¯æ–‡æ¡£çš„åˆ†æå’Œè§£è¯»ï¼Œèƒ½å¤Ÿç²¾å‡†åœ°è¯†åˆ«å‡ºæ–‡æ¡£ä¸­å¯¹å¼€å‘è€…å…·æœ‰é‡è¦æ„ä¹‰çš„é—®é¢˜ã€‚
    - Skills: ä½ å…·å¤‡å¯¹æ˜‡è…¾CANNæŠ€æœ¯çš„å…¨é¢ç†è§£ï¼ŒåŒ…æ‹¬è®¡ç®—æ¶æ„ã€æ¡†æ¶é€‚é…ã€æ€§èƒ½ä¼˜åŒ–ç­‰ï¼Œä»¥åŠå¯¹æŠ€æœ¯æ–‡æ¡£çš„æ·±åº¦åˆ†æèƒ½åŠ›ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä½å…³é”®ä¿¡æ¯å¹¶æç‚¼å‡ºæ ¸å¿ƒé—®é¢˜ã€‚
    - Goals: ä»æä¾›çš„æ–‡æ¡£å†…å®¹ä¸­ç²¾å‡†æç‚¼å‡ºæ˜‡è…¾CANNé¢†åŸŸå¼€å‘è€…å…³å¿ƒæˆ–äºŸéœ€è§£å†³çš„æ ¸å¿ƒé—®é¢˜ã€‚
    - Constrains: é—®é¢˜è¡¨è¿°éœ€èšç„¦å…·ä½“æ“ä½œåœºæ™¯ï¼ŒåŒ…å«æ˜ç¡®æŠ€æœ¯ä¸»ä½“ï¼Œä½“ç°å—ä¼—ï¼Œå­—æ•°æ§åˆ¶åœ¨30ä»¥å†…ï¼Œé‡‡ç”¨é™ˆè¿°å¥å‘ˆç°ï¼Œè¿”å›ç»“æœç”¨<summary>æ ‡ç­¾åŒ…è£¹ã€‚
    - OutputFormat: ä¸€å¥è¯ï¼Œç”¨<summary>æ ‡ç­¾åŒ…è£¹ã€‚
    - Workflow:
    1. ä»”ç»†é˜…è¯»å¹¶ç†è§£æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«ä¸æ˜‡è…¾CANNå¼€å‘ç›¸å…³çš„éƒ¨åˆ†ã€‚
    2. ç¡®å®šæ–‡æ¡£ä¸­æ¶‰åŠçš„å…·ä½“æ“ä½œåœºæ™¯å’Œæ˜ç¡®æŠ€æœ¯ä¸»ä½“ã€‚
    3. æ ¹æ®æ˜‡è…¾CANNå¼€å‘è€…çš„è§†è§’ï¼Œæç‚¼å‡ºäºŸéœ€è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶ç¡®ä¿è¡¨è¿°ç¬¦åˆè¦æ±‚ã€‚
    4. è¿”å›ç»“æœï¼Œä¿è¯åªæœ‰ä¸€ä¸ª<summary>æ ‡ç­¾åŒ…è£¹ï¼Œç»“æœè¦†ç›–å…±æ€§æè¿°ã€‚
    - Examples:
    - ä¾‹å­1ï¼š<summary>å¼€å‘è€…åœ¨å¯»æ‰¾CANNçš„ATBç®—å­çš„å¼€å‘æŒ‡å—</summary>
    - ä¾‹å­2ï¼š<summary>å¼€å‘è€…æ‰¾ä¸åˆ°vLLMæ”¯æŒå“ªäº›LLM</summary>
    - ä¾‹å­3ï¼š<summary>æ˜‡è…¾CANNå¼€å‘è€…å¦‚ä½•ç¡®ä¿æ¨¡å‹ç²¾åº¦å’Œæ€§èƒ½ä¼˜åŒ–</summary>
    """
    start_time = time.time()
    openai_api_key = "sk-xxxxx"
    openai_api_base = "https://api.siliconflow.cn/v1"
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )
    chat_outputs = client.chat.completions.create(
        model="Qwen/Qwen3-235B-A22B",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": content},
        ]
    )
    # print(chat_outputs.choices[0].message.content)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"exec time: {elapsed_time} s")
    return chat_outputs.choices[0].message.content


def merge_keywords(data):
    merged_keywords = {}
    for group_id, keyword_lists in data.items():
        # Flatten the list of lists into a single list of keywords
        all_keywords = [keyword for keyword_list in keyword_lists for keyword in keyword_list]
        # Remove duplicates by converting to a set and back to a list
        unique_keywords = list(set(all_keywords))
        merged_keywords[group_id] = unique_keywords

    return merged_keywords


def process_csv():
    input_file = "../data/topic_graph_doc_2025_1_1.xlsx"
    tpoic_path = "../data/topic_graph_info_2025_1_1.xlsx"
    output_file = "../data/topic_output_merged_new_2025_1_1.xlsx"
    topics = {}
    # hot_keys = {}
    topic_info = pd.read_excel(tpoic_path)
    # for _, row in topic_info.iterrows():
    #     topic = str(row['Topic'])
    #     if topic == '-1':
    #         continue
    # if topic not in topics:
    # hot_keys[topic] = []
    # hot_keys[topic].append((row['Aspect1'], row['Aspect1']))

    # hot_keys = merge_keywords(hot_keys)
    # print(hot_keys)
    df = pd.read_excel(input_file)
    for _, row in df.iterrows():
        topic = str(row['Topic'])
        if topic == '-1':
            continue
        if topic not in topics:
            topics[topic] = []
        topics[topic].append([row['title'], row['processed_body']])

    # for topic in topics:
    #     topics[topic] = [content for _, content in sorted(
    #         topics[topic],
    #         key=lambda x: x[0],
    #         reverse=True
    #     )]
    # print(topics)
    summaries = {}
    with tqdm(topics.items(), desc="ğŸ“Š å¤„ç†è¿›åº¦", unit="topic", bar_format="{l_bar}{bar:20}{r_bar}") as pbar:
        for topic, contents in pbar:
            print("\n")
            print(topic, len(contents))
            min_cluster = 20
            content_titles = ''
            content_block = ""
            for content in contents:
                content_titles += f"{content[0]}\n"
                content_block += f"{content[1]}\n"
                # print(content)
            print(content_titles)
            content = f"""
            ä»¥ä¸‹æ˜¯ç¤¾åŒºå…³äºè¯¥çƒ­ç‚¹è¯é¢˜çš„æ ‡é¢˜å’Œéƒ¨åˆ†å†…å®¹ï¼š
            æ ‡é¢˜ï¼š{content_titles}
            å†…å®¹ï¼š{content_block}
            """
            # print(content)
            llm_summary_result = llm_summary(content)
            print(llm_summary_result)
            if '<summary>' in llm_summary_result:
                summary = llm_summary_result.split('<summary>')[1].split('</summary>')[0].strip()
                summaries[topic] = summary
    df['summary'] = df['Topic'].astype(str).apply(lambda x: summaries.get(x, ''))
    df.to_excel(output_file, index=False)
    topic_info['summary'] = topic_info['Topic'].astype(str).apply(lambda x: summaries.get(x, ''))
    topic_info.to_excel(tpoic_path, index=False)


if __name__ == "__main__":
    process_csv()
