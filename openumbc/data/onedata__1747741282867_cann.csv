"html_url","namespace","repo_path","title","body","state"
"https://gitee.com/ascend/cann_op_contrib/issues/I8TMQI","ascend","cann_op_contrib","自定义算子导出模型加载失败","

![输入图片说明](https://foruda.gitee.com/images/1704356270651923696/06d52301_13697356.png ""7C794DE6-260A-4a52-9DE9-D5E87B0EE045.png"")
![输入图片说明](https://foruda.gitee.com/images/1704356528463642748/9c32d740_13697356.png ""7C794DE6-260A-4a52-9DE9-D5E87B0EE045.png"")

atc --singleop=./single_op.json --output=./op_model --insert_op_conf=./atc_model.cfg --soc_version=Ascend310P3

使用上述ATC指令把算子转换成om模型，在加载的时候会出现500002出错码，加载由prototxt转出的om模型可以正常加载。请问如何解决这个问题。
我参考的链接是https://bbs.huaweicloud.com/blogs/306349","open"
"https://gitee.com/ascend/cann_op_contrib/issues/I8PSU7","ascend","cann_op_contrib","不支持dymatic shape","EZ3002: Optype [Cluster_NMS] of Ops kernel [AIcoreEngine] is unsupported. Reason: The op is dynamic shape, but is not configured to support dynamic shape in op store[tbe-custom]:The op is dynamic shape, but is not configured to support dynamic shape in op store.
![输入图片说明](https://foruda.gitee.com/images/1702999308974814811/a7a46900_13727304.png ""微信图片_20231219232102.png"")","open"
"https://gitee.com/ascend/cann_op_contrib/issues/I7GSY4","ascend","cann_op_contrib","Ascend C目前不支持L1和UB之前的数据搬移","问题提出人：鹏程实验室","open"
"https://gitee.com/ascend/cann_op_contrib/issues/I6MYQK","ascend","cann_op_contrib","【资料】【改进建议】TIK作用域中的代码块，没有表达清楚具体指什么","![输入图片说明](https://foruda.gitee.com/images/1678776092573879179/fea926da_7863637.png ""屏幕截图"")
这里的代码块没有表达清楚具体指什么，函数结束属不属于代码块结束？
实测结果：TIK函数实际和python函数没有实际区别，函数结束不会导致定义的Tensor变量生命周期结束","open"
"https://gitee.com/ascend/cann_op_contrib/issues/I6MTHV","ascend","cann_op_contrib","vec_trans_scatter在repeat_times=1时的异常行为","一、问题现象（附报错日志上下文）：
vec_trans_scatter在处理fp32从(16,8) 到 (8,16) 转置的时候，如果repeat_times参数为立即数1，则目的操作数/源操作数的有效起始位置为dst_list/src_list加上dst_rep_stride/src_rep_stride，如果repeat_times参数为Scalar的1，则不会加上dst_rep_stride/src_rep_stride。

另外，更合理的行为不应该是repeat_times=1，则不看rep_stride吗？

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  CANN 6.0.0 alapha006
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5): 3.7.5
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04): Ubuntu 7.3.0-16ubuntu3

三、测试步骤：

        dst_rep_stride = 16
        src_rep_stride = 16

        dstHighHalf = False
        srcHighHalf = False
        dst_list = [dst_ub[8 * i] for i in range(16)]
        src_list = [src_ub[8 * i] for i in range(16)]
        self.tik_inst.vec_trans_scatter(dstHighHalf, srcHighHalf, dst_list, src_list, repeat_times, dst_rep_stride, src_rep_stride)

上述代码在repeat_times = 立即数1和Scalar1的行为不一致。

四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/metadef/issues/IC8Z1W","ascend","metadef","目前canfuse，ascir中都有metadef里GetDumpGraphPrefix的逻辑，在metadef里需要把这个函数的逻辑抽取出来，作为一个独立函数对外提供","一、需求场景&价值
在canfuse和ascir里目前获取dump目录前缀有相同的代码，代码冗余需要将这部分代码抽取出来，作为公共函数对外提供

二、需求建议实现的规格

三、竞品比较（选填）","open"
"https://gitee.com/ascend/metadef/issues/I55HTF","ascend","metadef","trace Ascend/samples startRun 定义 [属实眼瞎， 你牛逼你自己写啊，你不也是测试嘛，装啥呀]","一、问题现象（附报错日志上下文）：
xxxx

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/catlass/issues/IC8DD4","ascend","catlass","Clarify tensor variable names in MLA epilogue","In [block_epilogue_mla_softmax.hpp](https://gitee.com/ascend/ascendc-templates/blob/master/include/act/epilogue/block/block_epilogue_mla_softmax.hpp) and [block_epilogue_mla_rescal_o.hpp](https://gitee.com/ascend/ascendc-templates/blob/master/include/act/epilogue/block/block_epilogue_mla_rescal_o.hpp) there are a bunch of `UbTensor` named `tv`, `lp`, `ls`, `lm`, `hm`, `gm`, `dm`, `ll`, without any comments:

```cpp
        tvUbTensor16 = resource.ubBuf.template GetBufferByByte<ElementOutput>(LP_UB_TENSOR_OFFSET);
        lpUbTensor32 = resource.ubBuf.template GetBufferByByte<float>(LP_UB_TENSOR_OFFSET);
        lsUbTensor = resource.ubBuf.template GetBufferByByte<float>(LS_UB_TENSOR_OFFSET);
        lmUbTensor = resource.ubBuf.template GetBufferByByte<float>(LM_UB_TENSOR_OFFSET);
        hmUbTensor = resource.ubBuf.template GetBufferByByte<float>(HM_UB_TENSOR_OFFSET);
        gmUbTensor = resource.ubBuf.template GetBufferByByte<float>(GM_UB_TENSOR_OFFSET);
        dmUbTensor = resource.ubBuf.template GetBufferByByte<float>(DM_UB_TENSOR_OFFSET);
        llUbTensor = resource.ubBuf.template GetBufferByByte<float>(LL_UB_TENSOR_OFFSET);
        tvUbTensor = resource.ubBuf.template GetBufferByByte<float>(TV_UB_TENSOR_OFFSET);
```

and also `lo`, `gl`, `go`, ...

```cpp
        loUbTensor = resource.ubBuf.template GetBufferByByte<float>(LO_UB_TENSOR_OFFSET);
        dmUbTensor = resource.ubBuf.template GetBufferByByte<float>(DM_UB_TENSOR_OFFSET);
        llUbTensor = resource.ubBuf.template GetBufferByByte<float>(LL_UB_TENSOR_OFFSET);
        glUbTensor = resource.ubBuf.template GetBufferByByte<float>(GL_UB_TENSOR_OFFSET);
        tvUbTensor = resource.ubBuf.template GetBufferByByte<float>(TV_UB_TENSOR_OFFSET);
        goUbTensor32 = resource.ubBuf.template GetBufferByByte<float>(GO_UB_TENSOR_OFFSET);
        goUbTensor16 = resource.ubBuf.template GetBufferByByte<ElementOutput>(GO_UB_TENSOR_OFFSET);
        hmUbTensor = resource.ubBuf.template GetBufferByByte<float>(HM_UB_TENSOR_OFFSET);
        gmUbTensor = resource.ubBuf.template GetBufferByByte<float>(GM_UB_TENSOR_OFFSET);
```

It will be very helpful to provide the full names of these variables, and how they map to the FlashAttention formula.
","open"
"https://gitee.com/ascend/catlass/issues/IC7VFK","ascend","catlass","在kernel中加入 AscendC::printf(""test\n"");后，执行时会报错","一、问题现象（附报错日志上下文）：
在kernel中加入 AscendC::printf(""test\n"");后，执行时会报错
![输入图片说明](https://foruda.gitee.com/images/1747301230964920895/317cdcf5_11274154.png ""屏幕截图"")
二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
version: 1.0
runtime_running_version=[7.7.T20.0.B210:8.1.RC1]
compiler_running_version=[7.7.T20.0.B210:8.1.RC1]
hccl_running_version=[7.7.T20.0.B210:8.1.RC1]
opp_running_version=[7.7.T20.0.B210:8.1.RC1]
toolkit_running_version=[7.7.T20.0.B210:8.1.RC1]
aoe_running_version=[7.7.T20.0.B210:8.1.RC1]
ncs_running_version=[7.7.T20.0.B210:8.1.RC1]
opp_kernel_running_version=[7.7.T20.0.B210:8.1.RC1]
runtime_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
compiler_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
hccl_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
opp_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
toolkit_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
aoe_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
ncs_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
opp_kernel_upgrade_version=[7.7.T20.0.B210:8.1.RC1]
runtime_installed_version=[7.7.T20.0.B210:8.1.RC1]
compiler_installed_version=[7.7.T20.0.B210:8.1.RC1]
hccl_installed_version=[7.7.T20.0.B210:8.1.RC1]
opp_installed_version=[7.7.T20.0.B210:8.1.RC1]
toolkit_installed_version=[7.7.T20.0.B210:8.1.RC1]
aoe_installed_version=[7.7.T20.0.B210:8.1.RC1]
ncs_installed_version=[7.7.T20.0.B210:8.1.RC1]
opp_kernel_installed_version=[7.7.T20.0.B210:8.1.RC1]
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
在任意kernel的核函数中加入AscendC::printf(""test\n"");后都会报错
![输入图片说明](https://foruda.gitee.com/images/1747280475696665216/9160c1e5_11274154.png ""屏幕截图"")

四、日志信息:
plog部分日志
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.256.624 [stars_engine.cc:1551]557413 ProcLogicCqReport:Task run failed, device_id=0, stream_id=2, task_id=1, sqe_type=0(ffts), errType=0x1(task exception), sqSwStatus=0
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.290.970 [device_error_proc.cc:1434]557413 ProcessStarsCoreErrorInfo:report error module_type=5, module_name=EZ9999
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.290.980 [device_error_proc.cc:1434]557413 ProcessStarsCoreErrorInfo:The error from device(chipId:0, dieId:0), serial number is 1612, there is an fftsplus aivector error exception, core id is 37, error code = 0, dump info: pc start: 0x12c0c00b2dd0, current: 0x12c0c00b39a0, vec error info: 0xe700003cde, mte error info: 0x8306000054, ifu error info: 0x2fffffb500000, ccu error info: 0x2ef0c5276700005d, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c100340080.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.291.101 [device_error_proc.cc:1446]557413 ProcessStarsCoreErrorInfo:report error module_type=5, module_name=EZ9999
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.291.106 [device_error_proc.cc:1446]557413 ProcessStarsCoreErrorInfo:The extend info: errcode:(0, 0x4000, 0) errorStr: CCU instruction address check error. fixp_error0 info: 0x6000054, fixp_error1 info: 0x83, fsmId:0, tslot:2, thread:0, ctxid:0, blk:0, sublk:0, subErrType:4.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.291.156 [davinci_kernel_task.cc:1400]557413 SetStarsResultForDavinciTask:AICORE Kernel task happen error, retCode=0x26.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.291.808 [davinci_kernel_task.cc:1337]557413 PreCheckTaskErr:report error module_type=5, module_name=EZ9999
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.291.814 [davinci_kernel_task.cc:1337]557413 PreCheckTaskErr:Kernel task happen error, retCode=0x26, [aicore exception].
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.753 [davinci_kernel_task.cc:1248]557413 PrintErrorInfoForDavinciTask:Aicore kernel execute failed, device_id=0, stream_id=2, report_stream_id=2, task_id=1, flip_num=0, fault kernel_name=_Z7MLABf16mPhS_S_S_S_S_S_S_S_S_S_S_S_S_, fault kernel info ext=_Z7MLABf16mPhS_S_S_S_S_S_S_S_S_S_S_S_S_, program id=1, hash=9266734828636082699.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.815 [davinci_kernel_task.cc:1186]557413 GetArgsInfo:[AIC_INFO] args(0 to 15) after execute:0xdfffd7271000, 0x12c0c0013000, 0x12c0c0024000, 0x12c041200000, 0x12c0c0027000, 0x12c0c0068000, 0x12c0c0071000, 0x12c0c0072000, 0x12c041600000, 0x12c042400000, 0x12c042c00000, 0x12c043a00000, 0x12c0c0084000, 0x12c0c00a5000, 0x12c0c0083000,  
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.821 [davinci_kernel_task.cc:1189]557413 GetArgsInfo:tilingKey = 0, print 1 Times totalLen=(15*8)Bytes, argsSize=120, blockDim=24
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.830 [davinci_kernel_task.cc:1252]557413 PrintErrorInfoForDavinciTask:[AIC_INFO] after execute:args print end
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.854 [davinci_kernel_task.cc:1152]557413 GetMixCtxInfo:The DavinciTask Mix context-buf[0]=0x00000006.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.858 [davinci_kernel_task.cc:1152]557413 GetMixCtxInfo:The DavinciTask Mix context-buf[1]=0000000000.
[ERROR] RUNTIME(557413,19_mla):2025-05-14-13:19:08.297.862 [davinci_kernel_task.cc:1152]557413 GetMixCtxInfo:The DavinciTask Mix context-buf[2]=0000000000.

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/catlass/issues/IC5N0Y","ascend","catlass","右矩阵转置后计算错误","一、问题现象（附报错日志上下文）：
将右矩阵进行转置操作后进行计算，计算结果与原 torch 不一致，计算错误


二、软件版本:
- CANN 版本:  
CANN 8.1.RC1.alpha001
- Pytorch 版本:
torch                        2.1.0
torch_act                    0.1.0.20250506102930
torch-npu                    2.1.0.post10
- Python 版本:
Python 3.10.16
- 操作系统版本:
容器：Ubuntu 22.04.4 LTS

三、测试步骤：

- 编译 & 安装 whl 包
  bash scripts/build.sh python_extension
  pip install output/python_extension/torch_act-0.1.0.20250506102930-cp310-cp310-linux_aarch64.whl

- 使用以下代码进行测试：
 
```python
import torch_npu
import torch_act
import torch

a = torch.tensor([[1,2,3],[1,2,3]], device=""npu:0"", dtype=torch.float16)
b = torch.tensor([[1,1,1],[2,2,2]], device=""npu:0"", dtype=torch.float16)

org = torch.mm(a, b.T)
act = torch_act.basic_matmul(a, b.T, ""float16"")

print(""org: "", org)
print(""act: "", act)


# 结果：
'''
预期结果：
[[6, 12],
[6, 12]]

测试结果原生torch 计算正确，act计算错误。
org:  tensor([[ 6., 12.],
        [ 6., 12.]], device='npu:0', dtype=torch.float16)
act:  tensor([[ 9., 11.],
        [ 9., 11.]], device='npu:0', dtype=torch.float16)
'''
```


","open"
"https://gitee.com/ascend/catlass/issues/IC4OSO","ascend","catlass","算子使能到模型后性能降低","一、问题现象：
算子使能到模型后,模型性能降低。
- 问题1：当前算子是否在模型中有加速效果。
- 问题2：如何在模型中进行加速（给出的用例的方法是否可取）。
- 问题3：在模型层面 如何 无感 进行算子替换？

性能数据信息：https://gitee.com/ascend-operator/traces/tree/master/


二、软件版本:
- CANN 版本:  
  8.1.RC1.alpha001
- Pytorch版本:
  torch                        2.1.0
  torch_act                    0.1.0.20250428155022
  torch-npu                    2.1.0.post10
- Python 版本:
  Python 3.10.16
- 操作系统版本:
  容器 Ubuntu 22.04.4 LTS

三、测试步骤：
- 编译：bash scripts/build.sh python_extension  ,  安装  pip install   output/python_extension/torch_act-*whl
- 编写测试脚本：

```
import torch_npu
import torch
import torch_act
from torch import nn

M = 27
K = 7168
N = 1536

def mm_liner(A):
    mm_test = nn.Linear(K, N, bias=False).npu()
    mm_test(A)

if __name__ == ""__main__"":
    # 使用 torch.ops.Act...的方式测试时，加载so
    # torch.ops.load_library(""/path/ascendc-templates/output/python_extension/libact_torch.so"")
    # 生成测试数据
    a = torch.randn((M, K)).to(torch.bfloat16).npu()
    
    # 测试
    prof = torch_npu.profiler.profile(
        activities=[
            torch_npu.profiler.ProfilerActivity.CPU,
            torch_npu.profiler.ProfilerActivity.NPU
        ],
        schedule=torch_npu.profiler.schedule(wait=1, warmup=1, active=1, repeat=1, skip_first=1),
        on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(""./result""),
        with_stack=True,
        record_shapes=True,
        profile_memory=False,
        with_modules=False,
        with_flops=False,
    )
    prof.start()
    for step in range(5):
        mm_liner(a)
        prof.step()
    prof.stop()

```
> 测试说明：
>- 1：首先使用原生 torch 接口进行测试，得到性能数据1
>- 2： 修改 lib/python3.10/site-packages/torch/nn/modules/linear.py（约115行） 中 return F.linear(input, self.weight, self.bias) 为 return torch_act.basic_matmul(input, self.weight.T.contiguous(), ""bf16"") 得到性能数据2
>- 3：脚本加载so，修改 linear.py 中 return F.linear(input, self.weight, self.bias) 为 return torch.ops.ActTorch.basic_matmul(input, self.weight.T.contiguous(), ""bf16"") 得到性能数据3


- 结果
   - 原生torch接口性能最佳，torch_act 与 torch.ops.Act.. 性能较差

","open"
"https://gitee.com/ascend/cann-ops/issues/IC8SJJ","ascend","cann-ops","[Bug-Report|缺陷反馈]: [CANN训练营]: IsFinite算子当前在香橙派上不支持，需要补充原型","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：IsFinite
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/is_finite

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试
![输入图片说明](https://foruda.gitee.com/images/1747637924244351553/5e58e5f0_1252277.png ""屏幕截图"")

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747639512424038247/317cd165_1252277.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8RQG","ascend","cann-ops","[Bug-Report|缺陷反馈]: [CANN训练营]: scnrm2算子当前在香橙派上不支持，需要补充原型","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：scnrm2
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/scnrm2

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试
![输入图片说明](https://foruda.gitee.com/images/1747636839005143952/fc675a42_1252277.png ""屏幕截图"")

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747636780072007123/b745e9c0_1252277.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8REU","ascend","cann-ops","[Bug-Report|缺陷反馈]: [CANN训练营]: scopy算子当前在香橙派上不支持，需要补充原型","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：scopy
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/scopy

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试
![输入图片说明](https://foruda.gitee.com/images/1747635747318305669/fc2b9274_1252277.png ""屏幕截图"")

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747635646209728293/c1cf10e3_1252277.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/torchair/issues/IC7IKY","ascend","torchair","cann7.1又支持的torchair版本吗","cann7.1又支持的torchair版本吗","open"
"https://gitee.com/ascend/torchair/issues/IC56X2","ascend","torchair","AttributeError: 'LinearAllreduce' object has no attribute 'hcomm_info'","一、问题现象（附报错日志上下文）：
  根据 https://gitee.com/ascend/torchair/blob/master/npu_tuned_model/llm/llama/README.md 适配修改deepspeed/module_inject/layers.py的LinearAllreduce后执行单算子出错
   ```
   2025-04-30 15:41:18,951 - INFO - [LLM](utils.py:2368): Start to run model in eager(HOST API) mode
Traceback (most recent call last):
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/benchmark/deepspeed/benchmark_llama.py"", line 152, in <module>
    run_llama(args.model_path, **config)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/benchmark/deepspeed/benchmark_llama.py"", line 139, in run_llama
    model_runner.model_generate(_PROMPTS, **kwargs)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/benchmark/deepspeed/benchmark_llama.py"", line 81, in model_generate
    generate_ids = self.model.generate(**kwargs_params)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1541, in generate
    return self.greedy_search(
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2387, in greedy_search
    outputs = self(
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/modeling_llama.py"", line 1322, in forward
    outputs = self.model(
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/modeling_llama.py"", line 1092, in forward
    layer_outputs = decoder_layer(
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/modeling_llama.py"", line 625, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/test/llama149/torchair/npu_tuned_model/llm/llama/modeling_llama.py"", line 523, in forward
    attn_output = self.o_proj(attn_output)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/deepspeed/module_inject/layers.py"", line 39, in forward
    self.hcomm_info)
  File ""/home/ma-user/work/zhongyunde/source/backup/venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1695, in __getattr__
    raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
AttributeError: 'LinearAllreduce' object has no attribute 'hcomm_info'
[ERROR] 2025-04-30-15:41:21 (PID:817589, Device:0, RankID:-1) ERR99999 UNKNOWN application exception
   ```
软件版本: （46，30188） 
```
-- CANN 版本: 8.0.RC3 
-- transformers           4.31.0  --> 适配llame v2
-- Tensorflow/Pytorch/MindSpore 版本: torch 2.1.0/torch-npu 2.1.0  
-- Python 版本 ：Python 3.10.10  （source /home/ma-user/work/zhongyunde/source/backup/venv/bin/activate）
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): NA 
-- 操作系统版本 (e.g., Ubuntu 18.04): Linux version 4.19.90-vhulk2211.3.0.h1543.eulerosv2r10.aarch64 (cat /proc/version)
```

三、测试步骤：执行llama v2 单算子模式
 > deepspeed --num_gpus=1 torchair/npu_tuned_model/llm/llama/benchmark/deepspeed/benchmark_llama.py --model_path=llama-70b_qkv --execute_mode=eager

PS: 撤销相应的修改时能正常执行
![输入图片说明](https://foruda.gitee.com/images/1745999835127936957/dd44cce9_11650004.png ""屏幕截图"")

四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/cann-ops/issues/IC8QPJ","ascend","cann-ops","[Bug-Report|缺陷反馈]: [CANN训练营]: snrm2算子当前在香橙派上不支持，需要补充原型","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：snrm2
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/snrm2

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试
![输入图片说明](https://foruda.gitee.com/images/1747633803178078729/78e042b6_1252277.png ""屏幕截图"")

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747633679708671244/bd16bfa1_1252277.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IWS","ascend","cann-ops","[Bug-Report|缺陷反馈][CANN训练营]: is_inf算子在香橙派上测试泛化性不足，部分shape运行结果精度异常","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：is_inf
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/is_inf

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747479798004266915/fbd92bb2_15838416.png ""微信图片_20250517190257.png"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IUT","ascend","cann-ops","[Bug-Report|缺陷反馈]: isamax算子在香橙派 Ascend310b 大部分样例结果错误 ","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：isamax
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/isamax

大部分样例结果错误，还有部分超时，仅有下面三个样例顺利通过

============test 1=============
1
{'id': 1, 'shape': [1], 'n': 1, 'incx': 1, 'dtype': 'fp32'}

============test 2=============
2
{'id': 2, 'shape': [7], 'n': 7, 'incx': 1, 'dtype': 'fp32'}

============test 11=============
11
{'id': 11, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747479209073860852/8f05bab2_12901623.png ""屏幕截图 2025-05-17 184756.png"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8ITQ","ascend","cann-ops","[Bug-Report|缺陷反馈]: isamin 在Atlas 200I DK A2测试大部分均result error","### Describe the current behavior / 问题描述 (Mandatory / 必填)

============test 0=============
0
{'id': 0, 'shape': [1], 'n': 1, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
test pass

#####################################
INFO: you have passed the Precision!
#####################################
case_0 : 2025-05-17 18:24:51

============test 1=============
1
{'id': 1, 'shape': [7], 'n': 7, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_1 : 2025-05-17 18:25:01

============test 2=============
2
{'id': 2, 'shape': [8], 'n': 8, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_2 : 2025-05-17 18:25:12

============test 3=============
3
{'id': 3, 'shape': [8], 'n': 8, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_3 : 2025-05-17 18:25:23

============test 4=============
4
{'id': 4, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
test pass

#####################################
INFO: you have passed the Precision!
#####################################
case_4 : 2025-05-17 18:25:34

============test 5=============
5
{'id': 5, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_5 : 2025-05-17 18:25:44

============test 6=============
6
{'id': 6, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_6 : 2025-05-17 18:25:55

============test 7=============
7
{'id': 7, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_7 : 2025-05-17 18:26:06

============test 8=============
8
{'id': 8, 'shape': [9], 'n': 9, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_8 : 2025-05-17 18:26:16

============test 9=============
9
{'id': 9, 'shape': [15], 'n': 15, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_9 : 2025-05-17 18:26:27

============test 10=============
10
{'id': 10, 'shape': [15], 'n': 15, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_10 : 2025-05-17 18:26:37

============test 11=============
11
{'id': 11, 'shape': [15], 'n': 15, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
test pass

#####################################
INFO: you have passed the Precision!
#####################################
case_11 : 2025-05-17 18:26:48

============test 12=============
12
{'id': 12, 'shape': [15], 'n': 15, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_12 : 2025-05-17 18:26:58

============test 13=============
13
{'id': 13, 'shape': [15], 'n': 15, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_13 : 2025-05-17 18:27:09

============test 14=============
14
{'id': 14, 'shape': [16], 'n': 16, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_14 : 2025-05-17 18:27:20

============test 15=============
15
{'id': 15, 'shape': [16], 'n': 16, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_15 : 2025-05-17 18:27:30

============test 16=============
16
{'id': 16, 'shape': [16], 'n': 16, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_16 : 2025-05-17 18:27:41

============test 17=============
17
{'id': 17, 'shape': [16], 'n': 16, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_17 : 2025-05-17 18:27:52

============test 18=============
18
{'id': 18, 'shape': [17], 'n': 17, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_18 : 2025-05-17 18:28:02

============test 19=============
19
{'id': 19, 'shape': [17], 'n': 17, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_19 : 2025-05-17 18:28:13

============test 20=============
20
{'id': 20, 'shape': [17], 'n': 17, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_20 : 2025-05-17 18:28:24

============test 21=============
21
{'id': 21, 'shape': [19], 'n': 19, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_21 : 2025-05-17 18:28:34

============test 22=============
22
{'id': 22, 'shape': [19], 'n': 19, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_22 : 2025-05-17 18:28:45

============test 23=============
23
{'id': 23, 'shape': [19], 'n': 19, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_23 : 2025-05-17 18:28:56

============test 24=============
24
{'id': 24, 'shape': [20], 'n': 20, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_24 : 2025-05-17 18:29:06

============test 25=============
25
{'id': 25, 'shape': [20], 'n': 20, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_25 : 2025-05-17 18:29:17

============test 26=============
26
{'id': 26, 'shape': [20], 'n': 20, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_26 : 2025-05-17 18:29:27

============test 27=============
27
{'id': 27, 'shape': [21], 'n': 21, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_27 : 2025-05-17 18:29:38

============test 28=============
28
{'id': 28, 'shape': [21], 'n': 21, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_28 : 2025-05-17 18:29:48

============test 29=============
29
{'id': 29, 'shape': [21], 'n': 21, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_29 : 2025-05-17 18:29:59

============test 30=============
30
{'id': 30, 'shape': [256], 'n': 256, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_30 : 2025-05-17 18:30:09

============test 31=============
31
{'id': 31, 'shape': [256], 'n': 256, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_31 : 2025-05-17 18:30:20

============test 32=============
32
{'id': 32, 'shape': [256], 'n': 256, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_32 : 2025-05-17 18:30:30

============test 33=============
33
{'id': 33, 'shape': [256], 'n': 256, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_33 : 2025-05-17 18:30:41

============test 34=============
34
{'id': 34, 'shape': [257], 'n': 257, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_34 : 2025-05-17 18:30:51

============test 35=============
35
{'id': 35, 'shape': [44980], 'n': 44980, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_35 : 2025-05-17 18:31:02

============test 36=============
36
{'id': 36, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_36 : 2025-05-17 18:31:12

============test 37=============
37
{'id': 37, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_37 : 2025-05-17 18:31:23

============test 38=============
38
{'id': 38, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_38 : 2025-05-17 18:31:34

============test 39=============
39
{'id': 39, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_39 : 2025-05-17 18:31:44

============test 40=============
40
{'id': 40, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_40 : 2025-05-17 18:31:55

============test 41=============
41
{'id': 41, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_41 : 2025-05-17 18:32:05

============test 42=============
42
{'id': 42, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_42 : 2025-05-17 18:32:16

============test 43=============
43
{'id': 43, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_43 : 2025-05-17 18:32:26

============test 44=============
44
{'id': 44, 'shape': [131073], 'n': 131073, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_44 : 2025-05-17 18:32:37

============test 45=============
45
{'id': 45, 'shape': [10142617], 'n': 10142617, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_45 : 2025-05-17 18:32:49

============test 46=============
46
{'id': 46, 'shape': [64328047], 'n': 64328047, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error
case_46 : 2025-05-17 18:33:11

============test 47=============
47
{'id': 47, 'shape': [91789870], 'n': 91789870, 'incx': 1, 'dtype': 'fp32'}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
ERROR: run case 47 timeout !
case_47 2025-05-17 18:33:48

============test 48=============
48
{'id': 48, 'shape': [641830796], 'n': 641830796, 'incx': 1, 'dtype': 'fp32'}
run_all.sh: line 67: 35227 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
run_all.sh: line 67: 35277 Killed                  timeout 15 ./execute_isamin_op $i
Traceback (most recent call last):
  File ""/root/case_50/isamin/AclNNInvocation_test50/verify_result.py"", line 34, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/isamin/AclNNInvocation_test50/verify_result.py"", line 19, in verify_result
    real_result = np.fromfile(real_result, dtype=np.int32) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_48 : 2025-05-17 18:34:15

============test 49=============
49
{'id': 49, 'shape': [2147483648], 'n': 2147483648, 'incx': 1, 'dtype': 'fp32'}
run_all.sh: line 67: 35694 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
run_all.sh: line 67: 35778 Killed                  timeout 15 ./execute_isamin_op $i
Traceback (most recent call last):
  File ""/root/case_50/isamin/AclNNInvocation_test50/verify_result.py"", line 34, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/isamin/AclNNInvocation_test50/verify_result.py"", line 19, in verify_result
    real_result = np.fromfile(real_result, dtype=np.int32) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_49 : 2025-05-17 18:35:57

End Time 2025-05-17 18:35:57

### Environment / 环境信息 (Mandatory / 必填)

Atlas 200I DK A2

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在Atlas 200I DK A2测试；
仅保留src中common以及isamin算子目录，其余算子目录均删除。


### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试应通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747478440163602076/f96ad79c_8397253.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IR8","ascend","cann-ops","[Bug-Report|缺陷反馈]: sasum在香橙派 Ascend310b 大部分样例结果错误","### Describe the current behavior / 问题描述 (Mandatory / 必填)

1.sasum算子添加Ascend910b平台，编译期间报错，将op_kernel/sasum_aiv.h:182中，PIPE_V调整为PIPE_ALL
编译通过。
![输入图片说明](https://foruda.gitee.com/images/1747476819869699553/85910cb7_9064116.png ""屏幕截图"")
2. case50样例测试，
[test pass]:47
![输入图片说明](https://foruda.gitee.com/images/1747477064623774948/7b8fd360_9064116.png ""屏幕截图"")
[result error]:0-45
![输入图片说明](https://foruda.gitee.com/images/1747477048914771081/009d565e_9064116.png ""屏幕截图"")
[run case timeout]:46,48
![输入图片说明](https://foruda.gitee.com/images/1747477077271513650/f12139d3_9064116.png ""屏幕截图"")
[generate input failed]:49
![输入图片说明](https://foruda.gitee.com/images/1747477090462585713/31541eb4_9064116.png ""屏幕截图"")




### Environment / 环境信息 (Mandatory / 必填)

os:Ubuntu 22.04.3 LTS
Ascend310b

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

1. cann-ops-master/src/math/sasum/op_host/sasum.cpp 142:增加AddConfig(""ascend310b"")
2. cann-ops-master/下：bash build.sh -n sasum
3. cann-ops-master/build-out/下：./CANN-custom_ops--linux.aarch64.run
4. case_50/sasum/AclNNInvocation_test50下：./run_all.sh

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

1. 算子编译通过
2. 算子部署成功
3. case50全部通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747477421038236278/e4e2757f_9064116.png ""屏幕截图"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IQX","ascend","cann-ops","[Bug-Report|缺陷反馈]: sscal在Atlas 200I DK A2测试泛化性不足，部分shape运行结果精度异常","### Describe the current behavior / 问题描述 (Mandatory / 必填)

Describe the current behavior / 问题描述 (Mandatory / 必填)
sscal算子报错为：/home/trybest/cann-ops/build/binary/ascend310b/src/sscal/sscal.cpp:165:18: error: the ranges of 1st parameter must be [2,6],[10, 10]: pipe barrier(PIPE V),
1 error generated



0-42样例通过测试
-----------------------------下面未通过测试---------------------------------------
============test 43=============
43
{'id': 43, 'shape': [131073, 17, 1, 1, 20], 'alpha': 6.0, 'n': 131073, 'incx': 1, 'dtype': 'fp32', 'inputSize': 44564820}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
ERROR: run case 43 timeout !
case_43 2025-05-17 17:56:44

============test 44=============
44
{'id': 44, 'shape': [21, 1, 19, 255, 1, 9, 8, 9], 'alpha': 9.0, 'n': 21, 'incx': 1, 'dtype': 'fp32', 'inputSize': 65930760}
INFO: generate input data success!
[INFO]  Set input success
ERROR: run case 44 timeout !
case_44 2025-05-17 17:57:23

============test 45=============
45
{'id': 45, 'shape': [15, 9, 255, 21, 17, 8], 'alpha': 1.0, 'n': 15, 'incx': 1, 'dtype': 'fp32', 'inputSize': 98317800}
INFO: generate input data success!
[INFO]  Set input success
ERROR: run case 45 timeout !
case_45 2025-05-17 17:58:15

============test 46=============
46
{'id': 46, 'shape': [1, 7, 15, 16, 17, 15, 20, 16], 'alpha': 7.0, 'n': 1, 'incx': 1, 'dtype': 'fp32', 'inputSize': 137088000}
run_all.sh: line 67: 190321 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 190560 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_46 : 2025-05-17 17:58:38

============test 47=============
47
{'id': 47, 'shape': [7, 8, 131073, 19], 'alpha': 7.0, 'n': 7, 'incx': 1, 'dtype': 'fp32', 'inputSize': 139461672}
run_all.sh: line 67: 190577 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 190977 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_47 : 2025-05-17 17:59:01

============test 48=============
48
{'id': 48, 'shape': [9, 17, 21, 20, 19, 15, 1, 8], 'alpha': 2.0, 'n': 9, 'incx': 1, 'dtype': 'fp32', 'inputSize': 146512800}
run_all.sh: line 67: 190994 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 191397 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_48 : 2025-05-17 17:59:26

============test 49=============
49
{'id': 49, 'shape': [15, 15, 19, 20, 8, 17, 19, 1], 'alpha': 6.0, 'n': 15, 'incx': 1, 'dtype': 'fp32', 'inputSize': 220932000}
run_all.sh: line 67: 191613 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 191619 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_49 : 2025-05-17 17:59:52

End Time 2025-05-17 17:59:52


### Environment / 环境信息 (Mandatory / 必填)

Atlas 200I DK A2

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在Atlas 200I DK A2测试；
仅保留src中common以及sscal算子目录，其余算子目录均删除。
因为编译报错，后续测试结果将PIPE_V修改为PIPE_ALL，重新编译运行

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

case全部通过测试

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747477157970863671/c58bd27b_13572081.png ""屏幕截图 2025-05-17 181848.png"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IQV","ascend","cann-ops","[Bug-Report|缺陷反馈]: sscal在Atlas 200I DK A2测试泛化性不足，部分shape运行结果精度异常","### Describe the current behavior / 问题描述 (Mandatory / 必填)

Describe the current behavior / 问题描述 (Mandatory / 必填)
sscal算子报错为：/home/trybest/cann-ops/build/binary/ascend310b/src/sscal/sscal.cpp:165:18: error: the ranges of 1st parameter must be [2,6],[10, 10]: pipe barrier(PIPE V),
1 error generated



0-42样例通过测试
-----------------------------下面未通过测试---------------------------------------
============test 43=============
43
{'id': 43, 'shape': [131073, 17, 1, 1, 20], 'alpha': 6.0, 'n': 131073, 'incx': 1, 'dtype': 'fp32', 'inputSize': 44564820}
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
ERROR: run case 43 timeout !
case_43 2025-05-17 17:56:44

============test 44=============
44
{'id': 44, 'shape': [21, 1, 19, 255, 1, 9, 8, 9], 'alpha': 9.0, 'n': 21, 'incx': 1, 'dtype': 'fp32', 'inputSize': 65930760}
INFO: generate input data success!
[INFO]  Set input success
ERROR: run case 44 timeout !
case_44 2025-05-17 17:57:23

============test 45=============
45
{'id': 45, 'shape': [15, 9, 255, 21, 17, 8], 'alpha': 1.0, 'n': 15, 'incx': 1, 'dtype': 'fp32', 'inputSize': 98317800}
INFO: generate input data success!
[INFO]  Set input success
ERROR: run case 45 timeout !
case_45 2025-05-17 17:58:15

============test 46=============
46
{'id': 46, 'shape': [1, 7, 15, 16, 17, 15, 20, 16], 'alpha': 7.0, 'n': 1, 'incx': 1, 'dtype': 'fp32', 'inputSize': 137088000}
run_all.sh: line 67: 190321 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 190560 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_46 : 2025-05-17 17:58:38

============test 47=============
47
{'id': 47, 'shape': [7, 8, 131073, 19], 'alpha': 7.0, 'n': 7, 'incx': 1, 'dtype': 'fp32', 'inputSize': 139461672}
run_all.sh: line 67: 190577 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 190977 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_47 : 2025-05-17 17:59:01

============test 48=============
48
{'id': 48, 'shape': [9, 17, 21, 20, 19, 15, 1, 8], 'alpha': 2.0, 'n': 9, 'incx': 1, 'dtype': 'fp32', 'inputSize': 146512800}
run_all.sh: line 67: 190994 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 191397 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_48 : 2025-05-17 17:59:26

============test 49=============
49
{'id': 49, 'shape': [15, 15, 19, 20, 8, 17, 19, 1], 'alpha': 6.0, 'n': 15, 'incx': 1, 'dtype': 'fp32', 'inputSize': 220932000}
run_all.sh: line 67: 191613 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 191619 Killed                  timeout 15 ./execute_sscal_op $i
Traceback (most recent call last):
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/sscal/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_49 : 2025-05-17 17:59:52

End Time 2025-05-17 17:59:52


### Environment / 环境信息 (Mandatory / 必填)

Atlas 200I DK A2

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在Atlas 200I DK A2测试；
仅保留src中common以及sscal算子目录，其余算子目录均删除。
因为编译报错，后续测试结果将PIPE_V修改为PIPE_ALL，重新编译运行

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

case全部通过测试

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![输入图片说明](https://foruda.gitee.com/images/1747477157970863671/c58bd27b_13572081.png ""屏幕截图 2025-05-17 181848.png"")

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC8IJC","ascend","cann-ops","[Bug-Report|缺陷反馈]: scasum在Atlas 200I DK A2测试泛化性不足，部分shape运行结果精度异常","### Describe the current behavior / 问题描述 (Mandatory / 必填)

1. （在仅保留src中common以及math下scasum算子的前提下），算子编译报错：
/home/trybest/cann-ops/build/binary/ascend310b/src/scasum/scasum_aiv.h:165:18: error: the ranges of 1st parameter must be [2,6],[10, 10]: pipe barrier(PIPE V),
1 error generated.

2. (以下结果均为将PIPE_V修改为PIPE_ALL的前提下）：
test 17，41，42，43，44，45，46，47通过（且通过精度测试），
test 48，49报错信息如下：
============test 48=============
48
{'id': 48, 'shape': [64328047], 'n': 64328047, 'incx': 1, 'dtype': 'complex64'}
run_all.sh: line 67: 111280 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 111287 Killed                  timeout 15 ./execute_scasum_op $i
Traceback (most recent call last):
  File ""/root/case_50/scasum/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/scasum/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_48 : 2025-05-17 17:32:54

============test 49=============
49
{'id': 49, 'shape': [91789870], 'n': 91789870, 'incx': 1, 'dtype': 'complex64'}
run_all.sh: line 67: 111686 Killed                  python3 gen_data.py $i
ERROR: generate input data failed!
run_all.sh: line 44: return: can only `return' from a function or sourced script
INFO: generate input data success!
[ERROR]  failed to get file ../input/input_x.bin
[INFO]  Set input success
run_all.sh: line 67: 111693 Killed                  timeout 15 ./execute_scasum_op $i
Traceback (most recent call last):
  File ""/root/case_50/scasum/AclNNInvocation_test50/verify_result.py"", line 35, in <module>
    verify_result(sys.argv[1],sys.argv[2])
  File ""/root/case_50/scasum/AclNNInvocation_test50/verify_result.py"", line 20, in verify_result
    real_result = np.fromfile(real_result, dtype=dtype) # 从bin文件读取实际运算结果
FileNotFoundError: [Errno 2] No such file or directory: 'output/output_z.bin'

case_49 : 2025-05-17 17:33:25

其余test均报错：
INFO: generate input data success!
[INFO]  Set input success
[INFO]  Write output success
[ERROR] result error

### Environment / 环境信息 (Mandatory / 必填)

Atlas 200I DK A2

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

1. 算子增加.AddConfig后在Atlas 200I DK A2测试；
2. 仅保留src中common以及math下scasum算子目录，其余算子目录均删除。
3. 因为编译报错，后续测试结果将PIPE_V修改为PIPE_ALL，重新编译运行。

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

![编译报错](https://foruda.gitee.com/images/1747474728576915324/2f913f3c_11921186.png ""1.png"")

![大部分样例报错](https://foruda.gitee.com/images/1747474793340722631/e68f8ca5_11921186.png ""2.png"")

![测试结果与最后两个样例报错](https://foruda.gitee.com/images/1747474826171104412/64f153af_11921186.png ""3.png"")


### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops-adv/issues/IBVC86","ascend","cann-ops-adv","[Bug-Report|缺陷反馈]: 调ffnV3得到的output会出现inf、-inf","### Describe the current behavior / 问题描述 (Mandatory / 必填)

在特定的输入数据下，调ffnV3得到的output会出现inf、-inf

### Environment / 环境信息 (Mandatory / 必填)

按文档

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

按照doc编译调用步骤

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

预期结果不出现inf

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

无

### Special notes for this issue/备注 (Optional / 选填)



### 所属算子

FFN


","open"
"https://gitee.com/ascend/cann-ops/issues/IC8BFR","ascend","cann-ops","[Bug-Report|缺陷反馈][CANN训练营]: isamin算子在香橙派上测试泛化性不足，部分shape运行结果精度异常","### Describe the current behavior / 问题描述 (Mandatory / 必填)

算子名称：isamin
算子链接：https://gitee.com/ascend/cann-ops/tree/master/src/math/isamin
详细错误：
1. 社区任务XXX中caseX，输入x为XXXshape，输入y为XXXshape下期望运行结果为X，实际结果为X
2. 社区任务XXX中caseX，输入x为XXXshape，输入y为XXXshape下期望运行结果为X，实际结果为X
...

### Environment / 环境信息 (Mandatory / 必填)

香橙派AIPro

### Steps to reproduce the issue / 重现步骤 (Mandatory / 必填)

算子增加.AddConfig后在香橙派AIPro测试

### Describe the expected behavior / 预期结果 (Mandatory / 必填)

测试case均通过

### Related log / screenshot / 日志 / 截图 (Mandatory / 必填)

无

### Special notes for this issue/备注 (Optional / 选填)



","open"
"https://gitee.com/ascend/cann-ops/issues/IC5ZWC","ascend","cann-ops","[Requirement|需求建议]:  QuantBatchMatmulV3性能优化","### Backgroud（背景信息）

一下场景为 deepseek-r1 模型：

该场景下，cube利用率较低，需要优化：

aclnnQuantMatmulV4_QuantBatchMatmulV3_QuantBatchMatmulV3,
Input Shapes： ""1,7168;256,7168;256;1"" INT8;INT8;FLOAT;FLOAT
Output Shape：""1,256"" FLOAT16
cube_utilization(%)：13.6%

该场景下，cube利用率较高，

aclnnQuantMatmulV4_QuantBatchMatmulV3_QuantBatchMatmulV3,
Input Shapes： ""1,128;7168,128;7168;1"" INT8;INT8;FLOAT;FLOAT
Output Shape：""1,7168"" FLOAT16
cube_utilization(%)：75%

### Origin（信息来源）

tele

### Benefit / Necessity （价值/作用）

提升 deepseek-r1 模型运行效率

### Design（设计方案）

争对性优化 Input Shapes： ""1,7168;256,7168;256;1"" 输入场景下的性能

","open"
"https://gitee.com/ascend/cann-ops/issues/IC4LXE","ascend","cann-ops","[Question|问题咨询]: cann-ops是否能够像cutlass或者flashinfer一样当库在生产环境集成使用吗？","### 问题描述

请问下cann-ops是否能够像nvidai cutlass或者flashinfer一样，被上层库（比如vLLM、transformers等）[调用](https://github.com/flashinfer-ai/flashinfer?tab=readme-ov-file#trying-it-out)或者[集成](https://github.com/flashinfer-ai/flashinfer?tab=readme-ov-file#adoption)吗？

还是仅是一个AscendC实现的算子代码参考？

[1] https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/quickstart.md#launching-a-gemm-kernel-in-cuda
[2] https://github.com/flashinfer-ai/flashinfer?tab=readme-ov-file#trying-it-out

","open"
"https://gitee.com/ascend/cann-ops/issues/IBOX5E","ascend","cann-ops","CANN训练营实操挑战任务发放总表，做任务赢大奖，建议收藏页面！","

### 活动介绍

CANN训练营实操挑战活动通过学练结合的方式，帮助不同阶段的开发者快速提升Ascend C算子开发能力。任务与大奖按批次持续更新！

### 奖品与获奖规则

 **获奖规则：** 赢取任务积分，获得对应奖品

 **奖品图示：** 活动奖品以实际发放为准，活动解释权归活动方所有


![输入图片说明](https://foruda.gitee.com/images/1747318774954276907/a9ee0378_15726163.jpeg ""实操挑战奖品heji2 拷贝.jpg"")


**【任务详情】：**

| 序号 | 积分 | 链接 | 状态 | 完成队伍  | 奖品 |
|---|---|---|---|---|---|
|  1  | 100  | [【1】ClipByValue迁移](https://docs.qq.com/doc/DUmxXSE5TYmpoaWpD?u=e67d65d6ee404b97bf06f241574824cc)  | 已关闭  | enkilee  | 已关闭 |
|  2 | 100  | [【2】group_matmul迁移](https://docs.qq.com/doc/DUk9ma2NmYm1PaXZn?u=e67d65d6ee404b97bf06f241574824cc)  | 已关闭  | 已关闭  | 已关闭  |
| 3  | 100  |  [【3】matmul_all_reduce迁移](https://docs.qq.com/doc/DUnNjWkNEeEtaeVdY?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 4  | 100  | [【4】matmul_reduce_scatter迁移](https://docs.qq.com/doc/DUnVnTWhPU2VPZHpr?u=e67d65d6ee404b97bf06f241574824cc)  | 已关闭  | 石榴队  |   CANN周边POLO衫 |
|  5  | 100  | [【5】all_gather_matmul迁移](https://docs.qq.com/doc/DUmVnT3VMVFFmUHJO?u=e67d65d6ee404b97bf06f241574824cc)  | 已关闭  | 已关闭 | 已关闭  |
|  6 |  100 | [【6】cube_group_barrier迁移](https://docs.qq.com/doc/DUmJyUGFIcmREenRr?u=e67d65d6ee404b97bf06f241574824cc)  |已关闭   |已关闭   | 已关闭  |
| 7   | 100  |[【7】matmul_api_constant迁移](https://docs.qq.com/doc/DUkZkUkttSnR6TE5E?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |infinity   |已关闭 |
| 8   | 100  | [【8】MatmulABshareInvocation迁移](https://docs.qq.com/doc/DUmxXVnJuc2ZhYktr?u=e67d65d6ee404b97bf06f241574824cc) |   |   |  CANN周边POLO衫  |
|  9 | 100  |  [【9】cube_group迁移](https://docs.qq.com/doc/DUm1xT2RIaWJZR0hs?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | 已关闭  | 已关闭  |
| 10 | 100  |  [【10】mmad_kernellaunch迁移](https://docs.qq.com/doc/DUkhCZExvRHdPR21q?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
|11|100|[【11】unaligned_wholereduces_frameworklaunch迁移](https://docs.qq.com/doc/DUmNid0VNTFhDeU1X?u=e67d65d6ee404b97bf06f241574824cc)|||CANN周边POLO衫|
| 12  |100   | [【12】unaligned_reducemin_kernellaunch迁移](https://docs.qq.com/doc/DUk9zVHJCRVRVeHRX?u=e67d65d6ee404b97bf06f241574824cc)  |   |   | CANN周边POLO衫  |
| 13 |  100 |  [【13】unaligned_abs_kernellaunch迁移](https://docs.qq.com/doc/DUkhMc0VZbFRhQ3do?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 14 | 100  | [【14】sub_frameworklaunch迁移 ](https://docs.qq.com/doc/DUnhDZmFOamtyZXdJ?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 15 | 100  | [【15】reduce_frameworklaunch迁移 ](https://docs.qq.com/doc/DUmR3a09JUUh4b2Ni?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 16 | 100  |[【16】leakyrelu_frameworklaunch迁移 ](https://docs.qq.com/doc/DUnh2TEd4dkNIWmtp?u=e67d65d6ee404b97bf06f241574824cc)  |  |   | CANN周边POLO衫   |
| 17  | 100  |[【17】addtemplate_frameworklaunch迁移 ](https://docs.qq.com/doc/DUmh1WWVSS3lCcEZS?u=e67d65d6ee404b97bf06f241574824cc)  |   |   |CANN周边POLO衫   |
| 18  |  100 | [【18】broadcast_frameworklaunch迁移](https://docs.qq.com/doc/DUnhMT3VodFVrR1dq?u=e67d65d6ee404b97bf06f241574824cc)  |   |   | CANN周边POLO衫  |
| 19 |  100 | [【19】addn_frameworklaunch迁移](https://docs.qq.com/doc/DUkV5Tm9ieHdZTmRz?u=e67d65d6ee404b97bf06f241574824cc)  |   |  | CANN周边POLO衫  |
| 20  |  100 | [【20】MatmulLeakyRelu迁移](https://docs.qq.com/doc/DUml2eFBOcGJRQ1Vo?u=e67d65d6ee404b97bf06f241574824cc)  |   |   | CANN周边POLO衫  |
| 21 | 100 | [【21】Matmul迁移](https://docs.qq.com/doc/DUkhubWVWV0RaT2Zw?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | 已关闭  | 已关闭  |
| 22 | 100 | [【22】Xlogy迁移](https://docs.qq.com/doc/DUnNuR1dWY3ZISGJL?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 23 | 100 | [【23】UnalignAdd迁移](https://docs.qq.com/doc/DUnNwT1pLQmFIUE1G?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | Jerry | 已关闭 |
| 24 | 100 | [【24】Triu迁移](https://docs.qq.com/doc/DUm5ncE9xZ2tBV2Rp?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | enkilee  | 已关闭  |
| 25 | 100 | [【25】Tril迁移](https://docs.qq.com/doc/DUm9TZFdQTFFUTWFM?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |  enkilee | 已关闭  |
| 26 | 100 | [【26】Spence迁移](https://docs.qq.com/doc/DUnFwd0t2eVFzdXNS?u=e67d65d6ee404b97bf06f241574824cc)| 已关闭   | 取个队名 | 已关闭 |
| 27 | 100 | [【27】ScatterMax迁移](https://docs.qq.com/doc/DUnJubXhGcUN6R2FL?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 28 | 100 | [【28】QuantMatmul迁移](https://docs.qq.com/doc/DUktlQXVpeURwaEd2?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | 已关闭  | 已关闭  |
| 29 | 100 | [【29】PreLayerNorm迁移](https://docs.qq.com/doc/DUmlhV2FzZ2VLUmh5?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | yuuki   |  已关闭   |
| 30 | 100 | [【30】MseLoss迁移](https://docs.qq.com/doc/DUmVqWHZXUFNpc05E?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭   |enkilee   |已关闭  |
| 31 | 100 | [【31】MseLossGrad迁移](https://docs.qq.com/doc/DUkh0dEJkcmJxckZE?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | enkilee  |已关闭 |
| 32 | 100 | [【32】MoeSoftMaxTopk迁移](https://docs.qq.com/doc/DUmhqcFRxeEdHbVN4?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |yuuki      |已关闭  |
| 33 | 100 | [【33】LpNormV2迁移](https://docs.qq.com/doc/DUkp1U0NjeHR3ZHRt?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 34 | 100 | [【34】LessEqual迁移](https://docs.qq.com/doc/DUmdjck5uTGFPWmRk?u=e67d65d6ee404b97bf06f241574824cc) |   |  | CANN周边POLO衫  |
| 35  | 100 | [【35】Lerp迁移](https://docs.qq.com/doc/DUnpQbHhtcGh1TGRn?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | enkilee   |已关闭  |
| 36 | 100 | [【36】LayerNorm迁移](https://docs.qq.com/doc/DUk16VXRnT2tCeHBG?u=e67d65d6ee404b97bf06f241574824cc) |   |  | CANN周边POLO衫  |
| 37 | 100 | [【37】InstanceNorm迁移](https://docs.qq.com/doc/DUkVCQ0pGc2VEVnNz?u=e67d65d6ee404b97bf06f241574824cc) |   |  | CANN周边POLO衫  |
| 38  | 100 | [【38】GreaterEqual迁移](https://docs.qq.com/doc/DUmh0ZUhhckFUZERy?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 39  | 100 | [【39】GlobalAvgPool迁移](https://docs.qq.com/doc/DUmFrekxVRmZzWmpN?u=e67d65d6ee404b97bf06f241574824cc) |   |   | CANN周边POLO衫  |
| 40 | 100 |  [【40】Gelu迁移 ](https://docs.qq.com/doc/DUnphYWpsbmd5ZmFQ?u=e67d65d6ee404b97bf06f241574824cc)| 已关闭  |  Jerry | 已关闭  |
| 41 |  100 |  [【41】FastGelu迁移](https://docs.qq.com/doc/DUnRDSFZ5YXp2UVBv?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |  enkilee | 已关闭   |
| 42  | 100 | [【42】FastGeluGrad迁移](https://docs.qq.com/doc/DUmpHamV4SWpNVXNE?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |  enkilee  |已关闭   |
| 43 | 100 | [【43】Eye迁移](https://docs.qq.com/doc/DUmpVeUFYbHVvZWFD?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | 无论我爱你有多深  | 已关闭  |
| 44 | 100 | [【44】DepthToSpace迁移](https://docs.qq.com/doc/DUnVRdXJ2QW1wQUtn?u=e67d65d6ee404b97bf06f241574824cc) |   |    | CANN周边POLO衫  |
| 45 | 100 | [【45】Dawsn迁移](https://docs.qq.com/doc/DUndDWXRXZ2NuQVRu?u=e67d65d6ee404b97bf06f241574824cc) |   |   |CANN周边POLO衫   |
| 46  | 100 | [【46】Cross迁移](https://docs.qq.com/doc/DUnJsZnRTVGN3cnZu?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | enkilee  |  已关闭  |
| 47 | 100 | [【47】ScatterSub迁移](https://docs.qq.com/doc/DUm9KdmFyS1pqVU51?u=e67d65d6ee404b97bf06f241574824cc) |  |   | CANN周边POLO衫  |
| 48  | 100 | [【48】BallQuery迁移](https://docs.qq.com/doc/DUnFuVkpxdWFua1N0?u=e67d65d6ee404b97bf06f241574824cc)|   |   |  CANN周边POLO衫 |
| 49 | 100 | [【49】Addcmul迁移](https://docs.qq.com/doc/DUmZ6THJwWWNKakJ6?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  |  摸鱼队 | 已关闭  |
| 50 | 100 | [【50】Addcdiv迁移](https://docs.qq.com/doc/DUmJHcnJvQmJpdXZW?u=e67d65d6ee404b97bf06f241574824cc) | 已关闭  | 摸鱼队  | 已关闭 |
| 51 | / |[【51】Abs算子开发任务](https://docs.qq.com/doc/DUmFTSkdrd3ZaWXVr) | 已关闭  | 第一名：NoOne，第二名：有人喊我看虎哥，第三名：摸鱼队|第1名：香橙派OrangePi昇腾AIPro开发板20Tops算力可运行DeepSeekR1蒸馏模型 AIpro 24G（20T）主板+风扇+外壳+电源；如已有开发板可选择HUAWEI WATCH GT 5 Pro 46mm 曜石黑 黑色氟橡胶表带；第2-4名：HUAWEI FreeBuds 6i 有线充 幻夜黑  |
| 52 | / |[【52】BevPool算子开发任务](http://docs.qq.com/doc/DUmtHR3FVTXFvRFFr) |已关闭  |   第一名：摸鱼队 |第1名：香橙派OrangePi昇腾AIPro开发板20Tops算力可运行DeepSeekR1蒸馏模型 AIpro 24G（20T）主板+风扇+外壳+电源；如已有开发板可选择HUAWEI WATCH GT 5 Pro 46mm 曜石黑 黑色氟橡胶表带 |
| 53 | / |[【53】Radius算子开发任务](http://docs.qq.com/doc/DUkNoWk9iTFNrVkxJ) | 已关闭  | 第一名：NoOne，第二名：摸鱼队  | 第1名：香橙派OrangePi昇腾AIPro开发板20Tops算力可运行DeepSeekR1蒸馏模型 AIpro 24G（20T）主板+风扇+外壳+电源；如已有开发板可选择HUAWEI WATCH GT 5 Pro 46mm 曜石黑 黑色氟橡胶表带  |
| 54 | 50|[【54】基于CANN-Ops开放仓库完成Sscal算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Sscal%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   | 定制奖品 |
| 55 | 50|[【55】基于CANN-Ops开放仓库完成Snrm2算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Snrm2%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) |   |   | 定制奖品|
| 56 | 50|[【56】基于CANN-Ops开放仓库完成Scopy算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Scopy%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) |   |   | 定制奖品|
| 57 | 50|[【57】基于CANN-Ops开放仓库完成Scnrm2算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Scnrm2%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) |   |   |定制奖品 |
| 58 | 50|[【58】基于CANN-Ops开放仓库完成Scasum算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Scasum%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   | 定制奖品 |
| 59 | 50|[【59】基于CANN-Ops开放仓库完成Sasum算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Sasum%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   | 定制奖品 |
| 60 | 50|[【60】基于CANN-Ops开放仓库完成Isamin算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Isamin%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   | 定制奖品 |
| 61 | 50|[【61】基于CANN-Ops开放仓库完成Isamax算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90Isamax%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   |定制奖品 |
| 62 | 50|[【62】基于CANN-Ops开放仓库完成IsInf算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90IsInf%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) | 已关闭  |   | 定制奖品 |
| 63 | 50|[【63】基于CANN-Ops开放仓库完成IsFinite算子的端侧能力补齐](https://public-download.obs.cn-east-2.myhuaweicloud.com/task/%E5%9F%BA%E4%BA%8ECANN-Ops%E5%BC%80%E6%94%BE%E4%BB%93%E5%BA%93%E5%AE%8C%E6%88%90IsFinite%E7%AE%97%E5%AD%90%E7%9A%84%E7%AB%AF%E4%BE%A7%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81.docx) |   |   | 定制奖品|

### 活动时间

 :rocket:  CANN训练营营期

 **第一批：** 2025年2月28日-5月30日，第一批获奖名单待公示

 **第二批：** 2025年5月30日-7月30日


### 参与步骤：

 :one: **第一步:参与条件**

1.报名2025年CANN训练营：[报名链接](http://www.hiascend.com/developer/activities/details/32321315819f4b9fa73f50d99138a855/signup?channelCode=0&recommended=337643)

2.获得Ascend C算子开发能力（中级）认证证书：[认证链接](https://www.hiascend.com/edu/certification/detail/34bf904cb410497cb9c582be6c047ff7)

微认证代金券领取：[使用指南](https://www.hiascend.com/forum/thread-02104162960981826076-1-1.html)


 :two: **第二步:任务报名**

-  在此页面进行评论申请
示例：
【队名】：根本不报错
【序号】：51
【状态】：报名
【链接】：fork链接


 :three: **第三步:任务提交规则**

1.任务结束规则
- 首个提交验收通过后，不再接受其他提交验收，当前任务自动关闭。
- 任务51-53，第一名提交验收通过后，在后续7个工作日仍可接受其他提交验收，7个工作日后当前任务自动关闭。

2.任务验收规则
- 联系昇腾小助手提交验收申请，需要同步提交如下交付件：
（1）任务书中明确要求的交付内容
（2）任务自测试报告
（3）其他要求可以在任务申请成功后联系昇腾小助手沟通

 :four: **第四步:活动答疑**
- 对实操挑战活动有任何疑问，可添加昇腾小助手咨询
- 完成任务遇到技术问题，请在实操挑战交流群询问

 :calling:  **扫码添加昇腾小助手微信、实操挑战交流群**  


<div align=""left"" width=""200"">
        <img src=""https://foruda.gitee.com/images/1747393940128104412/b5b5099e_15726163.png "" width=30%>
     </div>


### 福利加码
欢迎各位参与实操挑战的开发者将自己的实操过程整理成笔记或者录制成视频并上传至以下链接中，CANN训练营将定期选取优质内容分享给更多开发者学习，更有机会获得神秘奖品~
上传链接：https://www.hiascend.com/forum/thread-0292174273505775008-1-1.html


","open"
"https://gitee.com/ascend/cann-community/issues/IBBNKQ","ascend","cann-community","[Documentation|文档反馈]:  英文命名错误","### Document Link（文档链接）

![输入图片说明](https://foruda.gitee.com/images/1734354219183859130/20d71265_14130145.png ""屏幕截图"")

### Issues Section（问题文档片段）

![输入图片说明](https://foruda.gitee.com/images/1734354224471291400/e9b44414_14130145.png ""屏幕截图"")

### Existing Issues（存在的问题）



","open"
"https://gitee.com/ascend/cann-ops-adv/issues/IC7QY5","ascend","cann-ops-adv","[Question|问题咨询]: pfa测试代码好像有问题","### 问题描述

![输入图片说明](https://foruda.gitee.com/images/1747208984095081371/d53075d5_15493901.png ""0.png"")test_prompt_flash_attention.cpp 调试跟踪到 prompt_flash_attention_split_n_s_no_tail.h 里边 409行，就退出，根本不跑后边数据处理流程。

### 所属算子

PFA


","open"
"https://gitee.com/ascend/cann-ops-adv/issues/IC3YLA","ascend","cann-ops-adv","[Question|问题咨询]: FA 的GTKWeave运行图 AIV有规律的空白区域是什么原因","### 问题描述


![输入图片说明](https://foruda.gitee.com/images/1745566877231299374/82bc0d33_15493901.png ""wenti.png"")
输入shape(20,1024,1,128) BSND格式。 我跑出了上边的GTKWeave运行图  。7,8,9,10后边AIC核 都有比较长的空白区域，是否合理？空白原因是什么呢？

通过对比更细节的图，我个人理解是：多发一次mmad产生的aic并行达到了。从图中看，aic 运行完 一个并行cycle 后(c1v1c2v2+空 c1v1c2v2) ,会空闲一段，想问一下，是不是由于aiv的分loop运行aic的输出数据，速度慢导致的等待，或者还有其他什么可能原因？

### 所属算子

FA


","open"
"https://gitee.com/ascend/cann-ops-adv/issues/IBYCEK","ascend","cann-ops-adv","[Question|问题咨询]: 如果想要优化核间、vector的策略，可以调整哪些部分的参数呢","### 问题描述

如果想要优化核间、vector的策略，可以调整哪些部分的参数呢

### 所属算子

FFN


","open"
"https://gitee.com/ascend/cann-ops-adv/issues/IBWG1K","ascend","cann-ops-adv","[Question|问题咨询]: 根目录下的cmakelists不是很懂怎么加-g，用msprof工具时无法显示源码，求助","### 问题描述

 根目录下的cmakelists不是很懂怎么加-g，用msprof工具时无法显示源码，求助

### 所属算子

FFN


","open"
"https://gitee.com/ascend/EdgeAndRobotics/issues/I9QS5D","ascend","EdgeAndRobotics","养鸭文 你在干嘛呢？ 喝穴水呢",,"open"
"https://gitee.com/ascend/torchair/issues/IC52F6","ascend","torchair","[版本匹配] 现在Llama2是不是可以使用新的transformers==4.44.2","在 https://gitee.com/ascend/torchair/blob/master/npu_tuned_model/llm/llama/README.md#llama2llama3 中看到 Llama2匹配transformers==4.31.0，请问如何才能确认当前是否匹配 ？
我已经能够跑非图模式，是否就已经代表环境是没有问题了？(基于torchair跑图模式有点问题，还在确认是本身的测试脚本问题，还是其它环境问题)


软件版本: （46，30188） 
```
-- CANN 版本: 8.0.RC3 
-- Tensorflow/Pytorch/MindSpore 版本: torch 2.1.0/torch-npu 2.1.0  
-- Python 版本 ：Python 3.10.10  （source /home/ma-user/work/zhongyunde/source/backup/venv/bin/activate）
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): NA 
-- 操作系统版本 (e.g., Ubuntu 18.04): Linux version 4.19.90-vhulk2211.3.0.h1543.eulerosv2r10.aarch64 (cat /proc/version)
```

参考 https://gitee.com/ascend/modelzoo/issues/IC4W6L?from=project-issue","open"
"https://gitee.com/ascend/torchair/issues/IBZ59K","ascend","torchair","torchair入图进行训练时，expand扩维后模型出现精度问题， 推理时无精度问题","一、需求场景&价值
torchair入图进行模型训练时出现精度问题， 但该场景下正常可提升性能，分析的可能是出现的一个bug。  图模式训练入图时bug较多，只是遇到了其中一个。
二、需求建议实现的规格

三、竞品比较（选填）","open"
"https://gitee.com/ascend/torchair/issues/IBOB0Z","ascend","torchair","torch.compile还不支持torch.all_to_all算子以及torch.all_gather算子","torch.compile还不支持torch.all_to_all算子以及torch.all_gather算子","open"
"https://gitee.com/ascend/torchair/issues/IBKZ9U","ascend","torchair","deepseek MLA","一、需求场景&价值
deepseekV3模型参数量巨大，在使用MHA模式的时候，大batch/input_len的数据因为显存问题不能推理，无法推理发挥优势。
","open"
"https://gitee.com/ascend/torchair/issues/IBH9F0","ascend","torchair","运行上手代码时报错，显示undefined symbol","一、问题现象（附报错日志上下文）：
运行上手代码时报错，显示undefined symbol
![输入图片说明](https://foruda.gitee.com/images/1736677807419934171/99b70d53_11331675.png ""屏幕截图"")
报错：

![输入图片说明](https://foruda.gitee.com/images/1736677861438328974/5e5ae1c7_11331675.png ""屏幕截图"")
","open"
"https://gitee.com/ascend/torchair/issues/IBF9U4","ascend","torchair","测试deepseekv3，手动切分与调整权重时出错。 ","按照README，手动切分与调整权重时出错

python3 scripts/split_weight.py --model-path /home/yhui/llm/DeepSeek-V3 --output-path /home/yhui/llm/DeepSeek-V3-tp --world-size 8

Traceback (most recent call last):
  File ""/home/yhui/torchair/npu_tuned_model/llm/deepseek_v3/scripts/split_weight.py"", line 193, in <module>
    origin_model = AutoModelForCausalLM.from_pretrained(args.model_path,
  File ""/home/yhui/anaconda3/envs/deepseek_v3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 553, in from_pretrained
    model_class = get_class_from_dynamic_module(
  File ""/home/yhui/anaconda3/envs/deepseek_v3/lib/python3.10/site-packages/transformers/dynamic_module_utils.py"", line 488, in get_class_from_dynamic_module
    final_module = get_cached_module_file(
  File ""/home/yhui/anaconda3/envs/deepseek_v3/lib/python3.10/site-packages/transformers/dynamic_module_utils.py"", line 315, in get_cached_module_file
    modules_needed = check_imports(resolved_module_file)
  File ""/home/yhui/anaconda3/envs/deepseek_v3/lib/python3.10/site-packages/transformers/dynamic_module_utils.py"", line 180, in check_imports
    raise ImportError(
ImportError: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`
[ERROR] 2025-01-02-17:21:09 (PID:828812, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception

npu下找不到flash_attn，这个怎么解决","open"
"https://gitee.com/ascend/torchair/issues/IB82BS","ascend","torchair","尝试单机使用pd_separate 部分示例代码运行错误，附错误报告","我试着在单机八卡的环境上试用pd分离的benchmark
## 环境信息
```sh
Atlas800训练服务器（型号：9000）
Architecture: aarch64
HostSystem: OpenEuler2203
CPU: Kunpeng-920
GPU: 910B*8
docker img: 
GuestSystem: Ubuntu 22.04.4 LTS
cann Version:8.0.RC3
Docker Image: Mindie # ref [mindie](https://www.hiascend.com/developer/ascendhub/detail/af85b724a7e5469ebd7ea13c3439d48f)
python: 3.9
pip:
--torch                  2.1.0
--torch-npu              2.1.0.post8
--transformers           4.31.0
model meta-llama/Llama-2-70b-hf

```
## 用到的操作
### 网络配置
```sh
## 配置ip
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 0 -ip -s address 192.168.100.101 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 1 -ip -s address 192.168.101.101 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 2 -ip -s address 192.168.102.101 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 3 -ip -s address 192.168.103.101 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 4 -ip -s address 192.168.100.100 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 5 -ip -s address 192.168.101.100 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 6 -ip -s address 192.168.102.100 netmask 255.255.255.0
sudo /usr/local/Ascend/driver/tools/hccn_tool -i 7 -ip -s address 192.168.103.100 netmask 255.255.255.0
## route
sudo hccn_tool -i 0 -ip_rule -a dir from ip 192.168.100.101  table 101
sudo hccn_tool -i 1 -ip_rule -a dir from ip 192.168.101.101  table 102
sudo hccn_tool -i 2 -ip_rule -a dir from ip 192.168.102.101  table 103
sudo hccn_tool -i 3 -ip_rule -a dir from ip 192.168.103.101 table 104
sudo hccn_tool -i 4 -ip_rule -a dir from ip 192.168.100.100 table 105
sudo hccn_tool -i 5 -ip_rule -a dir from ip 192.168.101.100 table 106
sudo hccn_tool -i 6 -ip_rule -a dir from ip 192.168.102.100 table 107
sudo hccn_tool -i 7 -ip_rule -a dir from ip 192.168.103.100 table 108
sudo hccn_tool -i 0 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.100.254  dev eth0 table 101
sudo hccn_tool -i 1 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.101.254  dev eth1 table 102
sudo hccn_tool -i 2 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.102.254  dev eth2 table 103
sudo hccn_tool -i 3 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.103.254  dev eth3 table 104
sudo hccn_tool -i 4 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.100.254  dev eth4 table 105
sudo hccn_tool -i 5 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.101.254  dev eth5 table 106
sudo hccn_tool -i 6 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.102.254  dev eth6 table 107
sudo hccn_tool -i 7 -ip_route -a ip 192.168.0.0 ip_mask 16 via 192.168.103.254  dev eth7 table 108
### 清除： sudo hccn_tool -i 1 -route -c
## gateway
sudo hccn_tool -i 3 -gateway -s gateway 192.168.103.102
sudo hccn_tool -i 7 -gateway -s gateway 192.168.103.102
## 指定netdetect ip
sudo hccn_tool -i 0 -netdetect -s address 192.168.100.101
sudo hccn_tool -i 1 -netdetect -s address 192.168.101.101
sudo hccn_tool -i 2 -netdetect -s address 192.168.102.101
sudo hccn_tool -i 3 -netdetect -s address 192.168.103.101
sudo hccn_tool -i 4 -netdetect -s address 192.168.100.100
sudo hccn_tool -i 5 -netdetect -s address 192.168.101.100
sudo hccn_tool -i 6 -netdetect -s address 192.168.102.100
sudo hccn_tool -i 7 -netdetect -s address 192.168.103.100

```
![输入图片说明](https://foruda.gitee.com/images/1732947167843230556/aa0fcd3e_13444340.png ""屏幕截图"")
## pd_seperate使用及报错
ref [Ascend/torchair - Gitee.com](https://gitee.com/ascend/torchair/tree/master/npu_tuned_model/llm/llama/benchmark/pd_separate)
```sh
# 在这之前我从huggingface上下载了meta-llama/Llama-2-70b-hf模型，并根据ref文档里的步骤进行了权重融合。
# ip信息也都改为了上面设置的设备ip信息
# LinearAllreduce类也替换，并import了torch_npu
deepspeed --num_gpus=8 benchmark/pd_separate/run_prompt.py --model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf 

```
我怀疑是我的网络配置有问题，因为实际上ping不通对应的GPU节点
报错文件
```sh
[2024-11-29 07:24:14,482] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:15,607] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-29 07:24:15,607] [INFO] [runner.py:607:main] cmd = /root/anaconda3/envs/hwbase39/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None benchmark/pd_separate/run_prompt.py --model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf
[2024-11-29 07:24:19,629] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:20,766] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-29 07:24:20,766] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-29 07:24:20,766] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-29 07:24:20,766] [INFO] [launch.py:164:main] dist_world_size=8
[2024-11-29 07:24:20,766] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-29 07:24:20,783] [INFO] [launch.py:256:main] process 2033374 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=0', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,796] [INFO] [launch.py:256:main] process 2033376 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=1', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,808] [INFO] [launch.py:256:main] process 2033378 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=2', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,821] [INFO] [launch.py:256:main] process 2033380 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=3', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,834] [INFO] [launch.py:256:main] process 2033382 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=4', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,854] [INFO] [launch.py:256:main] process 2033384 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=5', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,881] [INFO] [launch.py:256:main] process 2033386 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=6', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:20,903] [INFO] [launch.py:256:main] process 2033388 spawned with command: ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=7', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf']
[2024-11-29 07:24:25,730] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:25,812] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:25,858] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:25,895] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:25,941] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:25,949] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:26,034] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-29 07:24:26,174] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to npu (auto detect)
2024-11-29 07:24:42,348 - INFO - [LLM](llm_inference.py:42): Set execution using npu index: 0
2024-11-29 07:24:42,349 - INFO - [LLM](llm_inference.py:46): Try to load pretrained model in path: /root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf

Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
Traceback (most recent call last):
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.102.101:26000', 'ge.exec.deviceId': '2', 'ge.session_device_id': '2', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1701226688, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_2.json'}.
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.102.100:26000', 'ge.exec.deviceId': '6', 'ge.session_device_id': '6', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1684449472, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_6.json'}.
[ERROR] 2024-11-29-07:24:45 (PID:2033378, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[ERROR] 2024-11-29-07:24:45 (PID:2033386, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception

Loading checkpoint shards:   6%|▋         | 1/16 [00:03<00:47,  3.16s/it]Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.101.100:26000', 'ge.exec.deviceId': '5', 'ge.session_device_id': '5', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1684383936, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_5.json'}.
[ERROR] 2024-11-29-07:24:46 (PID:2033384, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.101.101:26000', 'ge.exec.deviceId': '1', 'ge.session_device_id': '1', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1701161152, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_1.json'}.
[ERROR] 2024-11-29-07:24:48 (PID:2033376, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[2024-11-29 07:24:48,938] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033374
Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.100.100:26000', 'ge.exec.deviceId': '4', 'ge.session_device_id': '4', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1684318400, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_4.json'}.
Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.103.101:26000', 'ge.exec.deviceId': '3', 'ge.session_device_id': '3', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1701292224, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_3.json'}.
[ERROR] 2024-11-29-07:24:49 (PID:2033382, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[ERROR] 2024-11-29-07:24:49 (PID:2033380, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/root/anaconda3/envs/hwbase39/lib/python3.9/threading.py"", line 980, in _bootstrap_inner
    self.run()
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/utils/multiprocess_util.py"", line 91, in run
    key, func, args, kwargs = self.task_q.get(timeout=TIMEOUT)
  File ""<string>"", line 2, in get
  File ""/root/anaconda3/envs/hwbase39/lib/python3.9/multiprocessing/managers.py"", line 810, in _callmethod
    kind, result = conn.recv()
  File ""/root/anaconda3/envs/hwbase39/lib/python3.9/multiprocessing/connection.py"", line 250, in recv
    buf = self._recv_bytes()
  File ""/root/anaconda3/envs/hwbase39/lib/python3.9/multiprocessing/connection.py"", line 414, in _recv_bytes
    buf = self._recv(4)
  File ""/root/anaconda3/envs/hwbase39/lib/python3.9/multiprocessing/connection.py"", line 383, in _recv
    raise EOFError
EOFError
/root/anaconda3/envs/hwbase39/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[2024-11-29 07:24:49,496] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033376
[2024-11-29 07:24:49,964] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033378
[2024-11-29 07:24:49,964] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033380
Traceback (most recent call last):
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 156, in <module>
    prompt_engine = init_llm_engine(local_rank)
  File ""/root/download/torchair/npu_tuned_model/llm/llama/benchmark/pd_separate/run_prompt.py"", line 101, in init_llm_engine
    engine.init(llm_config.generate_options())
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/v2/llm_datadist.py"", line 272, in init
    handle_llm_status(ret, '[LLMDataDist.init]', f'Failed to initialize llm engine, options = {options}')
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/llm_datadist/status.py"", line 115, in handle_llm_status
    raise LLMException(f""{func_name} failed, error code is {code_2_status(status)}, {other_info}."",
llm_datadist.status.LLMException: [LLMDataDist.init] failed, error code is LLMStatusCode.LLM_FAILED, Failed to initialize llm engine, options = {'ge.flowGraphMemMaxSize': '2589934592', 'llm.listenIpInfo': '192.168.103.100:26000', 'ge.exec.deviceId': '7', 'ge.session_device_id': '7', 'llm.MemoryUtilization': '0.95', 'llm.Role': 'Prompt', 'llm.ClusterInfo': '{""cluster_id"": 0, ""logic_device_id"": [""0:0:0:0""], ""listen_ip_info"": [{""ip"": 1684515008, ""port"": 26000}]}', 'ge.resourceConfigPath': '/root/download/torchair/npu_tuned_model/llm/llama/stub_numa_config_prompt_7.json'}.
[ERROR] 2024-11-29-07:24:50 (PID:2033388, Device:-1, RankID:-1) ERR99999 UNKNOWN application exception
[2024-11-29 07:24:50,470] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033382
[2024-11-29 07:24:50,804] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033384
[2024-11-29 07:24:50,805] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033386
[2024-11-29 07:24:50,806] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2033388
[2024-11-29 07:24:51,312] [ERROR] [launch.py:325:sigkill_handler] ['/root/anaconda3/envs/hwbase39/bin/python', '-u', 'benchmark/pd_separate/run_prompt.py', '--local_rank=7', '--model_path=/root/download/torchair/npu_tuned_model/llm/llama/meta-llama/Llama-2-70b-hf'] exits with return code = 1

```","open"
"https://gitee.com/ascend/torchair/issues/IB5SPG","ascend","torchair","关于torchair疑问","torchair中转换的GE算子和AscendC算子是一个吗？以Add举例
torchair是否可以理解为是将torch层算子通过优化手段将其转换为图，然后图再通过runtime 并按照aclop模式去执行对应的AscendC算子？
torchair是否不走pytoch adaptor层，直接通过图和runtime方式连接到算子，而torch_npu是通过pytorch adaptor的op-plugin方式连接到算子？","open"
"https://gitee.com/ascend/torchair/issues/IB5SGG","ascend","torchair","拉取master编译有异常","一、问题现象（附报错日志上下文）：
编译过程中出现
![输入图片说明](https://foruda.gitee.com/images/1732092934217391106/f30f3a10_7471722.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 :  8.0.RC3.alpha003
--Tensorflow/Pytorch/MindSpore 版本: pytorch 2.1.0
--Python 版本 (e.g., Python 3.7.5): 3.10.13
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
编译


四、日志信息:
","open"
"https://gitee.com/ascend/torchair/issues/I8NFM6","ascend","torchair","融合算子的通用scheduler生成",,"open"
"https://gitee.com/ascend/torchair/issues/I8NFK4","ascend","torchair","Auto Schedule：融合决策和算子通用scheduler",,"open"
"https://gitee.com/ascend/torchair/issues/IATUH1","ascend","torchair","6.0.rc2分支，readme中的“快速上手”章节例子执行报错","6.0.rc2分支，readme中的“快速上手”章节：
![输入图片说明](https://foruda.gitee.com/images/1727333389278285489/259a2782_14766326.png ""屏幕截图"")
例子执行是报错：
![输入图片说明](https://foruda.gitee.com/images/1727333480766959707/6e8bbadd_14766326.png ""屏幕截图"")
按照昇腾社区的指导书（https://www.hiascend.com/document/detail/zh/Pytorch/60RC2/modthirdparty/torchairuseguide/torchair_0009.html）
修改脚本如下：
![输入图片说明](https://foruda.gitee.com/images/1727333557940377659/8eec5615_14766326.png ""屏幕截图"")
可以执行成功，能够生成计算图文件
![输入图片说明](https://foruda.gitee.com/images/1727333607169723667/41756dca_14766326.png ""屏幕截图"")
","open"
"https://gitee.com/ascend/torchair/issues/IAOHVY","ascend","torchair","使用torchair编译llama2，kvcache模块报错","报错“NotImplementedError: torch.ops.aten.index_put.default ge_converter is not implemented!”。像是由切片写法引起的报错。想问下，是否不支持kvcache中的切片写法。如果不支持的话，该如何规避？
![输入图片说明](https://foruda.gitee.com/images/1725351221343713647/3c36ad6e_7795794.png ""屏幕截图"")

2024-09-03 16:08:52,237] [0/0] torch._dynamo.output_graph: [WARNING] nn.Module state_dict and backward hooks are not yet supported by torch.compile, but were detected in your model and will be silently ignored. See https://pytorch.org/docs/master/compile/nn-module.html for more information and limitations.
/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/utils/storage.py:38: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  if self.device.type != 'cpu':
  0%|                                                                                                                                                                                       | 0/1679 [01:21<?, ?it/s]
0it [01:22, ?it/s]
Traceback (most recent call last):
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 1044, in <module>
    main()
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/click/core.py"", line 1078, in main
    rv = self.invoke(ctx)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 929, in main
    process(model, 
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 1038, in process
    for _, codes in enumerate(generator):
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 644, in generate_long
    y = generate(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 300, in generate
    x = decode_n_tokens(
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 216, in decode_n_tokens
    next_token = decode_one_token(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 490, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 133, in _fn
    return fn(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 389, in _convert_frame_assert
    return _compile(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 569, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 491, in compile_inner
    out_code = transform_code_object(code, transform)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py"", line 1028, in transform_code_object
    transformations(instructions, code_options)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py"", line 458, in transform
    tracer.run()
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py"", line 2074, in run
    super().run()
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py"", line 724, in run
    and self.step()
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py"", line 688, in step
    getattr(self, inst.opname)(inst)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py"", line 2162, in RETURN_VALUE
    self.output.compile_subgraph(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/output_graph.py"", line 833, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/contextlib.py"", line 79, in inner
    return func(*args, **kwds)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/output_graph.py"", line 957, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/output_graph.py"", line 1024, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/output_graph.py"", line 1009, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/repro/after_dynamo.py"", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/__init__.py"", line 1607, in __call__
    return self.compiler_fn(model_, inputs_, **self.kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 460, in _npu_backend
    return aot_module_simplified(gm, example_inputs, fw_compiler=fw_compiler, bw_compiler=compiler,
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 3891, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_dynamo/utils.py"", line 189, in time_wrapper
    r = func(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 3429, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 2212, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 2392, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 1573, in aot_dispatch_base
    compiled_fw = compiler(fw_module, flat_args)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 433, in gear_compiler
    return compiler(gm, example_inputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 408, in wrapped_compiler
    return compiler(gm, example_inputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_utils/error_code.py"", line 46, in wapper
    raise type(e)(""\n"".join(msg))
torch._dynamo.exc.BackendCompilerFailed: backend=""functools.partial(<function _npu_backend at 0xfffd882278b0>, compiler_config=<torchair.configs.compiler_config.CompilerConfig object at 0xffff73f26610>, decompositions={<OpOverload(op='npu_define.all_to_all_single', overload='default')>: <function all_to_all_single_decomposition at 0xfffd832fe3a0>, <OpOverload(op='npu_define.all_to_all', overload='default')>: <function all_to_all_decomposition at 0xfffd832fe8b0>, <OpOverload(op='npu_define.allgather', overload='default')>: <function allgather_decomposition at 0xfffd832fe160>})"" raised:
NotImplementedError: torch.ops.aten.index_put.default ge_converter is not implemented!

While executing %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put.default](args = (%slice_7, [None, None, %arg279_1], %view_11), kwargs = {})
Original traceback:
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 121, in decode_one_token_ar
    x = model.forward_generate(x, input_pos, prompt_audios, spk_embedding)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 290, in forward_generate
    x = layer(x, freqs_cis, mask, input_pos=input_pos)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 478, in forward
    h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 531, in forward
    k, v = self.kv_cache.update(input_pos, k, v)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 84, in update
    k_out[:, :, input_pos] = k_val

Traceback (most recent call last):
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_utils/error_code.py"", line 43, in wapper
    return func(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 320, in __call__
    return self._get_compiled_gm(gm, example_inputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 356, in _get_compiled_gm
    return _GmRunner(self._gen_compiled_gm(gm, example_inputs))
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 366, in _gen_compiled_gm
    concrete_graph: ConcreteGraphBase = _NpuGraphConverter(
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 151, in run
    super().run(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/fx/interpreter.py"", line 138, in run
    self.env[node] = self.run_node(node)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 145, in run_node
    return super().run_node(n)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/fx/interpreter.py"", line 195, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 118, in inner
    result = f(self, target, args, kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 205, in call_function
    return self._wrap('call_function')(target, args, kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 192, in inner
    npu_outputs = self._graph.parse_node(target, args_npu, kwargs_npu, meta_outputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_ge_concrete_graph/continguous_utils.py"", line 149, in wrapper
    return func(self, target, args_new, kwargs_new, meta_outputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_ge_concrete_graph/fx2ge_converter.py"", line 801, in parse_node
    ge_outputs = converter(*args, **kwargs, meta_outputs=meta_outputs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_ge_concrete_graph/fx2ge_converter.py"", line 196, in wrapped_converter
    ge_outputs = converter(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/dynamo/torchair/_ge_concrete_graph/ge_converter/aten/index_put.py"", line 34, in conveter_aten_index_put_default
    raise NotImplementedError(""torch.ops.aten.index_put.default ge_converter is not implemented!"")
NotImplementedError: torch.ops.aten.index_put.default ge_converter is not implemented!

While executing %index_put : [num_users=1] = call_function[target=torch.ops.aten.index_put.default](args = (%slice_7, [None, None, %arg279_1], %view_11), kwargs = {})
Original traceback:
  File ""/data_sis/w00800709/fish-speech-v2/tools/llama/generate2_use_spk_embedding_list_or_ge.py"", line 121, in decode_one_token_ar
    x = model.forward_generate(x, input_pos, prompt_audios, spk_embedding)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 290, in forward_generate
    x = layer(x, freqs_cis, mask, input_pos=input_pos)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 478, in forward
    h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)
  File ""/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 531, in forward
    k, v = self.kv_cache.update(input_pos, k, v)
  File ""/data_sis/w00800709/fish-speech-v2/fish_speech/models/text2semantic/llama.py"", line 84, in update
    k_out[:, :, input_pos] = k_val



[ERROR] 2024-09-03-16:10:13 (PID:78101, Device:0, RankID:-1) ERR03007 GRAPH feature not supported
","open"
"https://gitee.com/ascend/torchair/issues/IALI8O","ascend","torchair","缺少torch.ops.aten.sym_size.int算子的转换方法","一、问题现象：
[ge_converter/aten/sym_size.py](https://gitee.com/ascend/torchair/blob/master/python/torchair/_ge_concrete_graph/ge_converter/aten/sym_size.py)中没有提供`torch.ops.aten.sym_size.int`算子的转换方法（仅提供了`torch.ops.aten.sym_size.default`和`torch.ops.aten.sym_size`），导致编译失败
","open"
"https://gitee.com/ascend/torchair/issues/IAJVYD","ascend","torchair","示例BUG，无法跑通","示例无法跑通，需要注释掉几行才行！！

![输入图片说明](https://foruda.gitee.com/images/1723603802160154686/05328530_12699457.png ""屏幕截图"")","open"
"https://gitee.com/ascend/torchair/issues/IAJ260","ascend","torchair","打包报错，因mindLink依赖Mindspeed依赖torchAir，已造成严重阻塞","为啥没个wheel包呢？跟着readme打包还报错，人快疯了
日志全文如下
(PyTorch-2.1.0) [ma-user build]$make torchair -j8
make[1]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[2]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[  1%] Generating _fake.cc
[  3%] Generating _fake.cc
[  5%] Generating _fake.cc
[  5%] Generating _fake.cc
[  8%] Generating _fake.cc
[  8%] Generating _fake.cc
Scanning dependencies of target copy_pyfiles
[ 10%] Generating _fake.cc
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
Scanning dependencies of target acl_op_compiler
Scanning dependencies of target ascend_protobuf
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
Scanning dependencies of target graph
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
Scanning dependencies of target hccl_stub
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
Scanning dependencies of target acl_tdt_channel
Scanning dependencies of target fmk_parser
Scanning dependencies of target acl_stub
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 12%] Copy pthon files
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 12%] Building CXX object CMakeFiles/acl_op_compiler.dir/_fake.cc.o
[ 14%] Building CXX object CMakeFiles/graph.dir/_fake.cc.o
[ 15%] Building CXX object CMakeFiles/ascend_protobuf.dir/_fake.cc.o
[ 17%] Building CXX object CMakeFiles/hccl_stub.dir/_fake.cc.o
[ 19%] Building CXX object CMakeFiles/acl_tdt_channel.dir/_fake.cc.o
[ 21%] Building CXX object CMakeFiles/fmk_parser.dir/_fake.cc.o
[ 22%] Building CXX object CMakeFiles/acl_stub.dir/_fake.cc.o
[ 24%] Linking CXX shared library stubs/unused/libgraph.so
[ 26%] Linking CXX shared library stubs/libacl_op_compiler.so
[ 28%] Linking CXX shared library stubs/unused/libascend_protobuf.so
[ 29%] Building CXX object CMakeFiles/hccl_stub.dir/cmake/hccl/hccl_stub.cpp.o
[ 29%] Linking CXX shared library stubs/libacl_tdt_channel.so
[ 29%] Linking CXX shared library stubs/libfmk_parser.so
[ 31%] Building CXX object CMakeFiles/acl_stub.dir/cmake/acl/acl_stub.cpp.o
[ 33%] Linking CXX shared library stubs/libhccl_stub.so
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 33%] Built target graph
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 33%] Built target copy_pyfiles
[ 33%] Built target ascend_protobuf
[ 33%] Built target acl_op_compiler
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 33%] Built target acl_tdt_channel
[ 33%] Built target fmk_parser
Scanning dependencies of target ge_local_stub
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 33%] Built target hccl_stub
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 35%] Building CXX object CMakeFiles/ge_local_stub.dir/cmake/graph_engine/ge_stub.cpp.o
[ 36%] Building CXX object CMakeFiles/ge_local_stub.dir/_fake.cc.o
Scanning dependencies of target hccl
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[3]: Entering directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 38%] Building CXX object CMakeFiles/hccl.dir/_fake.cc.o
[ 40%] Linking CXX shared library stubs/libhccl.so
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 42%] Built target hccl
In file included from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_base.h:16:0,
                 from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:16,
                 from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:4:
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/error_codes/rt_error_codes.h:1:1: error: expected unqualified-id before ‘.’ token
 ../../../../runtime/external/runtime/rt_error_codes.h
 ^
In file included from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:4:0:
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:212:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetExceptionInfoCallback(aclrtExceptionInfoCallback callback);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:279:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSubscribeReport(uint64_t threadId, aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:296:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtLaunchCallback(aclrtCallback fn, void *userData, aclrtCallbackBlockType blockType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:314:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtProcessReport(int32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:328:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtUnSubscribeReport(uint64_t threadId, aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:355:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateContext(aclrtContext *context, int32_t deviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:371:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyContext(aclrtContext context);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:407:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetCurrentContext(aclrtContext context);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:425:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetCurrentContext(aclrtContext *context);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:436:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCtxGetSysParamOpt(aclSysParamOpt opt, int64_t *value);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:447:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCtxSetSysParamOpt(aclSysParamOpt opt, int64_t value);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:475:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetDevice(int32_t deviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:502:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtResetDevice(int32_t deviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:513:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetDevice(int32_t *deviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:525:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetStreamFailureMode(aclrtStream stream, uint64_t mode);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:536:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetRunMode(aclrtRunMode *runMode);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:545:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSynchronizeDevice(void);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:556:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetTsDevice(aclrtTsId tsId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:567:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetDeviceUtilizationRate(int32_t deviceId, aclrtUtilizationInfo *utilizationInfo);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:578:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetDeviceCount(uint32_t *count);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:589:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateEvent(aclrtEvent *event);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:601:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateEventWithFlag(aclrtEvent *event, uint32_t flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:613:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateEventExWithFlag(aclrtEvent *event, uint32_t flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:633:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyEvent(aclrtEvent event);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:645:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtRecordEvent(aclrtEvent event, aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:661:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtResetEvent(aclrtEvent event, aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:674:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtQueryEvent(aclrtEvent event, aclrtEventStatus *status);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:686:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtQueryEventStatus(aclrtEvent event, aclrtEventRecordedStatus *status);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:698:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtQueryEventWaitStatus(aclrtEvent event, aclrtEventWaitStatus *status);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:709:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSynchronizeEvent(aclrtEvent event);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:722:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSynchronizeEventWithTimeout(aclrtEvent event, int32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:737:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtEventElapsedTime(float *ms, aclrtEvent startEvent, aclrtEvent endEvent);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:763:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMalloc(void **devPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:788:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMallocAlign32(void **devPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:813:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMallocCached(void **devPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:827:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemFlush(void *devPtr, size_t size);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:839:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemInvalidate(void *devPtr, size_t size);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:855:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtFree(void *devPtr);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:875:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMallocHost(void **hostPtr, size_t size);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:891:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtFreeHost(void *hostPtr);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:906:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemcpy(void *dst,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:929:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemset(void *devPtr, size_t maxCount, int32_t value, size_t count);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:956:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemcpyAsync(void *dst,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:978:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemcpy2d(void *dst,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1002:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemcpy2dAsync(void *dst,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1032:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemsetAsync(void *devPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1053:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtReserveMemAddress(void **virPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1070:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtReleaseMemAddress(void *virPtr);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1088:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMallocPhysical(aclrtDrvMemHandle *handle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1105:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtFreePhysical(aclrtDrvMemHandle handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1122:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMapMem(void *virPtr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1139:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtUnmapMem(void *virPtr);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1158:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyStreamConfigHandle(aclrtStreamConfigHandle *handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1172:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetStreamConfigOpt(aclrtStreamConfigHandle *handle, aclrtStreamConfigAttr attr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1184:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateStream(aclrtStream *stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1195:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateStreamV2(aclrtStream *stream, const aclrtStreamConfigHandle *handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1211:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtCreateStreamWithConfig(aclrtStream *stream, uint32_t priority, uint32_t flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1232:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyStream(aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1248:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyStreamForce(aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1260:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSynchronizeStream(aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1274:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSynchronizeStreamWithTimeout(aclrtStream stream, int32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1286:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtStreamQuery(aclrtStream stream, aclrtStreamStatus *status);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1300:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtStreamWaitEvent(aclrtStream stream, aclrtEvent event);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1316:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetGroup(int32_t groupId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1332:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetGroupCount(uint32_t *count);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1356:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyGroupInfo(aclrtGroupInfo *groupInfo);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1369:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetAllGroupInfo(aclrtGroupInfo *groupInfo);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1387:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetGroupInfoDetail(const aclrtGroupInfo *groupInfo,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1407:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDeviceCanAccessPeer(int32_t *canAccessPeer, int32_t deviceId, int32_t peerDeviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1421:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDeviceEnablePeerAccess(int32_t peerDeviceId, uint32_t flags);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1434:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDeviceDisablePeerAccess(int32_t peerDeviceId);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1448:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetMemInfo(aclrtMemAttr attr, size_t *free, size_t *total);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1459:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetOpWaitTimeout(uint32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1470:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetOpExecuteTimeOut(uint32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1481:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetStreamOverflowSwitch(aclrtStream stream, uint32_t flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1492:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetStreamOverflowSwitch(aclrtStream stream, uint32_t *flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1502:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSetDeviceSatMode(aclrtFloatOverflowMode mode);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1512:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetDeviceSatMode(aclrtFloatOverflowMode *mode);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1529:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetOverflowStatus(void *outputAddr, size_t outputSize, aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1544:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtResetOverflowStatus(aclrtStream stream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1556:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtSubscribeHostFunc(uint64_t hostFuncThreadId, aclrtStream exeStream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1572:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtProcessHostFunc(int32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1586:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtUnSubscribeHostFunc(uint64_t hostFuncThreadId, aclrtStream exeStream);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1598:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtQueryDeviceStatus(int32_t deviceId, aclrtDeviceStatus *deviceStatus);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1620:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDestroyBinary(aclrtBinary binary);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1633:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtBinaryLoad(const aclrtBinary binary, aclrtBinHandle *binHandle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1645:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtBinaryUnLoad(aclrtBinHandle binHandle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1658:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtBinaryGetFunction(const aclrtBinHandle binHandle, const char *kernelName,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1673:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtLaunchKernel(aclrtFuncHandle funcHandle, uint32_t blockDim,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1687:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemExportToShareableHandle(aclrtDrvMemHandle handle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1701:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemImportFromShareableHandle(uint64_t shareableHandle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1714:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemSetPidToShareableHandle(uint64_t shareableHandle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1727:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemGetAllocationGranularity(aclrtPhysicalMemProp *prop,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1739:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDeviceGetBareTgid(int32_t *pid);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1751:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtGetMemUceInfo(int32_t deviceId, aclrtMemUceInfo *memUceInfoArray,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1762:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtDeviceTaskAbort(int32_t deviceId, uint32_t timeout);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_rt.h:1773:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclrtMemUceRepair(int32_t deviceId, aclrtMemUceInfo *memUceInfoArray, size_t arraySize);
                     ^~~~~~~~
                     perror
In file included from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:5:0:
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:53:21: error: ‘aclDataType’ does not name a type
 ACL_FUNC_VISIBILITY aclDataType acltdtGetDataTypeFromItem(const acltdtDataItem *dataItem);
                     ^~~~~~~~~~~
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:99:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtGetSliceInfoFromItem(const acltdtDataItem *dataItem, size_t *sliceNum,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:113:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtGetDimsFromItem(const acltdtDataItem *dataItem, int64_t *dims, size_t dimNum);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:134:58: error: ‘aclDataType’ has not been declared
                                                          aclDataType dataType,
                                                          ^~~~~~~~~~~
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:149:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtDestroyDataItem(acltdtDataItem *dataItem);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:173:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtDestroyDataset(acltdtDataset *dataset);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:201:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtAddDataItem(acltdtDataset *dataset, acltdtDataItem *dataItem);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:236:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtStopChannel(acltdtChannelHandle *handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:280:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtDestroyChannel(acltdtChannelHandle *handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:292:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtCleanChannel(acltdtChannelHandle *handle);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:307:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtSendTensor(const acltdtChannelHandle *handle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:324:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtReceiveTensor(const acltdtChannelHandle *handle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_tdt.h:339:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError acltdtQueryChannelSize(const acltdtChannelHandle *handle, size_t *size);
                     ^~~~~~~~
                     perror
In file included from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:14:0,
                 from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:6:
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:45:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetModelDir(const char *modelDir);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:59:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopLoad(const void *model, size_t modelSize);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:90:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrBool(aclopAttr *attr, const char *attrName, uint8_t attrValue);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:103:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrInt(aclopAttr *attr, const char *attrName, int64_t attrValue);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:116:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrFloat(aclopAttr *attr, const char *attrName, float attrValue);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:129:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrString(aclopAttr *attr, const char *attrName, const char *attrValue);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:142:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrDataType(aclopAttr *attr, const char *attrName, aclDataType attrValue);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:156:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListDataType(aclopAttr *attr, const char *attrName, int numValues,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:171:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListBool(aclopAttr *attr, const char *attrName, int numValues,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:186:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListInt(aclopAttr *attr, const char *attrName, int numValues,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:201:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListFloat(aclopAttr *attr, const char *attrName, int numValues,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:216:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListString(aclopAttr *attr, const char *attrName, int numValues,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:232:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetAttrListListInt(aclopAttr *attr,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:266:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopExecute(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:304:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopExecuteV2(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:330:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCreateHandle(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:367:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopExecWithHandle(aclopHandle *handle,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:388:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCast(const aclTensorDesc *srcDesc,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:407:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCreateHandleForCast(aclTensorDesc *srcDesc,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:431:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCreateKernel(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:454:18: error: ISO C++ forbids declaration of ‘aclError’ with no type [-fpermissive]
 typedef aclError (*aclopCompileFunc)(int numInputs,
                  ^
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:454:18: error: typedef ‘aclError’ is initialized (use decltype instead)
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:454:20: error: ‘aclopCompileFunc’ was not declared in this scope
 typedef aclError (*aclopCompileFunc)(int numInputs,
                    ^~~~~~~~~~~~~~~~
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:473:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopRegisterCompileFunc(const char *opType, aclopCompileFunc func);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:484:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopUnregisterCompileFunc(const char *opType);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:499:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetKernelArgs(aclopKernelDesc *kernelDesc,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:516:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetKernelWorkspaceSizes(aclopKernelDesc *kernelDesc, int numWorkspaces,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:534:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopUpdateParams(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:550:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetMaxOpQueueNum(uint64_t maxOpNum);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:568:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopInferShape(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:588:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopStartDumpArgs(uint32_t dumpType, const char *path);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op.h:599:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopStopDumpArgs(uint32_t dumpType);
                     ^~~~~~~~
                     perror
In file included from /home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:6:0:
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:70:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCompile(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:101:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCompileAndExecute(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:129:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopCompileAndExecuteV2(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:145:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclSetCompileopt(aclCompileOpt opt, const char *value);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:168:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclGetCompileopt(aclCompileOpt opt, char *value, size_t length);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:180:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclopSetCompileFlag(aclOpCompileFlag flag);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:202:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclGenGraphAndDumpForOp(const char *opType,
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/third_party/ascend/include/ascendcl/external/acl/acl_op_compiler.h:230:21: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 ACL_FUNC_VISIBILITY aclError aclDestroyGraphDumpOpt(const aclGraphDumpOption *graphDumpOpt);
                     ^~~~~~~~
                     perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:14:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtSetDevice(int32_t deviceId) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:18:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtGetDevice(int32_t *deviceId) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:22:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtResetDevice(int32_t deviceId) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:26:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtGetCurrentContext(aclrtContext *context) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:30:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtSetCurrentContext(aclrtContext context) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:34:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtMemcpy(void *dst, size_t destMax,
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:41:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtMemcpyAsync(void *dst, size_t destMax,
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:49:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtSynchronizeDevice(void) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:53:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclrtSynchronizeStream(aclrtStream stream) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:61:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError aclGetCompileopt(aclCompileOpt opt, char *value, size_t length) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:69:1: error: ‘aclDataType’ does not name a type; did you mean ‘aclEngineType’?
 aclDataType acltdtGetDataTypeFromItem(const acltdtDataItem *dataItem) {
 ^~~~~~~~~~~
 aclEngineType
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:87:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtGetSliceInfoFromItem(const acltdtDataItem *dataItem, size_t *sliceNum,
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:92:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtGetDimsFromItem(const acltdtDataItem *dataItem, int64_t *dims, size_t dimNum) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:99:38: error: ‘aclDataType’ has not been declared
                                      aclDataType dataType,
                                      ^~~~~~~~~~~
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:106:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtDestroyDataItem(acltdtDataItem *dataItem) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:115:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtDestroyDataset(acltdtDataset *dataset) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:124:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtAddDataItem(acltdtDataset *dataset, acltdtDataItem *dataItem) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:136:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtStopChannel(acltdtChannelHandle *handle) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:147:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtDestroyChannel(acltdtChannelHandle *handle) {
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:151:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtReceiveTensor(const acltdtChannelHandle *handle,
 ^~~~~~~~
 perror
/home/ma-user/work/dips_llm/home/ma-user/work/torchair/cmake/acl/acl_stub.cpp:158:1: error: ‘aclError’ does not name a type; did you mean ‘perror’?
 aclError acltdtQueryChannelSize(const acltdtChannelHandle *handle, size_t *size) {
 ^~~~~~~~
 perror
cc1plus: warning: unrecognized command line option ‘-Wno-class-memaccess’
make[3]: *** [CMakeFiles/acl_stub.dir/build.make:80: CMakeFiles/acl_stub.dir/cmake/acl/acl_stub.cpp.o] Error 1
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[2]: *** [CMakeFiles/Makefile2:718: CMakeFiles/acl_stub.dir/all] Error 2
make[2]: *** Waiting for unfinished jobs....
[ 43%] Linking CXX shared library stubs/libge_local_stub.so
make[3]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
[ 45%] Built target ge_local_stub
make[2]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make[1]: *** [CMakeFiles/Makefile2:234: CMakeFiles/torchair.dir/rule] Error 2
make[1]: Leaving directory '/home/ma-user/work/dips_llm/home/ma-user/work/torchair/build'
make: *** [Makefile:177: torchair] Error 2
","open"
"https://gitee.com/ascend/torchair/issues/IAFEJ9","ascend","torchair","ASCEND_SDK_PATH","ASCEND_SDK_PATH是指什么，是/usr/local/Ascend/ascend-toolkit还是别的？
如果是别的又怎么去获取呢？这个没有在readme.md里体现出来。","open"
"https://gitee.com/ascend/torchair/issues/IA8OYI","ascend","torchair","cmake ..打包失败 File already exists but no hash specified (use URL_HASH)","我的服务器是无法联网的，cmake ..打包时需要在线下载https://gitee.com/openeuler/libboundscheck/repository/archive/v1.1.10.tar.gz该文件，
我离线将该文件下载到指定目录下：/root/wangjianqiang/torchair-master/build/_deps/secure_c-subbuild/secure_c-populate-prefix/src/
重新camke ..打包，显示：
-- File already exists but no hash specified (use URL_HASH):
  file='/root/wangjianqiang/torchair-master/build/_deps/secure_c-subbuild/secure_c-populate-prefix/src/v1.1.10.tar.gz'
Old file will be removed and new file downloaded from URL.
没有指定的hash文件，所以被移除了重新下载。

请问大佬们这个问题该如何解决","open"
"https://gitee.com/ascend/torchair/issues/I9T4FS","ascend","torchair","模型训练时指定自定义融合规则配置文件，训练报错","ImportError: /usr/local/python3.8.0/lib/python3.8/site-packages/torchair/core/_npu_graph_executor.so: undefined symbol: _ZN6at_npu6native14get_npu_formatERKN2at6TensorE","open"
"https://gitee.com/ascend/torchair/issues/I9S3RY","ascend","torchair","在算子 Torch.uniform_ 上报错","

```
NotImplementedError: torch.ops.aten.uniform.default ge_converter is not implemented!

While executing %uniform : [num_users=1] = call_function[target=torch.ops.aten.uniform.default](args = (%zeros,), kwargs = {})
Original traceback:
  File ""/home/pidm/models/unet_autoenc.py"", line 148, in forward
    cond_mask = prob_mask_like((x.shape[0],), prob = prob, device = x.device)
  File ""/home/pidm/models/unet_autoenc.py"", line 17, in prob_mask_like
    return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob

[ERROR] 2024-05-25-05:28:56 (PID:2855, Device:0, RankID:-1) ERR03007 GRAPH feature not supported
```","open"
"https://gitee.com/ascend/torchair/issues/I9FB7K","ascend","torchair","ge_converter/custom自定义算子和torch_npu版本不匹配(以npu_quant_conv2d为例)","https://gitee.com/ascend/torchair/commit/2aa7005c447a1fba2116ab843f7bffaff5bf9b93 该commit增加了`npu_quant_conv2d`的converter，但是torch_npu tag v6.0.rc1.alpha003-pytorch2.1.0并未注册`npu_quant_conv2d`算子：
https://gitee.com/ascend/pytorch/blob/v6.0.rc1.alpha003-pytorch2.1.0/torch_npu/meta/meta_registrations.py
在后来的commit才添加：
https://gitee.com/ascend/pytorch/commit/b71fd6bbca0c744d007edd394e5ae4e84671bb6b

因此如果使用torch_npu v6.0.rc1.alpha003 + 最新torchair源码，运行时会导致错误：

```
torch._dynamo.exc.BackendCompilerFailed: backend='functools.partial(<function _npu_backend at 0x7f4717ba9ea0>, compiler_config=<torchair.configs.compiler_config.CompilerConfig object at 0x7f4733659330>, decompositions={})' raised:
AttributeError: '_OpNamespace' 'npu' object has no attribute 'npu_quant_conv2d'

While executing %npu_rms_norm : [num_users=1] = call_function[target=torch.ops.npu.npu_rms_norm.default](args = (%embedding, %arg0_1, 1e-05), kwargs = {})

```

由于v6.0.rc1.alpha003-pytorch2.1.0 + CANN 8.0.RC1.alpha003是当前外部能够下载的最新版本
(https://www.hiascend.com/developer/download/community/result?module=cann+pt ，发布时间2024/03/19)，只能把torchair checkout到更早的commit，以规避torch_npu相应版本中没有的自定义算子。

给torchair加上和torch_npu对应的release tag，能更好地避免这类问题？","open"
"https://gitee.com/ascend/torchair/issues/I9F9C8","ascend","torchair","修复 npu_tuned_model 走greedy_search分支调用compile","`torch.compile`只添加在了`greedy_search`中：
https://gitee.com/ascend/torchair/blob/f7eac66304904b8ed6ce46f3ba3e9420c7ef043e/npu_tuned_model/llm/models/common/utils.py#L2365

`model.generate`默认走的`sample`，要改成`do_sample=False`才能调用到compile:
https://gitee.com/ascend/torchair/blob/f7eac66304904b8ed6ce46f3ba3e9420c7ef043e/npu_tuned_model/llm/runner/llm_runner.py#L117

```python
kwargs_params[""do_sample""] = False
generate_ids = self.model.generate(**kwargs_params)
```

否则，现在的脚本默认只跑的Eager + sample

或者，更简单的改法是不动utils.py，直接把torch.compile加在最外层：
```python
model.forward = torch.compile(model.forward, dynamic=False, fullgraph=True, backend=npu_backend)
generate_ids = model.generate(...)
```
","open"
"https://gitee.com/ascend/torchair/issues/I9EZM5","ascend","torchair","get_npu_format接口变更后导致CANN 8.0.RC1.alpha003环境中运行出错","https://gitee.com/ascend/torchair/pulls/659 (对应commit https://gitee.com/ascend/torchair/commit/59ddc20ad24229cce35c9c5995630fe21390ec50) 导致
最新社区版CANN8.0.RC1.alpha003 (发行日期2024/03/19) 环境下运行报错：

```
======================================================================
ERROR: test_npu_graph_executor_func (torchair_st.TorchairSt)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/agent/workspace/UT_Test/torchair/tests/build/st/torchair_st.py"", line 773, in test_npu_graph_executor_func
    from torchair.core import _npu_graph_executor
ImportError: /home/jenkins/agent/workspace/UT_Test/torchair/tests/build/ut/torchair/core/_npu_graph_executor.so: undefined symbol: _ZN6at_npu6native14get_npu_formatERKN2at6TensorEb
```

同样报错信息在该PR[第一个fail的UT里](https://gitee.com/ascend/torchair/pulls/659#note_26051957_conversation_107424919)可以找到。Log文件: https://container-obsfs-filesystem.obs.cn-north-4.myhuaweicloud.com/package/ascend/torchair/compile/202403/20240326/xudaohong_659_1568_20240326194559_a9157f99cbd1b94a8b693df063df393c8a59fdba/jenkins_log/UT_Test.txt

回滚到前一个commit(https://gitee.com/ascend/torchair/commit/3a3a7bcb7421381be98988fd8281c61c0da638e9)就可以正常运行。

get_npu_format接口修改应该还没包含在对外发行的CANN版本里","open"
"https://gitee.com/ascend/torchair/issues/I9E6MS","ascend","torchair","修复npu_tuned_model/llm用例中不支持的NPU融合算子","[新增llm example](https://gitee.com/ascend/torchair/commit/f7eac66304904b8ed6ce46f3ba3e9420c7ef043e)里调用的部分NPU自定义算子，在CANN对外商用版本([CANN 7.0.1.1](https://support.huawei.com/enterprise/zh/ascend-computing/cann-pid-251168373/software/262097058) + [v5.0.1.1-pytorch2.1.0](https://gitee.com/ascend/pytorch/releases/tag/v5.0.1.1-pytorch2.1.0) 发布时间2024-03-11)中尚不支持（仅在内部CANN包支持，但对外不可用）。回滚到旧算子可暂时规避。

在[modeling_llama.py](https://gitee.com/ascend/torchair/blob/f7eac66304904b8ed6ce46f3ba3e9420c7ef043e/npu_tuned_model/llm/models/llama2/modeling_llama.py)中：

- `LlamaRMSNorm.forward`中，将
```python
y, _, x = torch_npu.npu_add_rms_norm(residual, hidden_states, self.weight, self.variance_epsilon)
```

倒退回
```python
x = residual + hidden_states
y = torch_npu.npu_rms_norm(x, self.weight, self.variance_epsilon)[0]
```

- `apply_rotary_pos_emb`中，将
```python
return torch_npu.npu_apply_rotary_pos_emb(q, k, cos, sin)
```

倒退回
```python
q_embed = torch_npu.npu_rotary_mul(q, cos, sin)
k_embed = torch_npu.npu_rotary_mul(k, cos, sin)
return q_embed, k_embed
```

即可正常运行。

否则遇到错误`AttributeError: module 'torch_npu' has no attribute 'npu_add_rms_norm'`和`AttributeError: module 'torch_npu' has no attribute 'npu_apply_rotary_pos_emb'`

在社区版 [8.0.RC1.alpha002](https://gitee.com/ascend/pytorch/tree/v6.0.rc1.alpha002-pytorch2.1.0)，存在`torch_npu.npu_apply_rotary_pos_emb`接口，但仍然无法正常运行： https://gitee.com/ascend/op-plugin/issues/I9E41Q 。 在外部环境运行时只能先用旧算子。","open"
"https://gitee.com/ascend/torchair/issues/I9C0LD","ascend","torchair","图模式conv2dbackward，fp16场景精度不达标，误差千分之一以内","图模式conv2dbackward，fp16场景精度不达标，误差千分之一以内","open"
"https://gitee.com/ascend/torchair/issues/I9C0L2","ascend","torchair","图模式addmm float16存在误差,导致模型累计误差较大","图模式addmm float16存在误差,导致模型累计误差较大","open"
"https://gitee.com/ascend/torchair/issues/I949JF","ascend","torchair","编译报错","
```
[ 31%] Building CXX object CMakeFiles/hccl_stub.dir/cmake/hccl/hccl_stub.cpp.o
[ 33%] Building CXX object CMakeFiles/acl_stub.dir/cmake/acl/acl_stub.cpp.o
/home/HwHiAiUser/Ascend_Drive/torchair-master/cmake/acl/acl_stub.cpp:1:10: fatal error: acl/acl_rt.h: No such file or directory
    1 | #include ""acl/acl_rt.h""
      |          ^~~~~~~~~~~~~~
compilation terminated.
/home/HwHiAiUser/Ascend_Drive/torchair-master/cmake/hccl/hccl_stub.cpp:1:10: fatal error: hccl/hccl_types.h: No such file or directory
    1 | #include ""hccl/hccl_types.h""
      |          ^~~~~~~~~~~~~~~~~~~
compilation terminated.
make[3]: *** [CMakeFiles/acl_stub.dir/build.make:94: CMakeFiles/acl_stub.dir/cmake/acl/acl_stub.cpp.o] Error 1
make[2]: *** [CMakeFiles/Makefile2:426: CMakeFiles/acl_stub.dir/all] Error 2
make[2]: *** Waiting for unfinished jobs....
make[3]: *** [CMakeFiles/hccl_stub.dir/build.make:94: CMakeFiles/hccl_stub.dir/cmake/hccl/hccl_stub.cpp.o] Error 1
make[2]: *** [CMakeFiles/Makefile2:504: CMakeFiles/hccl_stub.dir/all] Error 2
[ 33%] Built target copy_pyfiles
[ 33%] Built target fmk_parser
[ 33%] Built target graph
[ 33%] Built target acl_op_compiler
[ 33%] Built target ascend_protobuf
[ 33%] Built target acl_tdt_channel
make[1]: *** [CMakeFiles/Makefile2:645: CMakeFiles/torchair.dir/rule] Error 2
make: *** [Makefile:390: torchair] Error 2
```

","open"
"https://gitee.com/ascend/torchair/issues/I8NFLI","ascend","torchair","融合决策：支持inductor中can_fuse判断",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YY","ascend","torchair","Autofuse/codegen：支持生成kernel侧代码",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YV","ascend","torchair","Backend：支持can_fuse进行融合范围决策",,"open"
"https://gitee.com/ascend/torchair/issues/I949A4","ascend","torchair","编译后跑`python example.py` undefined_symbol","1. 编译时没有set ASCEND_SDK_PATH 
2. 编译后跑`python example.py`

> lib/python3.8/site-packages/torchair/core/_torchair.so: undefined symbol: _ZN2ge7Session39UpdateGraphRefreshableFeatureMemoryBaseEjPKvm

---
ascend-toolkit/latest/version.cfg

```bash
runtime_running_version=[7.1.0.3.220:7.0.0]
compiler_running_version=[7.1.0.3.220:7.0.0]
opp_running_version=[7.1.0.3.220:7.0.0]
toolkit_running_version=[7.1.0.3.220:7.0.0]
aoe_running_version=[7.1.0.3.220:7.0.0]
ncs_running_version=[7.1.0.3.220:7.0.0]
runtime_upgrade_version=[7.1.0.3.220:7.0.0]
compiler_upgrade_version=[7.1.0.3.220:7.0.0]
opp_upgrade_version=[7.1.0.3.220:7.0.0]
toolkit_upgrade_version=[7.1.0.3.220:7.0.0]
aoe_upgrade_version=[7.1.0.3.220:7.0.0]
ncs_upgrade_version=[7.1.0.3.220:7.0.0]
runtime_installed_version=[7.1.0.3.220:7.0.0]
compiler_installed_version=[7.1.0.3.220:7.0.0]
opp_installed_version=[7.1.0.3.220:7.0.0]
toolkit_installed_version=[7.1.0.3.220:7.0.0]
aoe_installed_version=[7.1.0.3.220:7.0.0]
ncs_installed_version=[7.1.0.3.220:7.0.0]
```
---
ascend-toolkit/latest/toolkit/version.info
```bash
Version=7.1.0.3.220
version_dir=7.0.0
```
","open"
"https://gitee.com/ascend/torchair/issues/I93WIU","ascend","torchair","在 npu 上运行编译后的模型报错","我按照[官网](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC1alpha001/softwareinst/instg/instg_0001.html)的指南在一台8卡910B机器上安装了最新版本的 cann 框架和 2.1.0 的支持 npu 的 pytorch 框架，然后按照 torchair 的 readme 上的指南编译了 torchair 并安装。在运行 `example.py` 或者是 `example_llama.py` 等测例时都可以正常进行图编译，但是当我运行 `example_npu_dynamic_executor.py` 这个测例时，需要在 npu 上运行编译好的模型，会出现如下的报错：
```bash
Traceback (most recent call last):
  File ""/home/huangsh/Work/torchair/examples/example_npu_dynamic_executor.py"", line 36, in <module>
    graph_result = model(in1, in2, in3)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/huangsh/Work/torchair/examples/example_npu_dynamic_executor.py"", line 23, in forward
    def forward(self, x, y, z):
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 3905, in forward
    return compiled_fn(full_args)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 1482, in g
    return f(*args)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 2533, in runtime_wrapper
    all_outs = call_func_with_args(
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 1594, in rng_functionalization_wrapper
    return compiled_fw(args)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py"", line 1482, in g
    return f(*args)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torchair/npu_fx_compiler.py"", line 394, in inference
    compiled_result = npu_compiled_gm(*args, **kwargs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torchair/ge_concrete_graph/fx2ge_converter.py"", line 420, in __call__
    self.load(inputs)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torchair/ge_concrete_graph/fx2ge_converter.py"", line 476, in load
    initialize_graph_engine(global_compile_options)
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torchair/core/backend.py"", line 73, in initialize_graph_engine
    torch_npu_device = _try_get_torch_npu_device()
  File ""/home/huangsh/.local/lib/python3.9/site-packages/torchair/core/backend.py"", line 24, in _try_get_torch_npu_device
    from . import _npu_graph_executor
ImportError: /home/huangsh/.local/lib/python3.9/site-packages/torchair/core/_npu_graph_executor.so: undefined symbol: _ZN6at_npu6native14get_npu_formatERKN2at6TensorE
```

另外，在启动程序时，有如下的 warning，我不清楚这是否有帮助。
```bash
/home/huangsh/.local/lib/python3.9/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
```","open"
"https://gitee.com/ascend/torchair/issues/I91O14","ascend","torchair","torch._dynamo.exc.Unsupported: npu stream","Torchair doesn't seem to support wait_stream.wait_stream(), which is essential for synchronizing NPU streams.

The code:

```
    def forward(self, x):
        stream1 = torch.npu.Stream(device=self.device)
        with torch.npu.stream(stream1):
            y1 = self.ff1(x)

        stream2 = torch.npu.Stream(device=self.device)
        with torch.npu.stream(stream2):
            y2 = self.ff2(x)

        stream2.wait_stream(stream1)

        return y1+y2
```


The error:




> torch._dynamo.exc.Unsupported: npu stream
> 
> from user code:
>    File ""llm.py"", line 53, in forward
>     x = layer(x)
>   File ""/root/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
>     return forward_call(*args, **kwargs)
>   File ""llm.py"", line 234, in forward
>     stream2.wait_stream(stream1)

Versions:


> torchair: 0.1 (built from source)
> torch_npu: 2.1.0+gitb62d2b9
> torch: 2.1.0
> python: 3.8.18


","open"
"https://gitee.com/ascend/torchair/issues/I9099V","ascend","torchair","编译openllama模型时报错不支持Sxpy算子","一、问题现象（附报错日志上下文）：
编译openllama模型时报错不支持Sxpy算子

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  7.0.0.beta1
--Tensorflow/Pytorch/MindSpore 版本:  torch 2.1.0 torch-npu 2.1.0
--Python 版本 (e.g., Python 3.7.5): 3.8.10
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04): EulerOS 2.0

三、测试步骤：
xxxx


四、日志信息:
/usr/local/lib/python3.8/dist-packages/torch_npu/contrib/transfer_to_npu.py:208: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************

  warnings.warn(msg, ImportWarning)
WARNING 01-31 11:26:37 model.py:18] Mindspore lite is not installed, if you want to use mindspore lite to infer, please install mindspore lite first.
WARNING 01-31 11:26:37 default_config.py:357] Casting torch.float16 to torch.bfloat16.
INFO 01-31 11:26:37 ascend_llm_engine.py:84] Initializing an LLM engine with backend: pytorch
INFO 01-31 11:26:37 ascend_llm_engine.py:85] Initializing an LLM engine with config: model='/tmp-ms/xsc/vllm/model/openllma-7b', tokenizer='/tmp-ms/xsc/vllm/model/openllma-7b', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
INFO 01-31 11:27:18 llm_engine.py:205] # GPU blocks: 4, # CPU blocks: 0
[W AddKernelNpu.cpp:80] Warning: The oprator of add is executed, Currently High Accuracy but Low Performance OP with 64-bit has been used, Please Do Some Cast at Python Functions with 32-bit for Better Performance! (function operator())
Traceback (most recent call last):
  File ""offline_inference.py"", line 22, in <module>
    llm = LLM(model=""/tmp-ms/xsc/vllm/model/openllma-7b"",
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/entrypoints/llm.py"", line 89, in __init__
    self.llm_engine = AscendLLMEngine.from_engine_args(engine_args)
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/engine/ascend_llm_engine.py"", line 285, in from_engine_args
    engine = cls(*engine_configs,
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/engine/ascend_llm_engine.py"", line 137, in __init__
    self.warmup_generate()
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/engine/ascend_llm_engine.py"", line 271, in warmup_generate
    self.step()
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/engine/llm_engine.py"", line 553, in step
    output = self._run_workers(
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/engine/llm_engine.py"", line 691, in _run_workers
    output = executor(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/worker/torch_ge_worker.py"", line 65, in execute_model
    output = self.model(**model_inputs, input_metadata=input_metadata)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/model_executor/backend_warpper/ge.py"", line 36, in forward
    logits = self.base_model(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/tmp-ms/xsc/vllm/ascend-vllm/vllm/model_executor/models/llama/llama_base.py"", line 373, in forward
    def forward(self, input_ids: torch.LongTensor,
  File ""/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py"", line 328, in _fn
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_dynamo/external_utils.py"", line 17, in inner
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 3905, in forward
    return compiled_fn(full_args)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 1482, in g
    return f(*args)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 2533, in runtime_wrapper
    all_outs = call_func_with_args(
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 1506, in call_func_with_args
    out = normalize_as_list(f(args))
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 1594, in rng_functionalization_wrapper
    return compiled_fw(args)
  File ""/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py"", line 1482, in g
    return f(*args)
  File ""/usr/local/lib/python3.8/dist-packages/torch_npu/dynamo/torchair/npu_fx_compiler.py"", line 369, in inference
    compiled_result = npu_compiled_gm(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch_npu/dynamo/torchair/ge_concrete_graph/fx2ge_converter.py"", line 320, in __call__
    self.compile()
  File ""/usr/local/lib/python3.8/dist-packages/torch_npu/dynamo/torchair/ge_concrete_graph/fx2ge_converter.py"", line 388, in compile
    self._executor.compile()
  File ""/usr/local/lib/python3.8/dist-packages/torch_npu/dynamo/torchair/core/backend.py"", line 102, in compile
    return super(TorchNpuGraph, self).compile()
RuntimeError: EZ3003: No supported Ops kernel and engine are found for [Sxpy], optype [Sxpy].
        Possible Cause: The operator is not supported by the system. Therefore, no hit is found in any operator information library.
        Solution: 1. Check that the OPP component is installed properly. 2. Submit an issue to request for the support of this operator type.
        TraceBack (most recent call last):
        No supported Ops kernel and engine are found for [Sxpy_1], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_10], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_11], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_12], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_13], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_14], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_15], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_16], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_17], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_18], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_19], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_2], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_20], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_21], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_22], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_23], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_24], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_25], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_26], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_27], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_28], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_29], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_3], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_30], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_31], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_4], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_5], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_6], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_7], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_8], optype [Sxpy].
        No supported Ops kernel and engine are found for [Sxpy_9], optype [Sxpy].
        [Call][PreRun] Failed, graph_id:0, session_id:6.[FUNC:CompileGraph][FILE:graph_manager.cc][LINE:4116]
        [Compile][Graph]Compile graph failed, error code:1343225857, session_id:6, graph_id:0.[FUNC:CompileGraph][FILE:ge_api.cc][LINE:1150]
","open"
"https://gitee.com/ascend/torchair/issues/I8PQ3P","ascend","torchair","tiling策略：B+R",,"open"
"https://gitee.com/ascend/torchair/issues/I8PQ1B","ascend","torchair","B+R融合判断",,"open"
"https://gitee.com/ascend/torchair/issues/I8PH9W","ascend","torchair","Autofuse/codegen：支持stride表达的指令映射",,"open"
"https://gitee.com/ascend/torchair/issues/I8PH8N","ascend","torchair","Autofuse/codegen：支持多模板和tiling选择生成",,"open"
"https://gitee.com/ascend/torchair/issues/I8NXXJ","ascend","torchair","多模板：tiling切分多模板",,"open"
"https://gitee.com/ascend/torchair/issues/I8NFRR","ascend","torchair","计算优化：多计算合并和拆分，指令选择",,"open"
"https://gitee.com/ascend/torchair/issues/I8NFR2","ascend","torchair","workspace节点识别和设置",,"open"
"https://gitee.com/ascend/torchair/issues/I8NFPJ","ascend","torchair","节点scope设置：各层级buffer设置","是否只标记local buffer，还是需要标记VECIN等","open"
"https://gitee.com/ascend/torchair/issues/I8N5YL","ascend","torchair","E2E：支持llama2的执行",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YJ","ascend","torchair","E2E：支持flashattention的Inductor计算（验证矩阵向量混合计算）",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YI","ascend","torchair","E2E：支持softmax的Inductor计算（验证纯vector计算",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YH","ascend","torchair","E2E：支持layernorm的Inductor计算（验证纯vector计算）",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YG","ascend","torchair","E2E：验证llama2的融合性能",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YF","ascend","torchair","E2E：支持llama2的执行",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YE","ascend","torchair","E2E：支持Inductor的端到端场景",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YD","ascend","torchair","AscendC：提供高阶Api的内存/性能评估公式",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YC","ascend","torchair","AscendC：提供通过ascir的stride表达映射到Api代码接口",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5YB","ascend","torchair","AscendC：扩展高阶Api在Ascend910/Ascend310上的高阶Api信息",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5Y9","ascend","torchair","Autofuse/ascir：定义高阶Api原型",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5XP","ascend","torchair","Autofuse：支持生成AscendC的proto/tiling/kernel代码",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5XO","ascend","torchair","Autofuse：支持Optimize优化补充代码生成所需信息",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5WQ","ascend","torchair","AscendC：扩展AscendC进行高阶Api注册和映射",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5WN","ascend","torchair","Autofuse：定义ascir并提供构图接口",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5VV","ascend","torchair","Doc：支持Inductor的设计和使用文档",,"open"
"https://gitee.com/ascend/torchair/issues/I8N5V1","ascend","torchair","Backend：支持Inductor的AscendC后端",,"open"
"https://gitee.com/ascend/torchair/issues/I8N0CT","ascend","torchair","Autofuse：支持自动融合和代码生成",,"open"
"https://gitee.com/ascend/torchair/issues/I8MNDU","ascend","torchair","支持Inductor","### 动机

Inductor是Pytorch 2.0的重要特性，
`这里输入代码`可以尝试通过Inductor进行融合，
并通过AscendC生成融合后的算子来实现模型的加速。

### 如何开发

我想在这个issue中描述下整体想法，
但是对接inductor应该会有很多工作，
因此我也会创建很多issue描述来跟踪不同的部分。

但这个Issue将会是整体的入口，
我也会在这个issue去关联各个其他部分的issue。

我会设置一些E2E的case，给出期望的输入输出，用于各个模块进行对其和合作：
- #I8N09K：单输入单输出的Abs

### 整体方案

### 术语对照

我希望未来能和国内外各个开发者、高校、实验室合作，
因此维护一套准确的中英对照将会方便我们的交流。
","open"
"https://gitee.com/ascend/tensorflow/issues/I5BQ0C","ascend","tensorflow","Bazel 这一套是不是没有在使用了","请问通过build.sh构建生成whl这个流程是不是已经没有使用bazel BUILD这一套了？","open"
"https://gitee.com/ascend/tensorflow/issues/I6YQTR","ascend","tensorflow","【济南AICC】unet模型用Tensorflow框架训在npu练训练，AIcore利用率是0","一、问题现象（附报错日志上下文）：
![输入图片说明](https://foruda.gitee.com/images/1682406476154206277/c805e1ac_9812251.png ""微信截图_20230425150725.png"")

二、软件版本:
-- CANN 版本 ： CANN 5.1.RC1.1
--Tensorflow 版本: TensorFlow-1.15.0
--Python 版本 ： Python 3.7.10
--操作系统版本 ：EulerOS release 2.0 (SP8)
--架构：aarch64

三、测试步骤：
准备好官网数据集。执行执行train.py 


四、日志信息:
![输入图片说明](https://foruda.gitee.com/images/1682406686116503939/84956432_9812251.png ""屏幕截图"")

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tensorflow/issues/I6Y2YU","ascend","tensorflow","版本对应表","每个分支版本斗没有对应的cann版本，不是内部人员完全用不了 





","open"
"https://gitee.com/ascend/tensorflow/issues/I6OZP4","ascend","tensorflow","npu迁移tensorflow脚本后图报错","一、问题现象（附报错日志上下文）：
迁移前的训练代码：
https://openi.pcl.ac.cn/kewei/tensorflow1.15-npu/src/branch/master/estimator/Tutorial_TensorFlow_from_Estimators_to_Keras.ipynb
可以正常跑通
迁移后的训练代码：
https://openi.pcl.ac.cn/kewei/tensorflow1.15-npu/src/branch/master/estimator/Tutorial_TensorFlow_from_Estimators_to_Keras-npu.ipynb
报错：
InternalError: GeOp21_0GEOP::::DoRunAsync Failed
Error Message is : 
EZ9999: Inner Error!
EZ9999  The error from device(0), serial number is 1, there is an aicore error, core id is 2, error code = 0x800000, dump info: pc start: 0x1000120042580000, current: 0x120042587040, vec error info: 0xbffc9c5, mte error info: 0x61520cd, ifu error info: 0x2dffcbab37f00, ccu error info: 0x0, cube error info: 0xb9, biu error info: 0x0, aic error mask: 0x65000200d000288, para base: 0x12004021f800.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:364]
        The extend info from device(0), serial number is 1, there is an aicore error, core id is 2, aicore int: 0x1, aicore error2: 0x0, axi clamp ctrl: 0x0, axi clamp state: 0x1717, biu status0: 0x101e44800000000, biu status1: 0x940002092a0000, clk gate mask: 0x1000, dbg address: 0x0, ecc en: 0x0, mte ccu ecc 1bit error: 0x0, vector cube ecc 1bit error: 0x0, run stall: 0x1, dbg data0: 0x0, dbg data1: 0x0, dbg data2: 0x0, dbg data3: 0x0 dfx data: 0x0. [FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:393]
        The device(0), core list[0-0], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:414]
        coreId( 0):        0x800000    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:425]
        Aicore kernel execute failed, device_id=0, stream_id=69, report_stream_id=69, task_id=21707, flip_num=0, fault kernel_name=755208_1679381164278909_0_81_head/Adam/update_input_layer/relationship_embedding/embedding_weights/UnsortedSegmentSum, func_name=te_unsortedsegmentsum_9c88e6c0bbfef4361ef7d638a4c83c9e25267f74dfa1465ad15698da09fe14da_1__kernel0, program id=532, hash=12950126341868538522[FUNC:GetError][FILE:stream.cc][LINE:741]
        Aicore kernel execute failed, device_id=0, stream_id=69, report_stream_id=69, task_id=21711, flip_num=0, fault kernel_name=755208_1679381164275238_0_81_head/Adam/update_input_layer/marital_status_embedding/embedding_weights/UnsortedSegmentSum, func_name=te_unsortedsegmentsum_9c88e6c0bbfef4361ef7d638a4c83c9e25267f74dfa1465ad15698da09fe14da_1__kernel0, program id=519, hash=12950126341868538522[FUNC:GetError][FILE:stream.cc][LINE:741]
        rtStreamSynchronize execute failed, reason=[aicore exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:45]
        invoke rtStreamSynchronize failed, ret = 507015[FUNC:Synchronize][FILE:hybrid_execution_context.cc][LINE:91]
        failed to execute graph. model_id = 7[FUNC:HandleResult][FILE:hybrid_model_async_executor.cc][LINE:280]

	 [[{{node GeOp21_0}}]]

During handling of the above exception, another exception occurred:

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  5.1.RC1.1
--Tensorflow版本:1.15.0
--Python 版本 (e.g., Python 3.7.5):3.7.6
--操作系统版本 (e.g., Ubuntu 18.04):eulerosv2r8.aarch64

三、测试步骤：
运行上述代码即可。


","open"
"https://gitee.com/ascend/tensorflow/issues/I6OCV7","ascend","tensorflow","模型（tensorflow）迁移之后，在notebook上训练，AIcore使用率只有2%，每epoch的训练耗时是迁移之前的3倍","### 该问题是怎么引起的？
模型（tensorflow）迁移之后，在notebook上训练，AIcore使用率只有2%，每epoch的训练耗时是迁移之前的3倍

迁移之后在NPU上的耗时：
![输入图片说明](https://foruda.gitee.com/images/1679275478722768374/e1e465fc_1241261.png ""屏幕截图"")

![输入图片说明](https://foruda.gitee.com/images/1679275324142475841/aa47da7f_1241261.png ""屏幕截图"")

迁移之前在GPU上耗时：
![输入图片说明](https://foruda.gitee.com/images/1679275433541795844/65bab9a9_1241261.png ""屏幕截图"")


问题：
代码中已将NPU设置为默认设备，日志打印中确只能获取的CPU的device，是否因为没有用到NPU导致训练耗时长，但是NPU又有2%的使用率
![输入图片说明](https://foruda.gitee.com/images/1679275626955231101/da02146b_1241261.png ""屏幕截图"")

![输入图片说明](https://foruda.gitee.com/images/1679275645336232519/c606e80e_1241261.png ""屏幕截图"")



","open"
"https://gitee.com/ascend/tensorflow/issues/I6OC8G","ascend","tensorflow","编译失败","一、问题现象（附报错日志上下文）：
根据操作指导编译失败
[ 25%] Building CXX object compat_v1/CMakeFiles/_tf_adapter.dir/home/chenqiong/tensorflow/tf_adapter/kernels/aicore/k_means_centroids_v2.cc.o
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp: In lambda function:
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:300:27: error: 'aclrtFloatOverflowMode' was not declared in this scope
     aclrtSetDeviceSatMode(aclrtFloatOverflowMode(mode));
                           ^~~~~~~~~~~~~~~~~~~~~~
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:300:5: error: 'aclrtSetDeviceSatMode' was not declared in this scope
     aclrtSetDeviceSatMode(aclrtFloatOverflowMode(mode));
     ^~~~~~~~~~~~~~~~~~~~~
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:300:5: note: suggested alternative: 'aclrtSetDevice'
     aclrtSetDeviceSatMode(aclrtFloatOverflowMode(mode));
     ^~~~~~~~~~~~~~~~~~~~~
     aclrtSetDevice
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp: In lambda function:
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:304:5: error: 'aclrtFloatOverflowMode' was not declared in this scope
     aclrtFloatOverflowMode mode = ACL_RT_OVERFLOW_MODE_UNDEF;
     ^~~~~~~~~~~~~~~~~~~~~~
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:305:43: error: 'mode' was not declared in this scope
     aclError ret = aclrtGetDeviceSatMode(&mode);
                                           ^~~~
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:305:43: note: suggested alternative: 'modf'
     aclError ret = aclrtGetDeviceSatMode(&mode);
                                           ^~~~
                                           modf
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:305:20: error: 'aclrtGetDeviceSatMode' was not declared in this scope
     aclError ret = aclrtGetDeviceSatMode(&mode);
                    ^~~~~~~~~~~~~~~~~~~~~
/home/chenqiong/tensorflow/tf_adapter_2.x/npu_device/core/npu_wrapper.cpp:305:20: note: suggested alternative: 'aclrtGetDeviceCount'
     aclError ret = aclrtGetDeviceSatMode(&mode);
                    ^~~~~~~~~~~~~~~~~~~~~
                    aclrtGetDeviceCount
[ 26%] Building CXX object compat_v1/CMakeFiles/_tf_adapter.dir/home/chenqiong/tensorflow/tf_adapter/kernels/aicore/lamb_apply_optimizer_assign.cc.o
[ 26%] Building CXX object compat_v1/CMakeFiles/_tf_adapter.dir/home/chenqiong/tensorflow/tf_adapter/kernels/aicore/lamb_apply_weight_assign.cc.o


二、软件版本:
-- CANN 版本  6.0.RC1 
--Tensorflow版本: 2.6
--Python 版本: 3.7.5
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
./configure
/usr/local/Ascend/ascend-toolkit/latest
mkdir build
cd build
cmake ..
make -j16

四、日志信息:
","open"
"https://gitee.com/ascend/tensorflow/issues/I6N8EU","ascend","tensorflow","【AICC】reduce_sum算子输入越界","一、问题现象（附报错日志上下文）：
在昇腾910上运行tf1.15代码报错，日志反馈reduce_sum算子输入越界
![输入图片说明](https://foruda.gitee.com/images/1678848659700134088/b0de4110_10032685.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x): 5.1RC1.1
--Tensorflow版本: 1.15.0
--Python 版本 (e.g., Python 3.7.5):3.7.10
--操作系统版本 (e.g., Ubuntu 18.04):euleros v2r8

三、测试步骤：
在Atlas800 9000训练环境上运行代码
![输入图片说明](https://foruda.gitee.com/images/1678849196217773369/0fdfcd08_10032685.png ""屏幕截图"")

四、日志信息:
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.746  ===========FOLLOWING IS ARGUMENTS OF NODE gradients/capsule_network/prim_cap/Tile_grad/Sum, TYPE ReduceSumD=============
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.763  input0: {'shape': (1, 128, 1, 1152, 10, 1, 1, 8, 1, 1), 'ori_shape': (1, 128, 1, 1152, 10, 1, 1, 8, 1, 1), 'format': 'ND', 'sub_format': 0, 'ori_format': 'ND', 'dtype': 'float32', 'addr_type': 0, 'total_shape': [1, 128, 1, 1152, 10, 1, 1, 8, 1, 1], 'slice_offset': (), 'L1_addr_offset': 0, 'L1_fusion_type': -1, 'L1_workspace_size': -1, 'valid_shape': (), 'split_index': 0}
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.774  outputs0: {'shape': (128, 1152, 1, 8, 1), 'ori_shape': (128, 1152, 1, 8, 1), 'format': 'ND', 'sub_format': 0, 'ori_format': 'ND', 'dtype': 'float32', 'addr_type': 0, 'total_shape': [128, 1152, 1, 8, 1], 'slice_offset': (), 'L1_addr_offset': 0, 'L1_fusion_type': -1, 'L1_workspace_size': -1, 'valid_shape': (), 'split_index': 0}
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.781  Attributes are: {{'name': 'axes', 'dtype': 'list_int64', 'value': (0, 2, 4, 6, 8)}, {'name': 'keep_dims', 'dtype': 'bool', 'value': False}}
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.784  ===========FOLLOWING IS STACK INFO OF NODE gradients/capsule_network/prim_cap/Tile_grad/Sum=============
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.795  Stack: 
  Traceback (most recent call last):
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/te_fusion/compile_task_manager.py"", line 231, in run
    extra_params=self._extra_params)
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/te_fusion/fusion_manager.py"", line 1272, in build_single_op
    compile_info = call_op()
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/te_fusion/fusion_manager.py"", line 1259, in call_op
    opfunc(*inputs, *outputs, *new_attrs, **kwargs)
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 537, in _in_wrapper
    formal_parameter_list[i][1], op_name)
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 516, in _check_one_op_param
    _check_input(op_param, param_name, param_type, op_name)
  
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.804  Stack: 
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 299, in _check_input
    _check_input_output_dict(op_param, param_name, op_name)
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 223, in _check_input_output_dict
    param_name=param_name)
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 689, in check_shape
    _check_shape_range(max_rank, min_rank, param_name, shape)
  
[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.807  Stack: 
  File ""/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/utils/para_check.py"", line 727, in _check_shape_range
    % (error_info['param_name'], min_rank, max_rank, len(shape)))
RuntimeError: ({'errCode': 'E80012', 'op_name': 'reduce_sum_d', 'param_name': 'x', 'min_value': 0, 'max_value': 8, 'real_value': 10}, 'In op, the num of dimensions of input/output[x] should be inthe range of [0, 8], but actually is [10].')

[ERROR] TEFUSION(163974,python):2023-03-15-10:54:39.349.810  ===========END OF DETAILED OP INFO OF NODE gradients/capsule_network/prim_cap/Tile_grad/Sum=============","open"
"https://gitee.com/ascend/tensorflow/issues/I6GNCW","ascend","tensorflow","mt客户自定义tensorflow编包问题汇总","### 该问题是怎么引起的？

使用产品线CANN包+客户自定义tensorflow，会出现不适配的问题，解决办法是用客户tensorflow源码编包，编包过程遇到一下问题

### 重现步骤
1）检查基础条件
![输入图片说明](https://foruda.gitee.com/images/1676964121106689916/d4b3df9d_7745758.png ""屏幕截图"")

CMake 3.14.0当时未找到，只找到了CMake 3.12.0
SWIG安装依赖PCRE，未提供PCRE的说明，版本也没有说明
中间升级了一次SWIG
2）CMakeList.txt编译项
执行build.sh后，出现各种错误提示：
[图片上传中…(image-tomCoaoFEA9Bq4Ya8nFD)]
[图片上传中…(image-OqP3pz0w5fDrzP4V42AZ)]
3）解决了编译错误之后，会出现错误
No rule to make target ""/usr/local/Ascend/compiler/lib64/libge_runner.so'
需要安装toolkit包（产品线）
安装toolkit依赖驱动
[图片上传中…(image-Vv7l0ksUN1LNxTFFBaFe)]
退出容器重新挂载驱动
4）编包成功，安装后import npu_bridge,出现如下错误：
[图片上传中…(image-ODvLROyINriKWzrNBVDO)]
需要升级toolkit
5）npu_bridgexxx.whl在原训练镜像中更新，再次出现不匹配问题

### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I6D3Z4","ascend","tensorflow","将tensorflow FFT移植到Ascend310上运行","### 该问题是怎么引起的？
想在Ascend310芯片上跑下快速傅里叶变化，看到tf框架已经支持fft，有没有个demo可以跑到NPU上？




","open"
"https://gitee.com/ascend/tensorflow/issues/I69D0D","ascend","tensorflow","ARM芯片没有对应的1.15版本TF","![输入图片说明](https://foruda.gitee.com/images/1673083006714285442/dd3dacf7_12301488.png ""屏幕截图"")
是我理解有误吗？请问对于CPU为ARM架构的昇腾服务器，应该如何安装TF","open"
"https://gitee.com/ascend/tensorflow/issues/I698B8","ascend","tensorflow","BertTnews_for_ACL下载FineTuning代码参考链接不对，没有跳转到finetuning代码下载界面","### 该问题是怎么引起的？
代码链接：https://gitee.com/ascend/ModelZoo-TensorFlow/tree/master/ACL_TensorFlow/built-in/nlp/BertTnews_for_ACL
![输入图片说明](https://foruda.gitee.com/images/1672993681688722288/e7f27259_11188997.png ""屏幕截图"")

### 重现步骤
点击FineTuning代码参考没有跳转到FineTuning代码下载界面


### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I68TVT","ascend","tensorflow","ResNet-50 模型在NPU模式下运行缓慢","### 该问题是怎么引起的？
利用ResNet-50模型的用例转化为NPU运行时，相比于直接在CPU下运行, 运行十分缓慢；

2023-01-04 19:41:15.042396: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2023-01-04 19:41:15.072655: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.009ms.
  function_optimizer: function_optimizer did nothing. time = 0.002ms.

2023-01-04 19:41:37.619202: I core/npu_device.cpp:129] Iterator resource provider for AnonymousIterator0 created
2023-01-04 19:41:37.767476: I core/npu_device.cpp:129] Iterator resource provider for AnonymousIterator1 created
2023-01-04 19:41:37.796676: I core/npu_device.cpp:123] Stopping iterator resource provider for AnonymousIterator0
Epoch 1/10
2023-01-04 19:41:40.326688: I core/optimizers/runtime/npu_build_npu_op_optimizer.cpp:177] __inference_train_function_13922 fully compiled on npu
2023-01-04 19:41:40.346510: I core/npu_device.cpp:531] Concrete graph for __inference_train_function_13922 loop type no-loop
2023-01-04 19:41:40.349307: I kernels/iterator_h2d.cpp:69] Hdc channel for iterator resource AnonymousIterator1 to device [0] created
2023-01-04 19:41:40.349888: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)

输出后，大约20分钟才有下一步输出。

对比CPU运行模式观察发现
CPU模式：后台是只有一个python进程；通常几秒后就有下一步输出
NPU模式：后台启动最多时，启动了700多个python进程； 大约20分钟才有下一步输出。

问题：
1. 训练速度变慢的原因？
2. 如此大量的python进程是否是期望行为？


### 重现步骤
Tensorflow版本: 2.6
Python版本：3.7.5
Ascend: 6.0.0.alpha001

用例：
```
# from https://github.com/nachi-hebbar/Transfer-Learning-ResNet-Keras
# fix some bugs
#import matplotlib.pyplot as plt
import npu_device as npu
npu.open().as_default()

import numpy as np
import os
#import PIL
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

import pathlib
dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz""
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

print(data_dir)

img_height,img_width=180,180
batch_size=32
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""validation"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

resnet_model = Sequential()

pretrained_model= tf.keras.applications.ResNet50(include_top=False,
                   input_shape=(180,180,3),
                   pooling='avg',classes=5,
                   weights='imagenet')
for layer in pretrained_model.layers:
        layer.trainable=False

resnet_model.add(pretrained_model)
resnet_model.add(Flatten())
resnet_model.add(Dense(512, activation='relu'))
resnet_model.add(Dense(5, activation='softmax'))

#resnet_model.summary()

resnet_model.compile(optimizer=Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

# dump pbtxt
full_model = tf.function(lambda x: resnet_model(x))
concrete_fun = full_model.get_concrete_function(
    tf.TensorSpec(resnet_model.inputs[0].shape, resnet_model.inputs[0].dtype))

frozen_func = convert_variables_to_constants_v2(concrete_fun)
tf.io.write_graph(concrete_fun.graph, ""./"", ""test.pbtxt"", True)
tf.io.write_graph(frozen_func.graph, ""./"", ""f_test.pbtxt"", True)

epochs=10
history = resnet_model.fit(
  train_ds,
  validation_data=val_ds,
  batch_size=32,
  epochs=epochs
)

```


### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I66OKV","ascend","tensorflow","生成的tf脚本无法调用npu","### 该问题是怎么引起的？
将个人的tf模型（.py文件）通过“convert_tf2npu”转换成了npu版本。
显示转换成功，且“npu-smi info”有反应。
通过平台自带的notebook，TensorFlow-1.15.0，环境下运行
显示CPU占用率为100%，但NPU占用率为0.
### 重现步骤



### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I62CND","ascend","tensorflow","在进行TensorFlow 2.6网络模型迁移和训练的过程中，安装TensorFlow 2.6出现问题","在进行TensorFlow 2.6网络模型迁移和训练的过程中，根据（https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/600alpha001/moddevg/tfmigr2/tfmigr2_000003.html）提供的教程安装TensorFlow 2.6，在环境准备的阶段，安装TensorFlow 2.6的时候，根据教程（https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/600alpha001/softwareinstall/instg/atlasdeploy_03_0033.html）进行安装的时候，在 安装TensorFlow 这一部分的进行步骤6的时候出错，如下所示：![输入图片说明](https://foruda.gitee.com/images/1669080784951839311/c25ea65e_11731091.png ""TensorFlow安装出错截图.PNG"")
安装不上bazel，得不到教程中最后的讲的 编译好的TensorFlow。




","open"
"https://gitee.com/ascend/tensorflow/issues/I5S6B5","ascend","tensorflow","tf_adapter_2.x编译失败，请问哪个分支代码是tf2.4","### 该问题是怎么引起的？
按照 https://toscode.gitee.com/ascend/tensorflow/tree/master/tf_adapter_2.x 去编译
文档显示是tf2.4 但是编译时需要的tf是2.6


请问那个分支是tf2.4的plugin?


### 重现步骤



### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I5P3DR","ascend","tensorflow","TensorFlow 确定性 （每次训练结果完全一致，精确到bit）","提一个重要需求，希望为TensorFlow提供在Ascend芯片上的“确定性”功能，即同样的模型和代码，在相同的种子下，在相同的Ascend型号和数量下，每次训练，每一步，loss，输出，准确率等等完全一致，精确到bit。

NVIDIA官方已经为TensorFlow 2.8正式版及以后版本提供了完整确定性功能：https://github.com/NVIDIA/framework-determinism

对于相关技术的介绍，以及为什么这一需求相当重要，可以参考https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9911-determinism-in-deep-learning.pdf

确定性技术有助于深度学习系统，在一个控制变量的情况下，进行问题排查（如自动驾驶事故溯源）

也可以参考这篇paper （ECCV 2022），首个在语义分割任务上采用完整确定性技术进行实验的算法，https://arxiv.org/pdf/2203.07160.pdf

感谢

","open"
"https://gitee.com/ascend/tensorflow/issues/I5MEX5","ascend","tensorflow","tf2.x plugin 编译失败","### 该问题是怎么引起的？
按照 https://toscode.gitee.com/ascend/tensorflow/tree/master/tf_adapter_2.x 去编译
文档显示是tf2.4  但是编译时需要的tf是2.6
[图片上传中…(image-lXAHbE8HUod0mcoMVxb3)]

请问那个分支是tf2.4的plugin?


### 重现步骤



### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I5GKJD","ascend","tensorflow","编译失败","### 该问题是怎么引起的？
执行build时报错


### 重现步骤
python 3.7.5

Linux version 4.14.0-115.el7a.0.1.aarch64 (mockbuild@aarch64-01.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)) #1 SMP Sun Nov 25 20:54:21 UTC 2018

昇腾 910A

在conda虚拟环境下
参考文档 https://support.huaweicloud.com/instg-msInstall-cann/atlasde_03_0045.html#section1
完成tf1.15.0的编译安装，然后尝试编译tfadapter，执行到Building CXX object CMakeFiles/_tf_adapter.dir/dist/swig/ge_plugin_wrap.cxx.o时报错

SWIG Version 4.0.2

cmake version 3.14.2

类似问题：
https://forum.huawei.com/enterprise/en/error-to-install-tf-adapter/thread/690809-100504

### 报错信息
(tf1150-npu) [dev211@npu-7 tensorflow]$ ./build.sh
[INFO] ---------------- tfadapter build start ----------------
Using built-in specs.
COLLECT_GCC=g++
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/aarch64-linux-gnu/7.3.0/lto-wrapper
Target: aarch64-linux-gnu
Configured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,fortran,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/home/abuild/rpmbuild/BUILD/gcc-7.3.0/obj-aarch64-linux-gnu/isl-install --without-cloog --enable-gnu-indirect-function --build=aarch64-linux-gnu --with-stage1-ldflags=' -Wl,-z,relro,-z,now' --with-boot-ldflags=' -Wl,-z,relro,-z,now' --with-multilib-list=lp64
Thread model: posix
gcc version 7.3.0 (GCC) 
[INFO] Created dir /home/dev211/ljx/tensorflow/output
[INFO] Create build directory and build tfadapter
tensorflow path: /home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core.
ascend path: /usr/local/Ascend.
Configuration finished
[INFO] CMake Args: -DENABLE_OPEN_SRC=True -DBUILD_PATH=/home/dev211/ljx/tensorflow/build -DCMAKE_INSTALL_PREFIX=/home/dev211/ljx/tensorflow/output
[INFO] Created dir /home/dev211/ljx/tensorflow/build/tfadapter
-- Configuring done
-- Generating done
-- Build files have been written to: /home/dev211/ljx/tensorflow/build/tfadapter
Consolidate compiler generated dependencies of target _tf_adapter
make[2]: *** No rule to make target '/usr/local/Ascend/compiler/lib64/libge_runner.so', needed by 'dist/python/npu_bridge/_tf_adapter.so'.  Stop.
make[2]: *** Waiting for unfinished jobs....
[  1%] Building CXX object CMakeFiles/_tf_adapter.dir/dist/swig/ge_plugin_wrap.cxx.o
<command-line>:0:0: warning: ""_GLIBCXX_USE_CXX11_ABI"" redefined
<command-line>:0:0: note: this is the location of the previous definition
In file included from /home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/google/protobuf/map_type_handler.h:34:0,
                 from /home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/google/protobuf/map.h:49,
                 from /home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/google/protobuf/generated_message_table_driven.h:34,
                 from /home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/tensorflow/core/lib/core/error_codes.pb.h:26,
                 from /home/dev211/ljx/tensorflow/build/tfadapter/_deps/tensorflow-src/tensorflow/core/lib/core/status.h:23,
                 from /home/dev211/ljx/tensorflow/tf_adapter/util/ge_plugin.h:23,
                 from /home/dev211/ljx/tensorflow/tf_adapter/util/npu_plugin.h:23,
                 from /home/dev211/ljx/tensorflow/build/tfadapter/dist/swig/ge_plugin_wrap.cxx:5192:
/home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/google/protobuf/parse_context.h: In function ‘const char* google::protobuf::internal::InlineGreedyStringParserUTF8Verify(std::__cxx11::string*, const char*, google::protobuf::internal::ParseContext*, const char*)’:
/home/dev211/miniconda/envs/tf1150-npu/lib/python3.7/site-packages/tensorflow_core/include/google/protobuf/parse_context.h:614:17: warning: unused parameter ‘field_name’ [-Wunused-parameter]
     const char* field_name) {
                 ^~~~~~~~~~
make[1]: *** [CMakeFiles/Makefile2:83: CMakeFiles/_tf_adapter.dir/all] Error 2
make: *** [Makefile:91: all] Error 2

提示没有/usr/local/Ascend/compiler/lib64/libge_runner.so文件，查看Ascend目录，只有
ascend-toolkit  driver  firmware  host_servers_remove.sh  host_servers_setup.sh  host_services_exit.sh  host_services_setup.sh  host_sys_init.sh  tfplugin  toolbox  version.info
这些文件



","open"
"https://gitee.com/ascend/tensorflow/issues/I5FMSV","ascend","tensorflow","test","### 该问题是怎么引起的？



### 重现步骤



### 报错信息




","open"
"https://gitee.com/ascend/tensorflow/issues/I5DFLC","ascend","tensorflow","tf_adapter_2.x python and tf2.x version mismatch","一、问题现象（附报错日志上下文）：
在编译tf_adapter_2.x过程中，出现Python解释器版本和tf Adapter版本匹配问题，./configure无法继续。

tf 2.6需要哪个版本的python呢？

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  5.0.4
--Tensorflow/Pytorch/MindSpore 版本: tf 2.4.0
--Python 版本 (e.g., Python 3.7.5): Python 3.7.5
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)): N/A
--操作系统版本 (e.g., Ubuntu 18.04): Ubuntu 18.04

三、测试步骤：


~/SourceCode/git/ascend-tensorflow/tf_adapter_2.x (r1.9.0 ✘)✭ ᐅ ./configure
Please specify the location of python with valid tensorflow 2.6 site-packages installed. [Default is /software/kp920-RL8/Stages/2022a/software/Python/3.7.5-GCCcore-9.5.0/bin/python3]
(You can make this quiet by set env [ADAPTER_TARGET_PYTHON_PATH]): 
Invalid python path: /software/kp920-RL8/Stages/2022a/software/Python/3.7.5-GCCcore-9.5.0/bin/python3 compat tensorflow version is 2.6 got 2.4.1.
","open"
"https://gitee.com/ascend/tensorflow/issues/I5AEAY","ascend","tensorflow","ACL_TensorFlow与TensorFlow2和TensorFlow的区别","你好 请问ModelZoo-TensorFlow仓中ACL_TensorFlow与TensorFlow2和TensorFlow的区别？","open"
"https://gitee.com/ascend/tensorflow/issues/I5A20Z","ascend","tensorflow","昇腾 910 使用tf 1.15.0  训练报错：[libprotobuf FATAL tensorflow_fusion_op_parser_util.cc:1304] CHECK failed: (index) < (current_size_):","一、问题现象（附报错日志上下文）：
执行训练后出现报错如下，但是在一个更简单的环境中不会报错

```
NFO:root:[0] - [0] : start to train
[0] - [0] : start to train
2022-05-30 11:56:50.713248: I tf_adapter/kernels/geop_npu.cc:711] The model has been compiled on the Ascend AI processor, current graph id is:21
[libprotobuf FATAL tensorflow_fusion_op_parser_util.cc:1304] CHECK failed: (index) < (current_size_):
[libprotobuf FATAL tensorflow_fusion_op_parser_util.cc:1304] CHECK failed: (index) < (current_size_):
terminate called after throwing an instance of 'ascend_private::protobuf::FatalException'
  what():  CHECK failed: (index) < (current_size_):
Aborted (core dumped)
root@6a9c2aa075e6:~/tmp/cloud_npu_20220426100710# /usr/local/python37/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 91 leaked semaphores to clean up at shutdown
  len(cache))
```


二、软件版本:
-- CANN 版本: 5.0.4
--Tensorflow: 1.15.0
--Python 版本:  3.7.5
--操作系统版本: Ubuntu 18.04
-- docker 版本：20.10.14
通过docker 方式使用：
镜像为 ascendhub.huawei.com/public-ascendhub/tensorflow-modelzoo:21.0.4

三、测试步骤：
启动我们自己的训练脚本  python3 cloud-v1/servers/training_worker.py --args , 环境复现比较复杂，后边附带了执行中的日志信息


四、日志信息:
npu info
![输入图片说明](https://images.gitee.com/uploads/images/2022/0530/201142_19274cc7_8218412.png ""屏幕截图.png"")
/root/sacend/log 目录下内容为：
链接: https://pan.baidu.com/s/18G9narATdcRmFQlJ_GEt3g?pwd=ijqp 提取码: ijqp 

","open"
"https://gitee.com/ascend/tensorflow/issues/I52LIR","ascend","tensorflow","模型训练卡住 The InferenceContext of node _XXXX is null.","一、问题现象（附报错日志上下文）：
ner模型(bilstm+crf)训练过程卡住。

自动与手动迁移都尝试过，出现同样的问题，主要修改部分：
custom_op = tf_config.graph_options.rewrite_options.custom_optimizers.add()
custom_op.name = ""NpuOptimizer""
tf_config.graph_options.rewrite_options.remapping = rewriter_config_pb2.RewriterConfig.OFF  # 必须显式关闭
tf_config.graph_options.rewrite_options.memory_optimization = rewriter_config_pb2.RewriterConfig.OFF  # 必须显式关闭

with tf.Session(config=tf_config) as sess:
    ....

由于自动迁移的tf.Session(config=npu_config_proto(config_proto=tf_config))在运行过程中报错，所以使用tf.Session(config=tf_config)

日志信息：
2022-04-13 14:51:54,994 - /cache/train.log - INFO - Created model with fresh parameters.
INFO:/cache/train.log:Created model with fresh parameters.
WARNING:tensorflow:From /cache/user-job-dir/ner/utils.py:182: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.
WARNING:tensorflow:From /cache/user-job-dir/ner/utils.py:182: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.
2022-04-13 14:51:59.761716: W tf_adapter/util/infershape_util.cc:313] The InferenceContext of node _SOURCE is null.
2022-04-13 14:51:59.761787: W tf_adapter/util/infershape_util.cc:313] The InferenceContext of node _SINK is null.
2022-04-13 14:51:59.763735: W tf_adapter/util/infershape_util.cc:313] The InferenceContext of node init is null.
资源占用情况：
![输入图片说明](https://images.gitee.com/uploads/images/2022/0413/193659_c2b37004_10753756.png ""屏幕截图.png"")

二、软件版本:
ModelArts平台
--Tensorflow版本: 1.15
--Python 版本 : python 3.7

自动迁移信息补充：
![自动迁移-1](https://images.gitee.com/uploads/images/2022/0413/195913_2eb86699_10753756.png ""屏幕截图.png"")
![自动迁移-2](https://images.gitee.com/uploads/images/2022/0413/195935_1d601af0_10753756.png ""屏幕截图.png"")
![自动迁移-3](https://images.gitee.com/uploads/images/2022/0413/200005_a5c0c778_10753756.png ""屏幕截图.png"")","open"
"https://gitee.com/ascend/tensorflow/issues/I518WP","ascend","tensorflow","Optype [Conv2DBackpropFilter] of Ops kernel [AIcoreEngine] is unsupported","一、问题现象（附报错日志上下文）：
请问如何解决这个错误，EZ3002: Optype [Conv2DBackpropFilter] of Ops kernel [AIcoreEngine] is unsupported. Reason: [tbe-custom]:op type Conv2DBackpropFilter is not found in this op store

二、软件版本:
-- CANN 版本 :  5.0.4
--Tensorflow 版本: 1.15.0
--Python 版本: 3.7.6
--操作系统版本:Ubuntu 18.04

三、测试步骤：
运行命令 python3 run_training.py

四、日志信息:
2022-04-06 11:29:40.760147: I tf_adapter/kernels/geop_npu.cc:711] The model has been compiled on the Ascend AI processor, current graph id is:141
Traceback (most recent call last):
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: GeOp29_0GEOP::::DoRunAsync Failed
Error Message is : 
EZ3002: Optype [Conv2DBackpropFilter] of Ops kernel [AIcoreEngine] is unsupported. Reason: [tbe-custom]:op type Conv2DBackpropFilter is not found in this op store.
[tbe-builtin]:[Dynamic shape check]: The value depend of input[filter_size] is required, but this input does not linked to a const or constant node.
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/1024x1024/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/512x512/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/1024x1024/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/256x256/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/128x128/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/64x64/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/512x512/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/32x32/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/16x16/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/8x8/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/256x256/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/4x4/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/128x128/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/64x64/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/32x32/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/16x16/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/8x8/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/4x4/Conv/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].

	 [[{{node GeOp29_0}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_training.py"", line 223, in <module>
    main()
  File ""run_training.py"", line 218, in main
    run(**vars(args))
  File ""run_training.py"", line 128, in run
    dnnlib.submit_run(**kwargs)
  File ""/home/ma-user/work/stylegan2_npu_glint/dnnlib/submission/submit.py"", line 343, in submit_run
    return farm.submit(submit_config, host_run_dir)
  File ""/home/ma-user/work/stylegan2_npu_glint/dnnlib/submission/internal/local.py"", line 22, in submit
    return run_wrapper(submit_config)
  File ""/home/ma-user/work/stylegan2_npu_glint/dnnlib/submission/submit.py"", line 280, in run_wrapper
    run_func_obj(**submit_config.run_func_kwargs)
  File ""/home/ma-user/work/stylegan2_npu_glint/training/training_loop.py"", line 289, in training_loop
    tflib.run([G_train_op], feed_dict)
  File ""/home/ma-user/work/stylegan2_npu_glint/dnnlib/tflib/tfutil.py"", line 32, in run
    return tf.get_default_session().run(*args, **kwargs)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: GeOp29_0GEOP::::DoRunAsync Failed
Error Message is : 
EZ3002: Optype [Conv2DBackpropFilter] of Ops kernel [AIcoreEngine] is unsupported. Reason: [tbe-custom]:op type Conv2DBackpropFilter is not found in this op store.
[tbe-builtin]:[Dynamic shape check]: The value depend of input[filter_size] is required, but this input does not linked to a const or constant node.
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/1024x1024/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/512x512/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/1024x1024/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/256x256/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/128x128/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/64x64/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/512x512/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/32x32/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/16x16/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/8x8/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/256x256/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/4x4/ToRGB/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/128x128/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/64x64/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/32x32/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/16x16/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/8x8/Conv1/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].
        No supported Ops kernel and engine are found for [TrainG_grad/gradients/G_loss/G/G_synthesis/4x4/Conv/Conv2D_grad/Conv2DBackpropFilter], optype [Conv2DBackpropFilter].

	 [[{{node GeOp29_0}}]]


日志提供方式:


获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tensorflow/issues/I4XA1X","ascend","tensorflow","stylegan2-tf性能耗时太久了，如何优化","
下载stylegan2-tf https://github.com/NVlabs/stylegan2.git后，使用自动迁移工具进行代码迁移。
然后运行推理代码：python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \
  --seeds=6600-6625 --truncation-psi=0.5

运行环境：
Ascend: 1*Ascend910|CPU: 24核 96GB
昇腾910(32GB显存)单卡规格，配搭ARM处理器

然后使用profiling分析性能，发现有大部分算子在ai cpu上，请问下如何将这部分算子迁移到ai core上，谢谢。


![输入图片说明](https://images.gitee.com/uploads/images/2022/0310/172619_6134474a_10565007.png ""屏幕截图.png"")","open"
"https://gitee.com/ascend/tools/issues/IC3CNC","ascend","tools","check i:0 name:input in size:38400 needsize:102400 not match  ","一、问题现象（附报错日志上下文）：
[ERROR] check i:0 name:input in size:38400 needsize:102400 not match

[ERROR] check input vector failed ret:-1

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
这个needsize和input in size是如何计算的，需要查看源代码，然而源代码又臭又长，没有说明如何修改","open"
"https://gitee.com/ascend/tools/issues/IC346C","ascend","tools","使用Ais_bench推理多次报错[-1][ACL: general failure]","一、问题现象（附报错日志上下文）：
使用Ais_bench的API推理paddle的det模型，已经转为om，结构为`（-1，3，-1，-1）`，单次推理可以成功，但批量推理一个文件夹的数据会失败。即第一次推理成功，后面会报错。

可以查看下面的代码。
如果`session = InferSession(device_id, model_path)`初始化代码放在for循环中，即每次推理时都重新加载session，可以成功推理，但不符合生产环境，每次初始化会很耗时。

请问这种多次推理的情况，应该如何配置呢？关于Ais_bench的API文档没有看到有这方面的知识。

同时我也参考这个issue：#I9T6T3:推理时报错[-1][ACL: general failure]
设置`session.set_staticbatch()`，但仍错误

二、软件版本:

三、测试步骤：
代码
``` python
import os
import time
import numpy as np

from ais_bench.infer.interface import InferSession
from ais_bench.infer.common.utils import logger_print

class AisBenchInfer:
    def __init__(self, device_id=1):
        """"""
        初始化推理模型
        
        Args:
            device_id: 设备ID
            model_path: 模型路径
        """"""
        self.device_id = device_id
        self.model_path_rec = ""/home/aicc/mineru/model/d_n_recfix.om""
        self.model_path_det = ""/home/aicc/mineru/model/d_n_decfix_linux_aarch64.om""
        self.session_rec = InferSession(device_id, self.model_path_rec)
        self.session_det = InferSession(device_id, self.model_path_det)
        print(""初始化完成:"")
    
    def free_resource(self):
        """"""释放模型资源""""""
        if hasattr(self, 'session'):
            self.session.free_resource()
 
    @staticmethod
    def infer_folder_det(folder_path, device_id=0, model_path='/home/aicc/mineru/model/d_n_decfix_linux_aarch64.om'):
        """"""
        处理文件夹中的所有bin文件进行检测推理
        
        Args:
            folder_path: 包含bin文件和shape.txt文件的文件夹路径
            device_id: 设备ID
            model_path: 模型路径
            
        Returns:
            所有bin文件的推理结果字典，键为bin文件名，值为推理输出
        """"""
        session = InferSession(device_id, model_path)
        # session.set_staticbatch()
        results = {}
        
        # 获取文件夹中所有bin文件
        bin_files = [f for f in os.listdir(folder_path) if f.endswith('.bin') and not f.endswith('.shape.txt')]
        
        for bin_file in bin_files:
            bin_file_path = os.path.join(folder_path, bin_file)
            shape_file_path = bin_file_path + '.shape.txt'
            
            # 检查shape文件是否存在
            if not os.path.exists(shape_file_path):
                print(f""跳过 {bin_file}: 找不到shape文件"")
                continue
            
            # 读取shape数据
            with open(shape_file_path, 'r') as f:
                shape_str = f.read().strip()
            
            # 解析shape数据
            shape = tuple(map(int, shape_str.split(',')))
            
            # 读取bin数据
            ndata = np.fromfile(bin_file_path, dtype=np.float32)
            print(f""处理 {bin_file}"")
            print(f""原始数据shape: {ndata.shape}"")
            print(f""从shape文件读取的形状: {shape}"")
       
            
            # 重塑数据
            try:
                ndata = ndata.reshape(shape)
                print(f""重塑后的数据shape: {ndata.shape}"")
                
                # 执行推理
                outputs = session.infer([ndata], mode='dymshape')
                print(f""{bin_file} 推理成功"")
                
                # 记录结果
                results[bin_file] = outputs
                
            except Exception as e:
                print(f""处理 {bin_file} 时出错: {e}"")
        
        # 释放资源
        session.free_resource()
        
        return results
    

results = AisBenchInfer.infer_folder_det('/home/aicc/mineru/MinerU_1.3.0/demo/preprocessed_data/det')
```

四、日志信息:
```
[INFO] acl init success
[INFO] open device 0 success
[INFO] create new context
[INFO] load model /home/aicc/mineru/model/d_n_decfix_linux_aarch64.om success
[INFO] create model description success
处理 det_input_20250421_034744_568.bin
原始数据shape: (460800,)
从shape文件读取的形状: (1, 3, 160, 960)
重塑后的数据shape: (1, 3, 160, 960)
det_input_20250421_034744_568.bin 推理成功
处理 det_input_20250421_034744_662.bin
原始数据shape: (331776,)
从shape文件读取的形状: (1, 3, 192, 576)
重塑后的数据shape: (1, 3, 192, 576)
[WARN] exception_cb deviceId:0 streamId:4 taskId:308 failed:500002
[WARN] exception_cb deviceId:0 streamId:4 taskId:320 failed:500002
[WARN] exception_cb deviceId:0 streamId:4 taskId:335 failed:500002
[WARN] exception_cb deviceId:0 streamId:4 taskId:357 failed:500002
[ACL ERROR] EZ9999: Inner Error!
EZ9999: 2025-04-22-08:34:05.260.731  The error from device(chipId:0, dieId:0), serial number is 846, there is an aivec error exception, core id is 35, error code = 0x10, dump info: pc start: 0x12c0c002e06c, current: 0x12c0c002e080, vec error info: 0xeb109f6e3d, mte error info: 0xa10600004c, ifu error info: 0x517003d43c100, ccu error info: 0x378000005c00001b, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c10044d000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1212]
        TraceBack (most recent call last):
        The extend info: errcode:(0x10, 0, 0) errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses. fixp_error0 info: 0x600004c, fixp_error1 info: 0xa1 fsmId:0, tslot:0, thread:0, ctxid:0, blk:15, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1224]
        The error from device(chipId:0, dieId:0), serial number is 846, there is an aivec error exception, core id is 36, error code = 0x10, dump info: pc start: 0x12c0c002e06c, current: 0x12c0c002e080, vec error info: 0x571add9e04, mte error info: 0xa10600004c, ifu error info: 0x67e85f015cc0, ccu error info: 0x378000005c00001b, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c10044d000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1212]
        The extend info: errcode:(0x10, 0, 0) errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses. fixp_error0 info: 0x600004c, fixp_error1 info: 0xa1 fsmId:0, tslot:0, thread:0, ctxid:0, blk:16, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1224]
        The error from device(chipId:0, dieId:0), serial number is 846, there is an aivec error exception, core id is 37, error code = 0x10, dump info: pc start: 0x12c0c002e06c, current: 0x12c0c002e080, vec error info: 0x7600f5366f, mte error info: 0xa10600004c, ifu error info: 0x5f467beb04e40, ccu error info: 0x378000005c00001b, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c10044d000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1212]
        The extend info: errcode:(0x10, 0, 0) errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses. fixp_error0 info: 0x600004c, fixp_error1 info: 0xa1 fsmId:0, tslot:0, thread:0, ctxid:0, blk:17, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1224]
        ****** 备注：类似的错误日志 **********
       
        The error from device(chipId:0, dieId:0), serial number is 849, there is an aivec error exception, core id is 44, error code = 0x10, dump info: pc start: 0x12c0c00463b8, current: 0x12c0c00463e8, vec error info: 0x9116822c8d, mte error info: 0xf90600004c, ifu error info: 0x89cebc0f3200, ccu error info: 0x3f8000000c0000ee, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd00028c, para base: 0x12c100459400.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1212]
        The extend info: errcode:(0x10, 0, 0) errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses. fixp_error0 info: 0x600004c, fixp_error1 info: 0xf9 fsmId:1, tslot:0, thread:0, ctxid:0, blk:39, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_proc.cc][LINE:1224]
        get opdesc info failed, device_id:0, stream_id:4, task_id:357.[FUNC:GetOpDescInfo][FILE:ge_executor.cc][LINE:1310]
        [Get][OpDescInfo]get op desc faild, ge result[-1], deviceId[0], streamId[4], taskId[357][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        AIV Kernel happen error, retCode=0x31.[FUNC:GetError][FILE:stream.cc][LINE:1082]
        Aicore kernel execute failed, device_id=0, stream_id=4, report_stream_id=4, task_id=308, flip_num=0, fault kernel_name=te_add_bffaa59b24835a39a16ebebb901bfe6348252fb2631a844142f0942bca747739_1_210000000, fault kernel info ext=none, program id=42, hash=15921761091410364091.[FUNC:GetError][FILE:stream.cc][LINE:1082]
        [AIC_INFO] after execute:args print end[FUNC:GetError][FILE:stream.cc][LINE:1082]
        Aicore kernel execute failed, device_id=0, stream_id=4, report_stream_id=4, task_id=320, flip_num=0, fault kernel_name=te_add_bffaa59b24835a39a16ebebb901bfe6348252fb2631a844142f0942bca747739_1_210000000, fault kernel info ext=none, program id=42, hash=15921761091410364091.[FUNC:GetError][FILE:stream.cc][LINE:1082]
        Aicore kernel execute failed, device_id=0, stream_id=4, report_stream_id=4, task_id=335, flip_num=0, fault kernel_name=te_add_bffaa59b24835a39a16ebebb901bfe6348252fb2631a844142f0942bca747739_1_210000000, fault kernel info ext=none, program id=42, hash=15921761091410364091.[FUNC:GetError][FILE:stream.cc][LINE:1082]
        rtStreamSynchronizeWithTimeout execute failed, reason=[vector core exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
        Assert ((rt_ret) == 0) failed[FUNC:DoRtStreamSyncWithTimeout][FILE:utils.cc][LINE:45]
        Assert ((DoRtStreamSyncWithTimeout(default_stream_)) == ge::SUCCESS) failed[FUNC:ExecuteSync][FILE:model_v2_executor.cc][LINE:240]
        [Exec][Model]Execute model failed, ge result[1343225857], modelId[2147483648][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        [Exec][Model]modelId[2147483648] execute failed, result[500002][FUNC:ReportInnerError][FILE:log_inner.cpp][LINE:145]

[ERROR] execute model failed, modelId is 2147483648
[ERROR] acl execute failed:1
[ERROR] execute Infer failed ret:-1
处理 det_input_20250421_034744_662.bin 时出错: [-1][ACL: general failure] 
```","open"
"https://gitee.com/ascend/tools/issues/IBC57K","ascend","tools","ais_bench 推理和 torch不能同时使用","一、问题现象（附报错日志上下文）：
```py
import torch
import torch_npu

from ais_bench.infer.interface import InferSession
campplus_session = InferSession(device_id=0, model_path=""./campplus.om"")

feat = torch.randn(64, 64, 80)

embedding = campplus_session.infer(
    feeds=[feat.cpu().numpy()],
    mode='dymshape', # 动态Shape推理
    custom_sizes=1000000
)[0]#.flatten().tolist() 

print(embedding.shape) # (64, 192)

t = torch.from_numpy(embedding).to(""npu:0"")
print((t*t).shape) # 报错 
```

报错日志

```sh
[E compiler_depend.ts:419] call aclnnMul failed, detail:EZ9903: [PID: 707979] 2024-12-18-15:38:43.674.291 rtKernelLaunchWithHandleV2 failed: 107003
        Solution: In this scenario, collect the plog when the fault occurs and locate the fault based on the plog.
        TraceBack (most recent call last):
        Kernel launch with handle failed, stream is not in current ctx, stream_id=5.[FUNC:KernelLaunchWithHandle][FILE:api_impl.cc][LINE:489]
        rtKernelLaunchWithHandleV2 execute failed, reason=[stream not in current context][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53]
        rtKernelLaunchWithHandleV2 failed: 107003
        #### KernelLaunch failed: /usr/local/Ascend/ascend-toolkit/8.0.0.alpha002/opp/built-in/op_impl/ai_core/tbe//kernel/ascend910b/mul/Mul_ee98c6628030785f610b924ab1557b31_high_performance.o
        Kernel Run failed. opType: 3, Mul
        launch failed for Mul, errno:361001. **粗体** 
```

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/IBBEVY","ascend","tools","ais_bench只支持bin和npy文件的输入","一、需求场景&价值
效率提升，提升效率
二、需求建议实现的规格
ais_bench只支持bin和npy文件的输入，在离线推理前先把图片由JPG格式转换到bin或者npy，需要自己写代码格式转换；代码量100行以内。
三、竞品比较（选填）","open"
"https://gitee.com/ascend/tools/issues/IBBEUU","ascend","tools","离线推理的ais_bench工具只支持一次离线推理多个om模型","一、需求场景&价值
提升ais_bench推理效率
二、需求建议实现的规格
1、离线推理的ais_bench工具只支持一次离线推理一个om模型，但是客户侧会同时调用6个om模型对一张图片进行分类或者目标检测，重复操作6次；总耗时15分钟左右。

三、竞品比较（选填）","open"
"https://gitee.com/ascend/tools/issues/IBBES0","ascend","tools","msame新增支持npy和jpg格式的输入","一、需求场景&价值
易用性提升

二、需求建议实现的规格
msame只能支持bin格式的输入，在离线推理前先把图片由JPG格式转换到bin，需要自己写代码格式转换；代码量100行以内。
三、竞品比较（选填）","open"
"https://gitee.com/ascend/tools/issues/I762PG","ascend","tools","比对工具专家建议描述","1. 日志打屏描述
![输入图片说明](https://foruda.gitee.com/images/1684399485449992126/7be54d07_8515381.png ""屏幕截图"")
2. 落盘文件描述
![输入图片说明](https://foruda.gitee.com/images/1684399528481215890/05165c38_8515381.png ""屏幕截图"")","open"
"https://gitee.com/ascend/tools/issues/IBBEPS","ascend","tools","离线推理工具msame支持同时调用多个om","一、需求场景&价值
提升msame推理效率
二、需求建议实现的规格
离线推理工具msame只支持一次离线推理一个om模型，但是客户侧会同时调用6个om模型对一张图片进行分类或者目标检测，重复操作6次；总耗时15分钟左右。

三、竞品比较（选填）","open"
"https://gitee.com/ascend/tools/issues/IB76IY","ascend","tools","使用ais_bench在atlas500小站上推理yolo报模型加载失败","atlas500小站上使用镜像环境，运行yolov8 om模型推理报错
![输入图片说明](https://foruda.gitee.com/images/1732612754795983122/1fa3a06b_9606572.png ""11.png"")","open"
"https://gitee.com/ascend/tools/issues/IB5SIP","ascend","tools","convert_tensors_to_host   tensor.to_host() 报错","一、我的blip模型：
#Code from https://huggingface.co/Salesforce/blip-image-captioning-large
import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained(""Salesforce/blip-image-captioning-large"")
model = BlipForConditionalGeneration.from_pretrained(""Salesforce/blip-image-captioning-large"")

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' 
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# conditional image captioning
text = ""a photography of""
inputs = processor(raw_image, text, return_tensors=""pt"")

with torch.no_grad():
    torch.onnx.export(
        model, 
        tuple((inputs[""pixel_values""],inputs['input_ids'],inputs['attention_mask'])),
        f=""blip_model.onnx"",  
        input_names=['pixel_values', 'input_ids','attention_mask'], 
        output_names=['caption'],     
        do_constant_folding=True, 
        opset_version=13, 
    )


二、blip onnx  转换为om 
atc --model=/home/wjb/model/onnx/blip_model.onnx --framework=5 --output=/home/wjb/model/onnx/onnx_blip --soc_version=Ascend310P3

三、运行blip的om代码
from ais_bench.infer.interface import InferSession
import torch
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

model_id = '/home/wjb/model/blip-image-captioning-large/'
# 加载预处理器和模型
processor = BlipProcessor.from_pretrained(model_id)
model = InferSession(0, ""/home/wjb/model/onnx/onnx_blip.om"")  # 模型加载，0表示用第0张卡


# conditional image captioning
text = ""a photography of""
raw_image = Image.open('5.png').convert('RGB')
inputs = processor(raw_image, text, return_tensors=""pt"")

outputs = model.infer(feeds=[inputs[""pixel_values""],inputs['input_ids'],inputs['attention_mask']],mode='static')


logits = torch.Tensor(outputs[0])
predicted_ids = torch.argmax(logits, dim=-1)  # 形状: [1, 5]
decoded_text = processor.decode(predicted_ids[0], skip_special_tokens=True)
print(decoded_text)

exec_time = model.summary().exec_time_list[-1]
print(model.summary())
model.free_resource()

代码报错：

(rec) wjb@ubuntu-Atlas-800-Model-3010:~$ python test_blip.py
[INFO] acl init success
[INFO] open device 0 success
[INFO] load model /home/wjb/model/onnx/onnx_blip.om success
[INFO] create model description success
param2 pointer is nullptr
CheckCopyValid failed. ret=1004
TensorBuffer::TensorBufferCopy failed. ret=1004
Traceback (most recent call last):
  File ""/home/wjb/test_blip.py"", line 17, in <module>
    outputs = model.infer(feeds=[inputs[""pixel_values""],inputs['input_ids'],inputs['attention_mask']],mode='static')
  File ""/home/wjb/miniconda3/envs/rec/lib/python3.9/site-packages/ais_bench/infer/interface.py"", line 224, in infer
    return self.run(inputs, out_array)
  File ""/home/wjb/miniconda3/envs/rec/lib/python3.9/site-packages/ais_bench/infer/interface.py"", line 167, in run
    self.convert_tensors_to_host(outputs)
  File ""/home/wjb/miniconda3/envs/rec/lib/python3.9/site-packages/ais_bench/infer/interface.py"", line 77, in convert_tensors_to_host
    tensor.to_host()
RuntimeError: [1004][Invalid parameter]
[1003][Invalid Pointer] Free failed, ptrData is nullptr.[INFO] unload model success, model Id is 1
[INFO] end to reset device 0
[INFO] end to finalize acl


我该怎么解决呢？
","open"
"https://gitee.com/ascend/tools/issues/IB5IFE","ascend","tools","aclruntime分别调用free_resources接口，第二次执行的时候报错","一、问题现象（附报错日志上下文）：
![输入图片说明](https://foruda.gitee.com/images/1732010826616666166/f146e690_5326138.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
import aclruntime
import numpy as np
ACL_DTYPE = {
    0: np.float32,
    1: np.float16,
    2: np.int8,
    3: np.int32,
    4: np.uint8,
    6: np.int16,
    7: np.uint16,
    8: np.uint32,
    9: np.int64,
    10: np.uint64,
    11: np.double,
    12: np.bool_,
}
options = aclruntime.session_options()
options.log_level = 1
session = aclruntime.InferenceSession(""/tmp/pycharm_project_443/acl_test/uie_bs1_128.om"", 0, options)
input_desc_list = session.get_inputs()
input_data_list = []
for input_desc in input_desc_list:
    input_dtype = ACL_DTYPE[input_desc.datatype.value]
    input_shape = input_desc.shape
    np_array = np.random.randn(*input_shape).astype(input_dtype)
    baseTensor_data = aclruntime.BaseTensor(np_array.__array_interface__['data'][0], np_array.nbytes)
    input_data_list.append(baseTensor_data)
print(""000000000000000"")
outputs_names0 = [meta.name for meta in session.get_outputs()]
output_list0 = session.run(outputs_names0, input_data_list)
for output_data in output_list0:
    print(output_data.shape)
session.free_resource()
print(""=============================="")
session1 = aclruntime.InferenceSession(""/tmp/pycharm_project_443/acl_test/uie_bs1_128.om"", 0, options)
print(""111111111111111111"")
output_list1 = session1.run(outputs_names0, input_data_list)
session1.free_resource()
session.finalize()



四、日志信息:
[INFO] acl init success
[INFO] open device 0 success
[DEBUG] finish create context 0 in device 0
[DEBUG] model aclmdlLoadFromFile cost : 442.372986 (ms)
[INFO] load model /tmp/pycharm_project_443/acl_test/uie_bs1_128.om success
[INFO] create model description success
[DEBUG] get input dynamic gear count success
[DEBUG] start print model description
[DEBUG] NumInputs: 4
[DEBUG] NumOutputs: 2
[DEBUG] the size of 0 input: 1024
[DEBUG] the dims of 0 input:
1 128 
[DEBUG] the name of 0 input: input_ids
[DEBUG] the Format of 0 input: 2
[DEBUG] the DataType of 0 input: 9
[DEBUG] the size of 1 input: 1024
[DEBUG] the dims of 1 input:
1 128 
[DEBUG] the name of 1 input: token_type_ids
[DEBUG] the Format of 1 input: 2
[DEBUG] the DataType of 1 input: 9
[DEBUG] the size of 2 input: 1024
[DEBUG] the dims of 2 input:
1 128 
[DEBUG] the name of 2 input: position_ids
[DEBUG] the Format of 2 input: 2
[DEBUG] the DataType of 2 input: 9
[DEBUG] the size of 3 input: 1024
[DEBUG] the dims of 3 input:
1 128 
[DEBUG] the name of 3 input: attention_mask
[DEBUG] the Format of 3 input: 0
[DEBUG] the DataType of 3 input: 9
[DEBUG] the size of 0 output: 512
[DEBUG] the dims of 0 output:
1 128 
[DEBUG] the name of 0 output: p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0
[DEBUG] the Format of 0 output: 2
[DEBUG] the DataType of 0 output: 0
[DEBUG] the size of 1 output: 512
[DEBUG] the dims of 1 output:
1 128 
[DEBUG] the name of 1 output: p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0
[DEBUG] the Format of 1 output: 2
[DEBUG] the DataType of 1 output: 0
[DEBUG] end print model description
000000000000000
[DEBUG] start to ModelInference base_tensor
[DEBUG] Create OutMemory i:0 name:p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0 size:512
[DEBUG] Create OutMemory i:1 name:p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0 size:512
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] SetInputData successfully
[DEBUG] model execute success
[DEBUG] model aclExec cost : 2.123000
[DEBUG] get max dynamic batch size success
[DEBUG] AddOutTensors name:p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0 index:0 len:512 outdescsize:512 shapesize:2
[DEBUG] get max dynamic batch size success
[DEBUG] AddOutTensors name:p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0 index:1 len:512 outdescsize:512 shapesize:2
[DEBUG] destroy model input success
[1, 128]
[1, 128]
[INFO] unload model success, model Id is 1
[DEBUG] PyInferSession DestroySession successfully!
[DEBUG] end to destroy context 0 in device 0
[DEBUG] PyInferSession FreeResource successfully!
==============================
[DEBUG] finish create context 1 in device 0
[DEBUG] model aclmdlLoadFromFile cost : 253.326996 (ms)
[INFO] load model /tmp/pycharm_project_443/acl_test/uie_bs1_128.om success
[INFO] create model description success
[DEBUG] get input dynamic gear count success
[DEBUG] start print model description
[DEBUG] NumInputs: 4
[DEBUG] NumOutputs: 2
[DEBUG] the size of 0 input: 1024
[DEBUG] the dims of 0 input:
1 128 
[DEBUG] the name of 0 input: input_ids
[DEBUG] the Format of 0 input: 2
[DEBUG] the DataType of 0 input: 9
[DEBUG] the size of 1 input: 1024
[DEBUG] the dims of 1 input:
1 128 
[DEBUG] the name of 1 input: token_type_ids
[DEBUG] the Format of 1 input: 2
[DEBUG] the DataType of 1 input: 9
[DEBUG] the size of 2 input: 1024
[DEBUG] the dims of 2 input:
1 128 
[DEBUG] the name of 2 input: position_ids
[DEBUG] the Format of 2 input: 2
[DEBUG] the DataType of 2 input: 9
[DEBUG] the size of 3 input: 1024
[DEBUG] the dims of 3 input:
1 128 
[DEBUG] the name of 3 input: attention_mask
[DEBUG] the Format of 3 input: 0
[DEBUG] the DataType of 3 input: 9
[DEBUG] the size of 0 output: 512
[DEBUG] the dims of 0 output:
1 128 
[DEBUG] the name of 0 output: p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0
[DEBUG] the Format of 0 output: 2
[DEBUG] the DataType of 0 output: 0
[DEBUG] the size of 1 output: 512
[DEBUG] the dims of 1 output:
1 128 
[DEBUG] the name of 1 output: p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0
[DEBUG] the Format of 1 output: 2
[DEBUG] the DataType of 1 output: 0
[DEBUG] end print model description
111111111111111111
[DEBUG] start to ModelInference base_tensor
[DEBUG] Create OutMemory i:0 name:p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0 size:512
[DEBUG] Create OutMemory i:1 name:p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0 size:512
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] add input_ at CreateInput +1
[DEBUG] SetInputData successfully
[DEBUG] model execute success
[DEBUG] model aclExec cost : 3.405000
[DEBUG] get max dynamic batch size success
[DEBUG] AddOutTensors name:p2o.Sigmoid.0:0:save_infer_model/scale_0.tmp_0 index:0 len:512 outdescsize:512 shapesize:2
[DEBUG] get max dynamic batch size success
[DEBUG] AddOutTensors name:p2o.Sigmoid.1:0:save_infer_model/scale_1.tmp_0 index:1 len:512 outdescsize:512 shapesize:2
[DEBUG] destroy model input success
False
False
[INFO] unload model success, model Id is 2
[DEBUG] PyInferSession DestroySession successfully!
[DEBUG] end to destroy context 1 in device 0
[DEBUG] PyInferSession FreeResource successfully!
[ERROR] SetContext failed: device 0 is not set or context 0 is not created.
SetContext failed. ret=1
SetContext failed. ret=1
[ERROR] SetContext failed: device 0 is not set or context 0 is not created.
SetContext failed. ret=1
SetContext failed. ret=1
[ERROR] SetContext failed: device 0 is not set or context 1 is not created.
SetContext failed. ret=1
SetContext failed. ret=1
[ERROR] SetContext failed: device 0 is not set or context 1 is not created.
SetContext failed. ret=1
SetContext failed. ret=1
[DEBUG] end to destroy contexts in device 0
[INFO] end to reset device 0
[INFO] end to finalize acl

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/IB1LNN","ascend","tools","调用infer_pipeline报错，shape0 为[-1,-1]，不知道什么原因","def infer_pipeline_api_static():
    device_id = 0
    model_path = ""../../sampledata/add_model/model/add_model_bs1.om""
    # create session of om model for inference
    session = InferSession(device_id, model_path)
    # create new numpy data according inputs info
    shape0 = session.get_inputs()[0].shape
    ndata0 = np.full(shape0, 1).astype(np.float32)
    shape1 = session.get_inputs()[1].shape
    ndata1 = np.full(shape1, 1).astype(np.float32)
    feeds = [ndata0, ndata1]
    feeds_list = [feeds, feeds]
    # execute inference, inputs is ndarray list and outputs is ndarray list
    outputs = session.infer_pipeline(feeds_list, mode='static')
    print(f""outputs: {outputs}"")
    # free model resource and device context of session
    session.free_resource()","open"
"https://gitee.com/ascend/tools/issues/IALG44","ascend","tools","ais_bench动态shape模型推理报错","使用atc转换onnx模型，模型输入shape不固定
使用命令为
atc --framework=5 --model=./PaddleOCR/inference/det_onnx/model.onnx --output=./ch_PP-OCRv3_det_bs1_dynamic_shape --input_shape=""x:1,3,-1,-1"" --soc_version=Ascend310P3

模型转换成功，使用ais_bench进行模型推理时报错：
python3 -m ais_bench --model=ch_PP-OCRv3_det_bs1_dynamic_shape_linux_aarch64.om --input=./prep_data_dir/img_npy --output=./ --output_dirname=results_bs1 --auto_set_dymdims_mode=1 --outfmt=NPY
[INFO] acl init success
[INFO] open device 0 success
[INFO] load model ch_PP-OCRv3_det_bs1_dynamic_shape_linux_aarch64.om success
[INFO] create model description success
[INFO] try get model batchsize:1
[INFO] output path:./results_bs1
[INFO] get filesperbatch files0 size:10457088 tensor0size:0 filesperbatch:1 runcount:19
the dynamic_dims parameter is not specified for model conversionTraceback (most recent call last):
  File ""/usr/local/python3.10.8/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/python3.10.8/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/__main__.py"", line 18, in <module>
    exec(open(os.path.join(cur_path, ""infer/__main__.py"")).read())
  File ""<string>"", line 278, in <module>
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/infer_process.py"", line 762, in infer_process
    main(args)
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/infer_process.py"", line 490, in main
    warmup(session, args, intensors_desc, infileslist[0])
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/infer_process.py"", line 166, in warmup
    outputs = run_inference(session, args, infeeds, out_array=True)
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/infer_process.py"", line 181, in run_inference
    set_dymdims_shape(session, inputs)
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/infer_process.py"", line 147, in set_dymdims_shape
    session.set_dynamic_dims(dydims)
  File ""/usr/local/python3.10.8/lib/python3.10/site-packages/ais_bench/infer/interface.py"", line 127, in set_dynamic_dims
    self.session.set_dynamic_dims(dym_dims)
RuntimeError: [-1][ACL: general failure] 
[INFO] unload model success, model Id is 2147483648
[INFO] end to reset device 0
[INFO] end to finalize acl

在CANN提交工单给atc团队，以上模型转换命令是团队回复提供的，并且提交了报错日志，确认是在推理流程时出现的问题，模型转换没有问题，请查看哪里出现的问题？","open"
"https://gitee.com/ascend/tools/issues/IAL9F8","ascend","tools","ais_bench工具threads参数如何使用，测试加上后报错","### ais_bench性能测试 resnet50
不加threads正常 
```
python3 -m ais_bench --model ./resnet50_bs64.om --input ./prep_dataset/ --output ./ --output_dirname result --outfmt TXT --batchsize 16 
```
![输入图片说明](https://foruda.gitee.com/images/1724142444342181140/b5b964a4_8049142.png ""test1.png"")
加上报错
```
python3 -m ais_bench --model ./resnet50_bs64.om --input ./prep_dataset/ --output ./ --output_dirname result --outfmt TXT --batchsize 16 --threads 2 --pipeline 1
```
![输入图片说明](https://foruda.gitee.com/images/1724142525834578088/60fb3368_8049142.png ""test.png"")","open"
"https://gitee.com/ascend/tools/issues/IAJQK5","ascend","tools","aiisbench使用trtexec推理后端在传入onnx模型时对传入文件后缀校验存在问题","一、问题现象（附报错日志上下文）：
aisbench使用trtexec推理后端在传入onnx模型时，会因为文件名后缀不是om而报错
![输入图片说明](https://foruda.gitee.com/images/1723537333297690346/bb378f5b_14601627.png ""屏幕截图"")
于是尝试将onnx模型使用cann的atc工具转换为om模型：
![输入图片说明](https://foruda.gitee.com/images/1723537591434152448/5b2f57ed_14601627.png ""屏幕截图"")
传入om模型进行性能测试跑出来为空数据：
![输入图片说明](https://foruda.gitee.com/images/1723537643507574605/4339436c_14601627.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1723537672970884646/e22729be_14601627.png ""屏幕截图"")
在多次尝试后将onnx模型文件后缀改成om，传入进行测试，可以跑出测试结果：
![输入图片说明](https://foruda.gitee.com/images/1723537841122707296/4f41bb07_14601627.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1723538048176091806/0798b304_14601627.png ""屏幕截图"")
二、软件版本:
-- CANN 版本 (Ascend-cann-toolkit_8.0.RC2.alpha003_linux-x86_64):  
--TensorRT版本：TensorRT-10.2.0.19
--Python 版本 (Python 3.10.12):
-- cuda版本 ( release 12.5, V12.5.82)
--操作系统版本 ( Ubuntu 22.04.3 LTS):

三、测试步骤：
见问题现象


四、日志信息:
见问题现象

目测应该是aisbench使用trtexec推理后端对输入的模型文件后缀校验存在问题","open"
"https://gitee.com/ascend/tools/issues/IAHZZO","ascend","tools","转换失败","(run_atc) [root@tdcnode013260 saved_model2om]# python3 saved_model2om.py --input_path=/mnt/disk2/SophonOCR/repo/obj_recognition_table/1/model.savedmodel --output_path=/mnt/disk2/SophonOCR/repo/obj_recognition_table/1 --input_shape=""input_1:1,1024,1024,3"" --soc_version=Ascend310
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 1024, 1024, 3)
      name: serving_default_input_1:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['tf.identity'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 1024, 1)
      name: StatefulPartitionedCall:0
  outputs['tf.identity_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (1, 1024, 1)
      name: StatefulPartitionedCall:1
Method name is: tensorflow/serving/predict
[INFO]: Save Model has [ 2 ] outputs.
[INFO]: Outputs Nodes:  OrderedDict([('tf.identity', NodeInfo(name='StatefulPartitionedCall', shape=(1, 1024, 1), type='DT_FLOAT', full_name='StatefulPartitionedCall:0')), ('tf.identity_1', NodeInfo(name='StatefulPartitionedCall', shape=(1, 1024, 1), type='DT_FLOAT', full_name='StatefulPartitionedCall:1'))]) .
WARNING:tensorflow:From saved_model2om.py:93: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2024-08-05 18:58:28.040691: W tensorflow/core/platform/profile_utils/cpu_utils.cc:98] Failed to find bogomips in /proc/cpuinfo; cannot determine CPU frequency
2024-08-05 18:58:28.041596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaaac357d8870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-08-05 18:58:28.041620: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From saved_model2om.py:94: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:From saved_model2om.py:98: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /root/miniconda3/envs/run_atc/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
[INFO]: Inputs Nodes With Shapes:  OrderedDict([('input_1', NodeInfo(name='serving_default_input_1', shape=(1, 1024, 1024, 3), type='DT_FLOAT', full_name='serving_default_input_1:0'))]) .
[INFO]: Saved Model convert to Frozen Model done.
The input_shape of model: input_1:1,1024,1024,3
Traceback (most recent call last):
  File ""/root/miniconda3/envs/run_atc/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from sfcn0/kernel:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/Ascend/ascend-toolkit/latest/atc/python/func2graph/func2graph.py"", line 270, in <module>
    convert_graphs(model)
  File ""/usr/local/Ascend/ascend-toolkit/latest/atc/python/func2graph/func2graph.py"", line 198, in convert_graphs
    tf.import_graph_def(graph_def, name='')
  File ""/root/miniconda3/envs/run_atc/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/root/miniconda3/envs/run_atc/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/root/miniconda3/envs/run_atc/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py"", line 505, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 1 of node StatefulPartitionedCall was passed float from sfcn0/kernel:0 incompatible with expected resource.
ATC start working now, please wait for a moment.
ATC run failed, Please check the detail log, Try 'atc --help' for more information
E19000: Path[/tmp/saved_model2om/pb_dir_1722855507/graph_def_library.pbtxt] is empty. Reason: No such file or directory.
        Possible Cause: The file does not exist.
        Solution: Try again with a valid directory.
        TraceBack (most recent call last):
        Failed to find the subgraph library.
        ATC model parse ret fail.[FUNC:ParseGraph][FILE:omg.cc][LINE:788]

Ascend-cann-tfplugin_6.0.1_linux-aarch64.run（tfplugin版本）

Ascend-cann-toolkit_6.0.1_linux-aarch64.run（cann版本）
https://gitee.com/ascend/tools/tree/master/saved_model2om（参考这个文档去转换的）","open"
"https://gitee.com/ascend/tools/issues/IAG9SM","ascend","tools","利用 msit 工具对模型进行精度对比后发现 类型cast 算子精度不符合要求","一、问题现象（附报错日志上下文）：
使用ATC 对yolov5_4.0 版本的模型转换后 输出结果的boxes 坐标宽高为负值

二、软件版本:
-- CANN 版本 8.0rc2:  
-- 硬件版本 A310P3 

--Python 版本 (e.g., Python 3.9.2):

--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
通过对 onnx 和om 推理结果比对 ，发现宽高值不正确,不经过后处理结果 接近为0 或负值



利用 msit 工具对模型进行精度对比后发现 类型cast 算子精度不符合要求


四、日志信息:
推理过程无报错 但结果不准确

Detection Type: Global Consistency
Operator Index: 398
Expert Advice: The accuracy of some tensors is low, resulting in an unqualified final accuracy. This may be caused by quantization. Calibrate the data or contact Huawei for further

msit 日志提供方式:
msit 精度对比工具数据  外部网盘后提供链接。 https://www.alipan.com/t/5nXT9YVwmOqIzfyaPi6O  此链接失效后 评论会更新","open"
"https://gitee.com/ascend/tools/issues/IAF494","ascend","tools"," python3 -m ais_bench --model ../model/detrnoaipp.om 报错","利用atc转化模型再Ascend310上进行初步推理
 python3 -m ais_bench --model ../model/detrnoaipp.om 
然后报错了：报错信息如下，请问是什么原因？
root@davinci-mini:/home/FOD/MyFirstApp_ONNX/out# python3 -m ais_bench --model ../model/detrnoaipp.om 
[INFO] acl init success
[INFO] open device 0 success
[INFO] load model ../model/detrnoaipp.om success
[INFO] create model description success
[INFO] try get model batchsize:1
[INFO] exception_cb streamId:4 taskId:592 deviceId: 0 opName:PartitionedCall_Gather_2713_gatherv2_428 inputCnt:3 outputCnt:1
EE1001: The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        The error from device(0), serial number is 3, there is an aicore error, core id is 0, error code = 0x10, dump info: pc start: 0xe7ffc2ecf000, current: 0xe7ffc2ecf190, vec error info: 0x777567, mte error info: 0xb2, ifu error info: 0x39bab01a1380, ccu error info: 0x761c62720023275e, cube error info: 0xec, biu error info: 0, aic error mask: 0x65000200d000288, para base: 0xe7ffc57fb5b0, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:533]
        The extend info from device(0), serial number is 3, there is aicore error, core id is 0, aicore int: 0x1, aicore error2: 0, axi clamp ctrl: 0, axi clamp state: 0x1717, biu status0: 0x1c00800000000, biu status1: 0x940002092a0000, clk gate mask: 0, dbg addr: 0, ecc en: 0x3f3f, mte ccu ecc 1bit error: 0x1000, vector cube ecc 1bit error: 0, run stall: 0x1, dbg data0: 0, dbg data1: 0, dbg data2: 0, dbg data3: 0, dfx data: 0[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:564]
        The device(0), core list[0-0], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:587]
        coreId( 0):            0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:601]
        ctx is NULL![FUNC:MemCopySync][FILE:api_impl.cc][LINE:1543]
        The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        ctx is NULL![FUNC:HostMalloc][FILE:api_impl.cc][LINE:1346]
        The argument is invalid.Reason: rtMallocHost execute failed, reason=[context pointer null]
        Call rtMallocHost failed, size:96, ret:0x1a1fa[FUNC:LogExceptionArgs][FILE:exception_dumper.cc][LINE:293]
        alloc host memory failed, runtime result = 107002[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:4290]
        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]

[WARN] exception_cb MallocHost failed len:4800 ret:107002
[WARN] exception_cb input_0 save failed
EE1001: The argument is invalid.Reason: rtMallocHost execute failed, reason=[context pointer null]
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        ctx is NULL![FUNC:HostMalloc][FILE:api_impl.cc][LINE:1346]
        The argument is invalid.Reason: rtMallocHost execute failed, reason=[context pointer null]
        alloc host memory failed, runtime result = 107002[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:4290]
        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]

EE1001: The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        The error from device(0), serial number is 3, there is an aicore error, core id is 0, error code = 0x10, dump info: pc start: 0xe7ffc2ecf000, current: 0xe7ffc2ecf190, vec error info: 0x777567, mte error info: 0xb2, ifu error info: 0x39bab01a1380, ccu error info: 0x761c62720023275e, cube error info: 0xec, biu error info: 0, aic error mask: 0x65000200d000288, para base: 0xe7ffc57fb5b0, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:533]
        The extend info from device(0), serial number is 3, there is aicore error, core id is 0, aicore int: 0x1, aicore error2: 0, axi clamp ctrl: 0, axi clamp state: 0x1717, biu status0: 0x1c00800000000, biu status1: 0x940002092a0000, clk gate mask: 0, dbg addr: 0, ecc en: 0x3f3f, mte ccu ecc 1bit error: 0x1000, vector cube ecc 1bit error: 0, run stall: 0x1, dbg data0: 0, dbg data1: 0, dbg data2: 0, dbg data3: 0, dfx data: 0[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:564]
        The device(0), core list[0-0], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:587]
        coreId( 0):            0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:601]
        ctx is NULL![FUNC:MemCopySync][FILE:api_impl.cc][LINE:1543]
        The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        ctx is NULL![FUNC:HostMalloc][FILE:api_impl.cc][LINE:1346]
        The argument is invalid.Reason: rtMallocHost execute failed, reason=[context pointer null]
        Call rtMallocHost failed, size:96, ret:0x1a1fa[FUNC:LogExceptionArgs][FILE:exception_dumper.cc][LINE:293]
        alloc host memory failed, runtime result = 107002[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:4290]
        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]

[WARN] exception_cb MallocHost failed len:800 ret:107002
[WARN] exception_cb input_0 save failed
EZ9999: Inner Error!
EZ9999  Aicore kernel execute failed, device_id=0, stream_id=4, report_stream_id=3, task_id=592, flip_num=0, fault kernel_name=2158435995904263937-1_0_1_PartitionedCall_Gather_2713_gatherv2_428, program id=589, hash=8344706501436046081.[FUNC:GetError][FILE:stream.cc][LINE:1483]
        TraceBack (most recent call last):
        [AIC_INFO] after execute:args print end[FUNC:GetError][FILE:stream.cc][LINE:1483]
        Model synchronize execute failed, model_id=0![FUNC:GetStreamToSyncExecute][FILE:model.cc][LINE:707]
        rtModelExecute execute failed, reason=[the model stream execute failed][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:50]
        [Exec][Model]Execute model failed, ge result[507011], modelId[1][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        [Exec][Model]modelId[1] execute failed, result[507011][FUNC:ReportInnerError][FILE:log_inner.cpp][LINE:145]


DEVICE[0] PID[12871]: 
EXCEPTION STREAM:
  Exception info:TGID=12871, model id=0, stream id=4, stream phase=SCHEDULE
  Message info[0]:aicore exception, slot_id=1, stream_id=4
    Other info[0]:time=2024-07-24-01:36:21.322.368, function=bs_done_exception_proc_running, line=1010, error code=0x26 
EXCEPTION STREAM:
  Exception info:TGID=12871, model id=0, stream id=4, stream phase=SCHEDULE
  Message info[0]:model execute fail: model_id=0, fail_stream_id=4, pid=12871
    Other info[0]:time=2024-07-24-01:36:21.363.092, function=check_stream_status, line=2108, error code=0x91
[ERROR] execute model failed, modelId is 1
[ERROR] acl execute failed:1
[ERROR] Execute Infer failed ret:-1
Traceback (most recent call last):
  File ""/usr/local/python3.7.5/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/python3.7.5/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/__main__.py"", line 18, in <module>
    exec(open(os.path.join(cur_path, ""infer/__main__.py"")).read())
  File ""<string>"", line 278, in <module>
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/infer/infer_process.py"", line 762, in infer_process
    main(args)
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/infer/infer_process.py"", line 490, in main
    warmup(session, args, intensors_desc, infileslist[0])
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/infer/infer_process.py"", line 166, in warmup
    outputs = run_inference(session, args, infeeds, out_array=True)
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/infer/infer_process.py"", line 182, in run_inference
    outputs = session.run(inputs, out_array)
  File ""/usr/local/python3.7.5/lib/python3.7/site-packages/ais_bench/infer/interface.py"", line 164, in run
    outputs = self.session.run(self.outputs_names, inputs)
RuntimeError: [-1][ACL: general failure] 
[INFO] unload model success, model Id is 1
[INFO] end to reset device 0
[INFO] end to finalize acl
Segmentation fault","open"
"https://gitee.com/ascend/tools/issues/I9V22E","ascend","tools","yolov10模型转换后无法进行推理","一、问题现象（附报错日志上下文）：
![输入图片说明](https://foruda.gitee.com/images/1717550192658063728/a6a006cb_6584308.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x): 8.0.RC1.alpha002
--Python 版本 (e.g., Python 3.7.5):3.9.2
--操作系统版本 (e.g., Ubuntu 18.04):Ubuntu 22.04

三、测试步骤：
 ./msame --model ""/root/yolo_sdk_python_sample/model/best_v10_640.om"" --output ""/root/yolo_sdk_python_sample/out_log"" --loop 5

后使用ais_bench工具进行推理，指令如下：
python -m ais_bench --model ""/root/yolo_sdk_python_sample/model/best_v10_640.om"" --loop 5
出现报错：
![输入图片说明](https://foruda.gitee.com/images/1717550394128059359/85fd39dd_6584308.png ""屏幕截图"")

四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/I9UKC1","ascend","tools","DVPPLite 是否有python 接口？",,"open"
"https://gitee.com/ascend/tools/issues/I9PG7D","ascend","tools","onnx,om使用msquickcmp工具度校验后，csv内全是Nan"," @zhang-guowen123_admin  @bobosiki  @alexcheng88  @gaozijuan  @AnRuiXiang 
我用一键式全流程精度比对（推理）这里的代码跑了一下，发现csv里面都是nan，是什么原因呢。我的模型接受四个输入，将其变成4个bin文件，并且也用了不指定输入默认为0 进行比对，cvs里面也还是nan 比如：Index,OpType,NPUDump,DataType,Address,GroundTruth,DataType,TensorIndex,Shape,CosineSimilarity,MaxAbsoluteError,AccumulatedRelativeError,RelativeEuclideanDistance,KullbackLeiblerDivergence,StandardDeviation,MeanAbsoluteError,RootMeanSquareError,MaxRelativeError,MeanRelativeError,CompareFailReason 0,Data,img_tensor,NaN,20067848814592,img_tensor,NaN,img_tensor:output:0,""[1,3,640,640]"",NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,[img_tensor] There is no the ground truth dump file for img_tensor:output:0. 1,Data,label_feats,NaN,20069320478720,label_feats,NaN,label_feats:output:0,""[80,768]"",NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,[label_feats] There is no the ground truth dump file for label_feats:output:0. 2,Data,task_feats,NaN,20069321015296,task_feats,NaN,task_feats:output:0,""[256,768]"",NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN,[task_feats] There is no the ground truth dump file for task_feats:output:0.


而且跑完后  终端最终打印的 结果  PartitionedCall_/decoder/Gather_2_const_data_385,PartitionedCall_/decoder/Gather_2_gatherv2_386 of the first operator whose cosine similarity is less than 0.9 是表示这几层输出有问题嘛，这应该是om内的命名方式把.

期望各位回复","open"
"https://gitee.com/ascend/tools/issues/I9N27H","ascend","tools","模型转换后运行报错","一、模型转换成功，但有warning信息。
转换命令：
atc --model ./test_1v1_sim.onnx --output ./test_1v1_sim --framework 5 --soc_version Ascend310B1

Warning信息，如下所示：
W11001: Op [Cast_76] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Div_78] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [MatMul_80] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Softmax_81] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Cast_82] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Cast_132] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Div_134] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [MatMul_136] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Softmax_137] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Cast_138] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_370] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_895] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_972] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_1049] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_460] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_793] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_537] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_614] does not hit the high-priority operator information library, which might result in compromised performance.
W11001: Op [Expand_691] does not hit the high-priority operator information library, which might result in compromised performance.

二、利用maname运行报错如下：
运行命令：./msame_loop --model model/test_1v1_sim.om --output ./

[INFO] acl init success
[INFO] open device 0 success
[INFO] create context success
[INFO] create stream success
[INFO] get run mode success
[INFO] load model model/test_1v1_sim.om success
[INFO] create model description success
[INFO] get input dynamic gear count success
[INFO] create model output success
.//202248_8_40_21_701723
2022-04-08 08:40:21.705124: E external/org_tensorflow/tensorflow/core/framework/node_def_util.cc:675] NodeDef mentions attribute input_para_type_list which is not in the op definition: Op<name=AscendBatchMatMulV2; signature=x1:Ta, x2:Tb, bias:Tout, offset_w:Tc -> y:Tout; attr=Ta:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=Tb:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=Tc:type,allowed=[DT_INT8]; attr=Tout:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=adj_x1:bool,default=false; attr=adj_x2:bool,default=false; attr=offset_x:int,default=0> This may be expected if your graph generating binary is newer than this binary. Unknown attributes will be ignored. NodeDef: {{node MatMul_80}}
[INFO] exception_cb streamId:4 taskId:6 deviceId: 0 opName:MatMul_80 inputCnt:4 outputCnt:1
[INFO] exception_cb hostaddr:0xaaaae03a8850 devaddr:0xe80000031000 len:2048 write to filename:exception_cb_index_0_input_0_format_2_dtype_11_shape_1x2x2x64.bin
[INFO] exception_cb hostaddr:0xaaaadec1e160 devaddr:0xe80000031c00 len:1024 write to filename:exception_cb_index_0_input_1_format_2_dtype_0_shape_1x2x64x2.bin
[WARN] exception_cb get failed addr:0xe800016e9200 len:0
[WARN] exception_cb input_2 save failed
[INFO] exception_cb hostaddr:0xaaaadf923ca0 devaddr:0xe80000043800 len:64 write to filename:exception_cb_index_0_output_0_format_0_dtype_11_shape_1x2x2x2.bin
E39999: Inner Error!
E39999 The error from device(chipId:0, dieId:0), serial number is 3, an exception occurred during AICPU execution, stream_id:4, task_id:6, errcode:5, msg:aicpu execute failed..[FUNC:ProcessStarsAicpuErrorInfo][FILE:device_error_proc.cc][LINE:1188]
TraceBack (most recent call last):
Aicpu kernel execute failed, device_id=0, stream_id=4, task_id=6, errorCode=91.[FUNC:PrintAicpuErrorInfo][FILE:task_info.cc][LINE:1457]
Aicpu kernel execute failed, device_id=0, stream_id=4, task_id=6, flip_num=0, fault so_name=, fault kernel_name=, fault op_name=, extend_info=(info_type:4, info_len:9, msg_info:MatMul_80).[FUNC:GetError][FILE:stream.cc][LINE:1483]
Fail to synchronize forbbiden stream, retCode=0x7150050![FUNC:SynchronizeExecute][FILE:model.cc][LINE:667]
Model synchronize execute failed, model_id=0![FUNC:GetStreamToSyncExecute][FILE:model.cc][LINE:707]
rtModelExecute execute failed, reason=[the model stream execute failed][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:50]
[Exec][Model]Execute model failed, ge result[507011], modelId[1][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
[Exec][Model]modelId[1] execute failed, result[507011][FUNC:ReportInnerError][FILE:log_inner.cpp][LINE:145]

[ERROR] execute model failed, modelId is 1","open"
"https://gitee.com/ascend/tools/issues/I9JIJ9","ascend","tools","pt2pb/README.md文件错字","“预置条件”-“1.pytorch模型文件”章节中，“pytorch模型保存有两种，一种是保存有权重参数和网络结构，另外一种是指保存权重参数”的说明，“指保存”应该为“只保存”","open"
"https://gitee.com/ascend/tools/issues/I9BLXU","ascend","tools","dsmi_get_memcory_info接口耗时长，需要优化","昇腾硬件设备：atlas300ipro
cann版本:6.0.1

程序运行后，当显存超过18G会崩溃，所以需要在程序中将显存使用量限制在18G以下。通过dsmi_get_memcory_info()获取显存使用情况，但该接口耗时2.9ms左右，影响业务效率，需要优化，请提供一个优化方案。","open"
"https://gitee.com/ascend/tools/issues/I6R3YE","ascend","tools","【ais_bench 推理工具】 批量跑数据时，dump单算子数据都放在同一个目录下","![输入图片说明](https://foruda.gitee.com/images/1680005940303161869/2150458d_11059285.png ""屏幕截图"")
ais_bench 推理批量数据时，指定dump单算子数据，所有数据的算子都保存在同一个文件夹下，
[图片上传中…(image-PI6GDF5KDrDK5a8GF3oS)]
分析数据困难，难以将输入和算子对应上","open"
"https://gitee.com/ascend/tools/issues/I9BHGJ","ascend","tools","推理出现问题","推理的时候出现以下问题
![输入图片说明](https://foruda.gitee.com/images/1711363887380437159/bf2b3b7d_10926410.png ""issue.png"")

这个是我模型转换的代码
if __name__ == '__main__':
    print(""\nModel: {}"".format(config.model))


    model = TimmModel(config.model,
                          pretrained=True,
                          img_size=config.img_size)
    
    data_config = model.get_config()
 
    print(""Start from:"", config.checkpoint_start)

    # resnet50_model = torch.load('resnet50.pth', map_location='cpu')

    model_state_dict = torch.load('pretrained/university/convnext_base.fb_in22k_ft_in1k_384/weights_e1_0.9515.pth', map_location='cpu')  
    model.load_state_dict(model_state_dict, strict=False)

    # model.load_state_dict(resnet50_model) 

    batch_size = 1  #批处理大小
    input_shape = (3, 384, 384)   #输入数据,改成自己的输入shape

    model.eval()

    dummy_input = torch.randn(batch_size, *input_shape) 

    torch.onnx.export(model, 
                      dummy_input, 
                      ""./test_cpu.onnx"", 
                      input_names = [""input""],   # 构造输入名
                      output_names = [""output""],    # 构造输出名
                      opset_version=11,    # ATC工具目前支持opset_version=9，10，11，12，13
                      )  #支持输出动态轴




    # onnx_path = './test.onnx'
    # print(""----- pth导出为onnx模型 -----"")

    # torch.onnx.export(model, dummy_input, onnx_path, export_params=True,input_names=['input'], output_names=['output'])
    print(""success!"")




config的部分
@dataclass
class Configuration:

    # Model
    model: str = 'convnext_base.fb_in22k_ft_in1k_384'
    
    # Override model image size
    img_size: int = 384
    
    # Evaluation
    batch_size: int = 2
    verbose: bool = True
    gpu_ids: tuple = (0,)
    normalize_features: bool = True
    eval_gallery_n: int = -1             # -1 for all or int
    
    # Dataset
    dataset: str = 'U1652-D2S'           # 'U1652-D2S' | 'U1652-S2D'
    data_folder: str = ""./data/U1652""
    
    # Checkpoint to start from
    checkpoint_start = 'pretrained/university/convnext_base.fb_in22k_ft_in1k_384/weights_e1_0.9515.pth'
    # checkpoint_start = 'university/convnext_base.fb_in22k_ft_in1k_384/project/weights_end.pth'
  
    # set num_workers to 0 if on Windows
    num_workers: int = 0 if os.name == 'nt' else 4 
    
    # train on GPU if available
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu' 
    

#-----------------------------------------------------------------------------#
# Config                                                                      #
#-----------------------------------------------------------------------------#

config = Configuration() ","open"
"https://gitee.com/ascend/tools/issues/I9BHDL","ascend","tools"," Check i:0 name:input in size:602112 needsize:1769472 not match","问题现象：
 Check i:0 name:input in size:602112 needsize:1769472 not match

[ERROR] Check InVector failed ret:-1
Traceback (most recent call last):
  File ""/root/classification/predict.py"", line 64, in <module>
    main()
  File ""/root/classification/predict.py"", line 48, in main
    output = torch.squeeze(model.infer([img.to(device)])[0]).cpu()
  File ""/root/miniconda3/envs/Sample4Geo/lib/python3.9/site-packages/ais_bench/infer/interface.py"", line 170, in infer
    return self.run(inputs, out_array=True)
  File ""/root/miniconda3/envs/Sample4Geo/lib/python3.9/site-packages/ais_bench/infer/interface.py"", line 95, in run
    outputs = self.session.run(self.outputs_names, inputs)
RuntimeError: [-1][ACL: general failure]

请问这个needsize和input in size具体是怎么计算的。
当时pth转onnx，dummy_input设置成(1,3,224,224)
在运行的时候 data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

","open"
"https://gitee.com/ascend/tools/issues/I97SZU","ascend","tools","ais-bench的下载地址www.aipubservice.com网页无内容","构建ais-bench性能测试软件包步骤中，从www.aipubservice.com下载ais-bench stubs基础测试工具包失败，网页好像没有内容？我浏览器访问也没有东西，请问是换地址了吗？","open"
"https://gitee.com/ascend/tools/issues/I97EDM","ascend","tools","YOLOV8模型转完OM文件后推理成功但精度损失较大","一、问题现象（附报错日志上下文）：
1. ONNX模型经OpenCV 4.8.0 DNN模块完成推理，结果如下：
![输入图片说明](https://foruda.gitee.com/images/1710125302182083406/f23f290d_9204526.jpeg ""onnx推理结果.jpg"")
2. OM模型基于Ascend C++接口推理，经msame工具验证，可以推理，msame工具测试结果如下：
![输入图片说明](https://foruda.gitee.com/images/1710125611469591648/3c013b8f_9204526.jpeg ""msame结果.jpg"")
推理结果如下：
![输入图片说明](https://foruda.gitee.com/images/1710130359508344494/0bdafba1_9204526.jpeg ""om结果.jpg"")

二、软件版本:
-- CANN 版本: 5.0.4  
--Tensorflow/Pytorch/MindSpore 版本: 模型训练使用的PyTorch，部署推理使用Ascend C++接口
--Python 版本: 3.7.5
-- MindStudio 版本: 未使用
--操作系统版本: Ubuntu 18.04

三、测试步骤：
ONNX模型使用atc命令转OM模型后，以一张有目标物体（挖掘机ECT）的图片作为测试用例


四、日志信息:
链接：https://pan.baidu.com/s/1mJvVIjova9WfvsYvSqLe1w?pwd=1qaz 
提取码：1qaz 
--来自百度网盘超级会员V4的分享","open"
"https://gitee.com/ascend/tools/issues/I94MMR","ascend","tools","请问 loadgen-0.0.1-cp36-cp36m-linux_aarch64.whl从哪里获取？",,"open"
"https://gitee.com/ascend/tools/issues/I94CV5","ascend","tools","使用 ais_bench 对 om 模型进行推理报错，如何确定报错算子是哪个？","一、问题现象（附报错日志上下文）：
使用 ais_bench 对 om 模型进行推理报错，如何确定报错算子是哪个？

二、软件版本:
-- CANN 版本：7.0.0alpha001  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：

python -m ais_bench --model xx.om --display_all_summary true --loop 10 --warmup_count 2

报错信息：

```
EE1001: The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        Solution: 1.Check the input parameter range of the function. 2.Check the function invocation relationship.
        TraceBack (most recent call last):
        The error from device(0), serial number is 25, there is an aicore error, core id is 0, error code = 0x10, dump info: pc start: 0x12404183a000, current: 0x12404183a18c, vec error info: 0xa3f57ff, mte error info: 0xb8, ifu error info: 0x2287b31a93900, ccu error info: 0x702612520024575d, cube error info: 0xa2, biu error info: 0, aic error mask: 0x65000200d000288, para base: 0x12404002e338, errorStr: Illegal instruction, which is usually caused by unaligned UUB addresses.[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:533]
        The extend info from device(0), serial number is 25, there is aicore error, core id is 0, aicore int: 0x81, aicore error2: 0, axi clamp ctrl: 0, axi clamp state: 0x1717, biu status0: 0x1c00800000000, biu status1: 0x940002092a0000, clk gate mask: 0, dbg addr: 0, ecc en: 0x3f3f, mte ccu ecc 1bit error: 0x1000, vector cube ecc 1bit error: 0, run stall: 0x1, dbg data0: 0, dbg data1: 0, dbg data2: 0, dbg data3: 0x1, dfx data: 0x1[FUNC:PrintCoreErrorInfo][FILE:device_error_proc.cc][LINE:564]
        The device(0), core list[0-0], error code is:[FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:587]
        coreId( 0):            0x10    [FUNC:PrintCoreInfoErrMsg][FILE:device_error_proc.cc][LINE:601]
        ctx is NULL![FUNC:MemCopySync][FILE:api_impl.cc][LINE:1594]
        The argument is invalid.Reason: rtMemcpy execute failed, reason=[context pointer null]
        ctx is NULL![FUNC:HostMalloc][FILE:api_impl.cc][LINE:1397]
        The argument is invalid.Reason: rtMallocHost execute failed, reason=[context pointer null]
        Call rtMallocHost failed, size:96, ret:0x1a1fa[FUNC:LogExceptionArgs][FILE:exception_dumper.cc][LINE:293]
        alloc host memory failed, runtime result = 107002[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        ctx is NULL![FUNC:GetDevErrMsg][FILE:api_impl.cc][LINE:4448]
        The argument is invalid.Reason: rtGetDevMsg execute failed, reason=[context pointer null]
```



四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/I8TX85","ascend","tools","Atlas200DK A2使用 msame没有权限问题","执行Msame报错：current user does not have permission
我的版本是7.0RC1，CANN是制卡工具自带的。
按照官方说明在HwHiAiUser下安装的工具
![输入图片说明](https://foruda.gitee.com/images/1704441719566474591/5652c6d0_9150832.jpeg ""12312.jpg"")","open"
"https://gitee.com/ascend/tools/issues/I8MQAH","ascend","tools","aclruntime文档在哪儿","前面一直使用acl加载及执行模型，最近发现存在aclruntime，但是从cann的文档中没有发现他的相关介绍及api文档，想问下aclruntime相较于acl有哪些优势，相关资料从哪儿找","open"
"https://gitee.com/ascend/tools/issues/I68DR0","ascend","tools","readme修改建议","需要用到msame，但在readme中未提及
![输入图片说明](https://foruda.gitee.com/images/1672668571698462771/c8c271b2_10722427.png ""屏幕截图"")
","open"
"https://gitee.com/ascend/tools/issues/I8MG6E","ascend","tools","芯片型号：SD3403/IPV928 MindStudio版本：MindStudio_2.2.4_linux，在精度分析profiling时出错"," **output：** 
2023-12-07 11:03:30  Run successfully, exit status: 0
2023-12-07 11:03:30  Start copying profiling data from remote...
2023-12-07 11:03:34  Done.
2023-12-07 11:03:34  The profiling data files are saved in: /root/ms_test/profiling/
2023-12-07 11:03:35  Synchronizing remote project changes to local...
2023-12-07 11:03:38  Done.
2023-12-07 11:03:38  End Profile Data Collection.
2023-12-07 11:03:38  Start parses profiling result!
2023-12-07 11:03:38  Run parse command: python3.7 /usr/local/Ascend/ascend-toolkit/6.10.t01spc030b070/toolkit/tools/profiler/profiler_tool/analysis/msprof/msprof.pyc export summary -dir  /root/ms_test/profiling --format=json
2023-12-07 11:03:38  Failed to parse profiling result.
2023-12-07 11:03:38  Run command python3.7 /usr/local/Ascend/ascend-toolkit/6.10.t01spc030b070/toolkit/tools/profiler/profiler_tool/analysis/msprof/msprof.pyc export summary -dir  /root/ms_test/profiling --format=json failed. reason: null
![输入图片说明](https://foruda.gitee.com/images/1701918270370678318/d35446f5_12751417.png ""微信图片_20231207110417.png"")
工程目录下的profiling文件夹下无法出现“JOB.....”文件
当我自己手动在profiling下创建一个“JOB.....”后，报错信息改变：
2023-12-07 10:53:59  Run successfully, exit status: 0
2023-12-07 10:53:59  Start copying profiling data from remote...
2023-12-07 10:54:03  Done.
2023-12-07 10:54:03  The profiling data files are saved in: /root/ms_test/profiling/
2023-12-07 10:54:04  Synchronizing remote project changes to local...
2023-12-07 10:54:07  Done.
2023-12-07 10:54:07  End Profile Data Collection.
2023-12-07 10:54:07  Start parses profiling result!
2023-12-07 10:54:07  Run parse command: python3.7 /usr/local/Ascend/ascend-toolkit/6.10.t01spc030b070/toolkit/tools/profiler/profiler_tool/analysis/msprof/msprof.pyc export summary -dir  /root/ms_test/profiling --format=json
2023-12-07 10:54:07  Thu 07 Dec 2023 10:54:07 [WARNING] [MSVP] [38939] msprof_export.pyc: Invalid parsing dir(""/root/ms_test/profiling/JOBZHOSMWBCHAGQXFKRMCYRKOXJNFJXL""), dir must be profiling data dir or it's parent directory
2023-12-07 10:54:07  Run parse command: python3.7 /usr/local/Ascend/ascend-toolkit/6.10.t01spc030b070/toolkit/tools/profiler/profiler_tool/analysis/msprof/msprof.pyc export timeline -dir  /root/ms_test/profiling
2023-12-07 10:54:07  Thu 07 Dec 2023 10:54:07 [WARNING] [MSVP] [38941] msprof_export.pyc: Invalid parsing dir(""/root/ms_test/profiling/JOBZHOSMWBCHAGQXFKRMCYRKOXJNFJXL""), dir must be profiling data dir or it's parent directory
2023-12-07 10:54:07  End parsing profile data.
2023-12-07 10:54:07  Start to merge data into JSON file.
2023-12-07 10:54:07  Failed to merge data into a JSON file because parsedJsonDir is not a directory!
2023-12-07 10:54:07  Failed to merge data into a JSON file because parsedJsonDir is not a directory!
2023-12-07 10:54:07  Failed to merge JSON. The obtained merged JSON path is empty!
2023-12-07 10:54:07  End merging data into a JSON file.
![输入图片说明](https://foruda.gitee.com/images/1701918015538436134/3e48421e_12751417.png ""微信图片_20231207095155.png"")



","open"
"https://gitee.com/ascend/tools/issues/I8M3G5","ascend","tools","ascend 改造过的netron版本哪里能下载？","通过atc模型转换后，用公网的netron看om，发现是空白的，ascend 改造过的netron版本哪里能下载？","open"
"https://gitee.com/ascend/tools/issues/I8EWKC","ascend","tools","msprof 性能数据导出报错","msprof 性能数据导出报错

使用 query 查询结果：
![](https://foruda.gitee.com/images/1699433380315055052/6063f5c0_9106882.png ""屏幕截图"")

导出数据时候报错：
![输入图片说明](https://foruda.gitee.com/images/1699433417323700796/7db1334c_9106882.png ""屏幕截图"")

cann 版本：6.3.RC2.alpha002","open"
"https://gitee.com/ascend/tools/issues/I8320S","ascend","tools","【ptdbg_ascend文档】描述有歧义","https://gitee.com/ascend/tools/blame/master/ptdbg_ascend/doc/ptdbg_ascend%E7%B2%BE%E5%BA%A6%E5%B7%A5%E5%85%B7%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E_v3.4.md

源码的478行对filter_switch的解释中 hook_name无论取dump还是overflow_check给的描述都是""默认不配置"",该描述如何理解，且该描述后一个跟着""filter_switch=""ON"""",另一个跟着 ""filter_switch=""OFF""""

468行原文如下：
| filter_switch     | dump bool和整型的tensor以及浮点、bool和整型的标量的过滤开关。可取值""ON""（表示开启过滤，即不dump）或""OFF""（表示关闭过滤）。参数示例：filter_switch=""OFF""。PrecisionDebugger模块hook_name=dump时，默认不配置，即filter_switch=""ON""，表示过滤上述数据；PrecisionDebugger模块hook_name=overflow_check时，默认不配置，即filter_switch=""OFF""，表示dump上述数据。 | 否       |","open"
"https://gitee.com/ascend/tools/issues/I7WY56","ascend","tools","【ptdbg_ascend】建议在README中添加对于标量计算余弦相似度结果为1的解释，避免客户误解","问题现象：
建议在README中添加对于标量计算余弦相似度结果为1的解释，避免客户误解
初步分析：

软件版本：

日志信息：
--提供方式 
建议打包后上传到网盘中将下载链接粘贴回复内容

复现步骤：
","open"
"https://gitee.com/ascend/tools/issues/I7VEQM","ascend","tools","模型推理程序使用多stream方案，stream同步卡死。","模型推理程序使用多stream进行同步会发生卡死，需要提供多stream能力。","open"
"https://gitee.com/ascend/tools/issues/I7UTM9","ascend","tools","【ais-bench】aisbench提供了whl 包，但是没有提供完整性校验的机制","需要提供README中whl包的哈希校验方式","open"
"https://gitee.com/ascend/tools/issues/I7MLY5","ascend","tools","msame推理报错Inner Error","转模型使用了input_shape_range
转化命令：atc --model=embed_tokens.onnx --framework=5 --log=info --input_format=ND --output=embed_tokens.om  --input_shape_range=""seg_t:[1,1~100]"" --out_nodes=""/Gather:0"" --soc_version=Ascend310P3 --log=debug

用msame推理出现如下日志错误：
root@3e8faf9789dd:/home/lichunxiang/model# /home/lichunxiang/software/tools-master_new/tools-master/msame/out/main --model ""embed_tokens.om"" --output ""./"" --outfmt BIN --loop 100 --outputSize ""100000"" --input ""seg.bin"" --dymShape ""seg_t:1,42""
[INFO] acl init success
[INFO] open device 0 success
[INFO] create context success
[INFO] create stream success
[INFO] get run mode success
[INFO] load model embed_tokens.om success
[INFO] create model description success
[INFO] get input dynamic gear count success
[INFO] check Dynamic Shape success
[INFO] create model output success
.//2023720_6_18_52_453810
[INFO] start to process file:seg.bin
[INFO] set Dynamic shape success
[INFO] model execute success
Inference time: 1.259ms
[WARN] exception_cb deviceId:0 streamId:6 taskId:2 failed:500002
EZ9999: Inner Error!
EZ9999  Kernel task happen error, retCode=0x26, [aicore exception].[FUNC:PreCheckTaskErr][FILE:task.cc][LINE:1163]
        TraceBack (most recent call last):
        get opdesc info failed, device_id:0, stream_id:6, task_id:2.[FUNC:GetOpDescInfo][FILE:ge_executor.cc][LINE:1324]
        [Get][OpDescInfo]get op desc faild, ge result[-1], deviceId[0], streamId[6], taskId[2][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        Aicore kernel execute failed, device_id=0, stream_id=6, report_stream_id=6, task_id=2, flip_num=0, fault kernel_name=te_gatherv2_f509d81cdb7dc9b84aa2010a56a1919d7da3acc7aa2e60f2ab7c8c938d5b467a_1_900015010, program id=0, hash=11876320769233916746.[FUNC:GetError][FILE:stream.cc][LINE:1133]
        rtStreamSynchronize execute failed, reason=[aicore exception][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:49]
        Call rtStreamSynchronize(default_stream_) fail, ret: 0x7BC87[FUNC:ExecuteSync][FILE:model_v2_executor.cc][LINE:193]
        [Exec][Model]Execute model failed, ge result[507015], modelId[2147483648][FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161]
        [Exec][Model]modelId[2147483648] execute failed, result[507015][FUNC:ReportInnerError][FILE:log_inner.cpp][LINE:145]


DEVICE[0] PID[46370]:
EXCEPTION STREAM:
  Exception info:TGID=21665, model id=65535, stream id=6, stream phase=SCHEDULE
  Message info[0]:RTS_HWTS: Aicore exception, slot_id=13, stream_id=6
    Other info[0]:time=2023-07-20-14:18:45.973.740, function=process_hwts_error_exception, line=2720, error code=0x26
[ERROR] execute model failed, modelId is 2147483648
Inference time: 119.97ms
[ERROR] model execute failed
[INFO] unload model success, model Id is 2147483648
[INFO] destroy model input success
[ERROR] Sample process failed
[INFO] end to destroy stream
[INFO] end to destroy context
[INFO] end to reset device is 0
[INFO] end to finalize acl
","open"
"https://gitee.com/ascend/tools/issues/I7F5D5","ascend","tools","Stubs压缩包提供的下载地址无法下载","![输入图片说明](https://foruda.gitee.com/images/1687316305118778096/d7c6e703_8835794.png ""1687316213123.png"")
请问是否下载地址失效，如是请提供一下新的下载地址谢谢！","open"
"https://gitee.com/ascend/tools/issues/I7CESS","ascend","tools","在使用msame进行推理时出现错误Segmentation fault","板卡为Atlas 200 DK，操作系统为ubuntu18.04，使用msame工具推理om模型时发生错误","open"
"https://gitee.com/ascend/tools/issues/I7ANDU","ascend","tools","get_max_relative_err没有对除以0进行处理","一、问题：
如图， get_max_relative_err没有对除以0进行处理， 可能导致后续数据中有nan， 走入np.isnan的分支， 导致函数基本不可用
![输入图片说明](https://foruda.gitee.com/images/1685786965579723665/e449d0de_4987488.png ""未命名图片.png"")

PS：用的是master分支
","open"
"https://gitee.com/ascend/tools/issues/I77SGG","ascend","tools","【msquickcmp精度比对工具】没有对om输出节点的数据类型进行校验","目前已有输入节点的数据类型强校验，在atc命令中修改输入节点的数据类型后，会无法调用msame进行比对；但是对输出节点没有数据类型校验，会导致精度比对结果能够正常输出，但是结果全部都是错误的。","open"
"https://gitee.com/ascend/tools/issues/I73Z4V","ascend","tools","npu算力切分后无法挂载到容器","系统：麒麟v10，驱动：Ascend-hdk-910-npu-driver_6.0.0_linux-aarch64.run，
CANN版本：Ascend-cann-toolkit_6.0.1_linux-aarch64.run
容器内驱动版本与物理机驱动版本一致
Atlas 910B卡30核切分为vir16核，vir08核
![输入图片说明](https://foruda.gitee.com/images/1684321238928139735/df1f782c_9153221.jpeg ""mmexport1684321280039(1).jpg"")
容器启动命令：
![输入图片说明](https://foruda.gitee.com/images/1684321786765838875/475dd195_9153221.png ""屏幕截图"")
容器启动正常，但无法调用npu；报错如下：
![输入图片说明](https://foruda.gitee.com/images/1684321657781588048/b5dcd90d_9153221.jpeg ""mmexport1684321659245(1).jpg"")
![输入图片说明](https://foruda.gitee.com/images/1684321673265984266/9ad9d088_9153221.jpeg ""mmexport1684321600727(1).jpg"")","open"
"https://gitee.com/ascend/tools/issues/I71F6H","ascend","tools","i3d模型输入很大时（700M）构造数据非常慢。卡了40s","i3d模型输入很大时（700M）构造数据非常慢。卡了40s

代码如下：
def get_pure_infer_data(size, pure_data_type):
    lst = []
    if pure_data_type == ""random"":
        # random value from [0, 255]
        lst = [random.randrange(0, 256) for _ in range(size)]
    else:
        # zero value, default
        lst = [0 for _ in range(size)]

    barray = bytearray(lst)
    ndata = np.frombuffer(barray, dtype=np.uint8)
    return ndata
需要完善和优化","open"
"https://gitee.com/ascend/tools/issues/I70F1C","ascend","tools","使用精度对比工具msquickcmp报错ONNXRuntimeError","一、问题现象（附报错日志上下文）：
使用msquickcmp工具执行报错
python3 main.py -m ./yolov5s_nms.onnx -om ./yolov5s_nms_bs1_0413.om -i ./input.bin -c /usr/local/Ascend/ascend-toolkit/latest -o ./dump_result/
![输入图片说明](https://foruda.gitee.com/images/1683252819786710102/7feb6ba9_11983859.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 :5.1.2rc
--Pytorch 版本:1.10.0
--Python 版本:3.7.5
--操作系统版本:Ubuntu 18.04 容器
![输入图片说明](https://foruda.gitee.com/images/1683254582263646666/bb480592_11983859.png ""屏幕截图"")

三、测试步骤：
1.根据链接文档，生成yolov5s_nms.onnx，xhttps://gitee.com/ascend/modelzoo-GPL/tree/master/built-in/ACL_Pytorch/Yolov5_for_Pytorch
大致步骤：wget https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt
bash pth2onnx.sh --tag 4.0 --model yolov5s --nms_mode nms_op （简化没有成功，但也导出了onnx）

2.使用aipp_yolov5.cfg文件，将yolov5s_nms.onnx转换为yolov5s_nms_bs1_0413.om（om模型可以成功推理）
atc --framework=5 --model=yolov5s_nms.onnx --output=yolov5s_nms_bs1_0413 --input_format=NCHW --input_shape=""images:1,3,640,640;img_info:1,4"" --log=error --soc_version=Ascend310P3 --insert_op_conf=aipp_yolov5.cfg --optypelist_for_implmode=""Sigmoid"" --op_select_implmode=high_performance

3.将两个模型复制到msquickcmp路径下，先执行文件生成模型的输入数据（.bin），文件内容如下：（/opt/tools/msquickcmp/input_bin/中只有一张图片1280*720）
![输入图片说明](https://foruda.gitee.com/images/1683253723886888780/e1da92b1_11983859.png ""屏幕截图"")
生成后执行：python3 main.py -m ./yolov5s_nms.onnx -om ./yolov5s_nms_bs1_0413.om -i ./input.bin -c /usr/local/Ascend/ascend-toolkit/latest -o ./dump_result/

4.报错
![输入图片说明](https://foruda.gitee.com/images/1683252819786710102/7feb6ba9_11983859.png ""屏幕截图"")

环境信息：
![](https://foruda.gitee.com/images/1683254540189331160/5c34c68b_11983859.png ""屏幕截图"")

yolov5s_nms.onnx；yolov5s_nms_bs1_0413.om；aipp_yolov5.cfg
百度网盘链接：链接：https://pan.baidu.com/s/1RTMxBX-gOCV38roZcy4cPQ?pwd=wz54 
提取码：wz54

","open"
"https://gitee.com/ascend/tools/issues/I6YRZV","ascend","tools","save out files error","[ERROR] save out files error array shape:(1, 16128, 198) filesinfo:[['test.npy', 'padding_infer_fake_file', 'padding_infer_fake_file', 'padding_infer_fake_file']] files_count_perbatch:4 ndata.shape0:1
Inference array Processing:   0%|                                                                                                | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File ""/usr/local/python3.9.2/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/local/python3.9.2/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/python3.9.2/lib/python3.9/site-packages/ais_bench/__main__.py"", line 3, in <module>
    exec(open(os.path.join(cur_path, ""infer/__main__.py"")).read())
  File ""<string>"", line 402, in <module>
  File ""<string>"", line 324, in main
  File ""<string>"", line 160, in infer_loop_array_run
  File ""/usr/local/python3.9.2/lib/python3.9/site-packages/ais_bench/infer/io_oprations.py"", line 203, in save_tensors_to_file
    raise RuntimeError()
RuntimeError
","open"
"https://gitee.com/ascend/tools/issues/I6VZ9C","ascend","tools","导入ais报错","一、问题现象（附报错日志上下文）：
导入ais报错
![输入图片说明](https://foruda.gitee.com/images/1681550182687877533/c5cd619e_4979999.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 (e.g., CANN 6.0.1):  
--Python 版本 (e.g., Python 3.7.5):
--操作系统版本 (e.g., Ubuntu 20.04):","open"
"https://gitee.com/ascend/tools/issues/I6UVNL","ascend","tools","200dk 扩容教程大概率失败","您好，在官方的文档中：
[Atlas200dk合设环境搭建](https://gitee.com/ascend/samples/wikis/%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/Atlas200dk%E5%90%88%E8%AE%BE%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA--%E7%94%A8%E9%95%9C%E5%83%8F%E6%81%A2%E5%A4%8D%E7%9A%84%E6%96%B9%E5%BC%8F) 最后提到的磁盘扩容方法在Atlas 200dk上失效，已测试文档中CANN 5.1rc1的三个版本，设备使用reboot重启可以正常开机，但在断电重启后，均无法正常加载，请问是否有其他使用 dd镜像的扩容方法？","open"
"https://gitee.com/ascend/tools/issues/I6SKEX","ascend","tools","ptdbg_ascend的dump张量支持异步dump，加快dump速度","当前在def dump_tensor(x, prefix, dump_step, dump_file_name): 函数中采用np.save的阻塞式dump，建议支持新开线程进行异步dump，可大幅提升dump速率。

当numpy的大小为128M时，保存耗时约300ms。若每步有100个算子大小为128M，则需要30s，通过异步可几乎完全隐藏在算子执行中。可将np.save替换为一下函数，实测可行：


```
def async_save(output_path, tensor):
    import threading
    t = threading.Thread(target=np.save, args=(output_path, tensor))
    t.start()
```
","open"
"https://gitee.com/ascend/tools/issues/I6QUJT","ascend","tools","【ais-bench 推理工具】使用--dymShape_range参数执行推理失败","一、问题现象（附报错日志上下文）：
ais-bench 推理工具使用--dymShape_range参数执行推理失败

二、软件版本:
-- CANN 版本 ：CANN 6.0.2 

--Python 版本：Python 3.7.5


三、测试步骤：

python3 -m ais_bench --model=encoder.om --dymShape_range=""input_data:1~8,12,160;input_mask:1·812，160；..."" --outputSize=""40000000,40000000,..."" --output out --loop 10 --device 1

四、日志信息:

执行失败，MemoryError","open"
"https://gitee.com/ascend/tools/issues/I6PPF8","ascend","tools","ais_bench 没有校验输入的大小","当模型的输入要求是 dtype=float32时，输入的数据的dtype=float64,这时候会读成2个文件，导致推理的精度为0
建议检验模型所需大小 （size / batch） 和输入文件的大小，来给用户一个提示","open"
"https://gitee.com/ascend/tools/issues/I6POHQ","ascend","tools","bilstmcrf执行一键精度对比时报错","一：参考https://gitee.com/ascend/ModelZoo-TensorFlow/tree/master/ACL_TensorFlow/contrib/nlp/Transformer_for_ACL/
transformer的pb模型转换om成功,推理成功。
转换命令：  
![输入图片说明](https://foruda.gitee.com/images/1679571750069914606/bc537667_9153221.png ""屏幕截图"")
现在需要用精度比对工具测试，报错如下：  
```
python3 main.py -m /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/Transformer_for_ACL/save/model/transformer_tf.pb -om /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/Transformer_for_ACL/save/model/transformor_batchsize_1.om -c /usr/local/Ascend/ascend-toolkit/latest -o /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/Transformer_for_ACL/ -s ""input:1,128""
```
![输入图片说明](https://foruda.gitee.com/images/1679621588925390556/156be716_9153221.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1679622712023200761/0f985466_9153221.png ""屏幕截图"")

二：参考https://gitee.com/ascend/ModelZoo-TensorFlow/tree/master/ACL_TensorFlow/contrib/nlp/seq2seq_ID1474_for_ACL/
seq2seq的pb模型转om成功，推理成功。
转换命令：
![输入图片说明](https://foruda.gitee.com/images/1679621766049593237/96856ece_9153221.png ""屏幕截图"")
需要精度比对工具测试，报错如下：
```
python3 main.py -m /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/seq2seq_ID1474_for_ACL/pb_model/seq2seq.pb -om /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/seq2seq_ID1474_for_ACL/om_model/seq2seq.om -c /usr/local/Ascend/ascend-toolkit/latest -o /home/xxz/ModelZoo-TensorFlow-master/ACL_TensorFlow/contrib/nlp/seq2seq_ID1474_for_ACL/om_model/ -s ""encoder0:1;encoder1:1;encoder2:1;encoder3:1;encoder4:1;encoder5:1;encoder6:1;encoder7:1;encoder8:1;encoder9:1;encoder10:1;encoder11:1;encoder12:1;encoder13:1;encoder14:1;encoder15:1;encoder16:1;encoder17:1;encoder18:1;encoder19:1;encoder20:1;encoder21:1;encoder22:1;encoder23:1;encoder24:1;encoder25:1;encoder26:1;encoder27:1;encoder28:1;encoder29:1;encoder30:1;encoder31:1;encoder32:1;encoder33:1;encoder34:1;encoder35:1;encoder36:1;encoder37:1;encoder38:1;encoder39:1;decoder0:1""
```
![输入图片说明](https://foruda.gitee.com/images/1679622666109321327/f988d5c9_9153221.png ""屏幕截图"")","open"
"https://gitee.com/ascend/tools/issues/I6OVCJ","ascend","tools","tools下single_op_test里面的错误解决描述比较模糊，让人难以理解","假设Toolkit软件包安装目录为： /usr/local/Ascend。则命令为：

cd /usr/local/Ascend/toolkit/tools/operator_cmp/compare

上面的tools中的operator_cmp不知道在哪","open"
"https://gitee.com/ascend/tools/issues/I6O6FI","ascend","tools","【ais-bench推理工具】【英文版说明没有更新】","【ais-bench推理工具】【英文版说明没有更新】","open"
"https://gitee.com/ascend/tools/issues/I6O6F8","ascend","tools","【ais-bench推理工具】【模型推理运行正常，析构失败】","【ais-bench推理工具】【模型推理运行正常，析构失败】
反馈人：dingli
请联系并进行分析","open"
"https://gitee.com/ascend/tools/issues/I6O6ES","ascend","tools","【ais-bench推理工具】【64bs resnet模型端到端耗时过长，需要优化】","通过interface接口。64bs resnet模型端到端耗时过长，需要优化","open"
"https://gitee.com/ascend/tools/issues/I6O6EK","ascend","tools","【ais-bench推理工具】【动态aipp支持】","支持动态aipp。当前不支持。","open"
"https://gitee.com/ascend/tools/issues/I6O6DC","ascend","tools","【ais-bench推理工具】【错误信息完善】","很多情况下，aclruntime的一些错误。都看不出具体信息。需要做个进行完善。正确提示
经常提示 ACL: general failure","open"
"https://gitee.com/ascend/tools/issues/I6O6CY","ascend","tools","【ais-bench推理工具】【错误码提示不清晰，需要完善】","模型有30个输入。其中一个输入的大小不对。这种情况下打印提示不直观。不知道那个文件有问题。
错误信息见附件。需要完善

反馈人：tianyinghui","open"
"https://gitee.com/ascend/tools/issues/I6O6B8","ascend","tools","【ais-bench推理工具】【多输入输出参数时拷贝耗时不准确】","如果一个模型有多个输入参数。 结束的汇总数据 H2D的数据 是按照所有拷贝的平均值进行统计的。
这样不能体现一个总体耗时情况。
调研下，是否算上总共的耗时和。还是说沿用当前场景。
输出也是一样","open"
"https://gitee.com/ascend/tools/issues/I6O6AL","ascend","tools","【ais-bench推理工具】增加宏，开启GE profiling模式","有些场景下 客户可能只想通过 acl.json进行设置。直接开启GE profliing 而不是 msprof。所以建议增加一个宏。
如果有客户有需要。那就通过 acl.json进行使能。

代码增加 no_msprof = os.getenv(""NO_MSPROF"")
        if no_msprof:
                不进入msprof拉起分支
        else:
              which(""msprof"")
              xxxxx


反馈人：dingli","open"
"https://gitee.com/ascend/tools/issues/I6MQQD","ascend","tools","ATC onnx 模型转换失败 No parser is registered for Op [NonZero_329, optype [ai.onnx::11::NonZero]]","一、问题现象（附报错日志上下文）：
开源fast-bev模型从pytorch 转onnx成功，并验证精度准确
在用atc工具从onnx转 om 格式的时候，遇到报错：
No parser is registered for Op [NonZero_329, optype [ai.onnx::11::NonZero]].
No parser is registered for Op [NonZero_362, optype [ai.onnx::11::NonZero]].

原onnx模型里 NonZero 算子input是 bool 型tensor， output 是int64 tensor

二、软件版本:
-- Ascend-cann-toolkit_5.0.mdc610_linux-x86_64.run
   CANN-aoe-3.20.109.0.b371-ubuntu20.04.x86_64.run
--Python 版本 (e.g., Python 3.7.5):
--操作系统版本 (e.g., Ubuntu 20.04):

","open"
"https://gitee.com/ascend/tools/issues/I6L14D","ascend","tools","ais_bench多卡推理场景，推理结果目录只有一个，多个卡的结果互相覆盖","1. 问题：
ais_bench多卡推理场景，推理结果目录只有一个，多个卡的结果互相覆盖

2. 复现方法：
多卡推理指令参考：
python3 -m ais_bench --model ./models/om/bert_base_chinese_bs64.om --input ./preprocessed_data/input_data --output ./output_data --output_dirname bs64 --batchsize 64 --device 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 --display_all_summary=true

3. 初步定位原因
代码中推理结果目录以output_dirname命名，或者时间戳命名
![输入图片说明](https://foruda.gitee.com/images/1678259720411759349/f5d123fa_8833045.png ""ais_bench.PNG"")

（1）如果指定output_dirname，每张卡的目录名都一样，会互相覆盖
（2）如果不指定output_dirname,默认会以时间戳命名，由于多进程执行时间差异很小，基本在秒级以下，所以时间戳目录也是一样的，也会互相覆盖

4. 建议：
建议多卡推理场景区分推理结果目录，可以用卡号来区分","open"
"https://gitee.com/ascend/tools/issues/I6KNTU","ascend","tools","pt溢出检测报错原因","一、问题现象（附报错日志上下文）：
使用ptdbg_ascend进行溢出检测，检测到1次溢出后，报错
二、软件版本:
ptdbg_ascend

三、测试步骤：
使用ptdbg_ascend进行溢出检测，检测到1次溢出后，报错


四、日志信息:
[图片上传中…(image-R8LpnQm7l1no9arJO30S)]
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/I6J3QF","ascend","tools","ais_benchdump数据用msaccucmp.py对比仅显示算子输出，msamedump数据正常","![输入图片说明](https://foruda.gitee.com/images/1677747948501567559/3c5e71a5_10978145.png ""屏幕截图"")
联系00619759","open"
"https://gitee.com/ascend/tools/issues/I6IVKW","ascend","tools","ssd-mobilev2 tensorflow模型迁移问题","一、问题现象（附报错日志上下文）：
安装指导教程进行迁移后，训练过程中报错

二、软件版本:
-- CANN 版本 ：6.0.1  
--Tensorflow 版本:1.15.0
--Python 版本：3.7.5
-- MindStudio版本：未安装
--操作系统版本：ubuntu20.04.1的版本

三、测试步骤：
1.pip install -r requirements.txt
2.pip install zy_od_1.0.tar.gz
3.pip install zy_slim_1.0.tar.gz
4.python train.py --logtostderr --train_dir checkpoint --pipeline_config_path data/cat_dog.config


四、日志信息:
![输入图片说明](https://foruda.gitee.com/images/1677678910940203033/71881038_12527974.png ""日志1.png"")
![输入图片说明](https://foruda.gitee.com/images/1677678921597717981/56ec5146_12527974.png ""日志2.png"")
![输入图片说明](https://foruda.gitee.com/images/1677678930745334706/f829bc1d_12527974.png ""日志3.png"")

五、代码及数据集
链接：https://pan.baidu.com/s/1_5FWydOeyE9H--AFniQWKw 
提取码：8ji4","open"
"https://gitee.com/ascend/tools/issues/I6ISAT","ascend","tools","iresnet模型om输出与onnx输出不匹配","一、问题现象（附报错日志上下文）：
客户的iresnet模型om输出与onnx输出不匹配，余弦相似度只有70%
使用msquickcmp工具进行分析后发现从一地个BN层开始出现问题：
![输入图片说明](https://foruda.gitee.com/images/1677657751583110030/2334465c_8051247.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1677658320142637493/37b20282_8051247.png ""屏幕截图"")
这个问题是客户现场着急交付的模型，还请各位专家尽快支持一下","open"
"https://gitee.com/ascend/tools/issues/I6IJ6X","ascend","tools","显存足够的情况下ais_bench推理报错显存溢出","显存为20g，输出4g提示显存不够，联系00619759
![输入图片说明](https://foruda.gitee.com/images/1677586306668597013/7a582e68_10978145.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1677586315103798358/fcb252a1_10978145.png ""屏幕截图"")","open"
"https://gitee.com/ascend/tools/issues/I6IC54","ascend","tools","om模型文件有可视化查看工具吗，netron打不开","问题现象：
atc从AIR文件转成的OM，是否有工具可以打开查看","open"
"https://gitee.com/ascend/tools/issues/I6HX32","ascend","tools","系统回根据LD_LIBRARY_PATH环境变量查找","问题现象：系统回根据LD_LIBRARY_PATH环境变量查找 “回”字输入错误，是否更正为“会”？

初步分析：readme文档更正

软件版本：

日志信息：
--提供方式 
建议打包后上传到网盘中将下载链接粘贴回复内容

复现步骤：
","open"
"https://gitee.com/ascend/tools/issues/I6HL4Q","ascend","tools","ATC模型转换失败","转换一个LSTM的模型失败，由onnx转om。
报错：
EZ3002: Optype [DynamicRNN] of Ops kernel [AIcoreEngine] is unsupported. Reason: [tbe-custom]:op type DynamicRNN is not found in this op store.
[tbe-builtin]:[Static shape check]:data type DT_FLOAT of input [x] is not supported. All supported data type and format of tensor input0.x is: 
Data Type: {DT_FLOAT16,DT_FLOAT16,DT_FLOAT16,DT_FLOAT16}
Format:{FRACTAL_NZ,FRACTAL_NZ,FRACTAL_NZ,FRACTAL_NZ}
.
        No supported Ops kernel and engine are found for [LSTM_8/DynamicRnn], optype [DynamicRNN].
        No supported Ops kernel and engine are found for [LSTM_18/DynamicRnn], optype [DynamicRNN].
        No supported Ops kernel and engine are found for [LSTM_28/DynamicRnn], optype [DynamicRNN].
        build graph failed, graph id:0, ret:-1[FUNC:BuildModel][FILE:ge_generator.cc][LINE:1419]
","open"
"https://gitee.com/ascend/tools/issues/I6F0T9","ascend","tools","ais_bench 推理程序的 help 文本疑似有误","问题现象：
ais_bench 推理程序的 help 文本疑似有误。
对 --loop 选项的描述文本中单词 ""PrueInfer"" 疑似有误，也许是纯推理的英文 ""PureInfer"" 字母顺序错乱所致。
```
 --loop LOOP, -l LOOP  the round of the PrueInfer.
```
出现问题的 ais_bench 版本为 0.0.2 ，通过命令一键式编译安装。

软件版本：
ais_bench 0.0.2

日志信息：
该问题无需提供日志信息。

复现步骤：
通过命令一键式编译安装 ais_bench :
```
pip3 install -v 'git+https://gitee.com/ascend/tools.git#egg=aclruntime&subdirectory=ais-bench_workload/tool/ais_bench/backend'
```
指定 `--help` 选项查看帮助文本即得。
```
python3 -m ais_bench --help
```
","open"
"https://gitee.com/ascend/tools/issues/I6DDHB","ascend","tools","msprof命令行方式采集性能数据报错","![输入图片说明](https://foruda.gitee.com/images/1675738850891864301/344deb90_10754915.png ""屏幕截图"")
训练任务结束，提示profiling failed
![输入图片说明](https://foruda.gitee.com/images/1675738932318909818/f45d495f_10754915.png ""屏幕截图"")
程序运行过程中生成了这个文件
![输入图片说明](https://foruda.gitee.com/images/1675738993847836692/1c8155ee_10754915.png ""屏幕截图"")
![输入图片说明](https://foruda.gitee.com/images/1675739056798318930/9b00d05f_10754915.png ""屏幕截图"")
日志debug/plog输出错误信息，File size is invalid. fileName:606808_profiling_output_record, size:0.","open"
"https://gitee.com/ascend/tools/issues/I6CQ2K","ascend","tools","安装aclruntime时报错subprocess-exited-with-error","一、问题现象（附报错日志上下文）：
![输入图片说明](https://foruda.gitee.com/images/1675407352136455432/3527ba8b_11983859.png ""屏幕截图"")

二、软件版本:
-- CANN 版本 (CANN 6.0.RC1):  
--Tensorflow/Pytorch/MindSpore 版本:1.13.1
--Python 版本 (Python 3.7.5):
--操作系统版本 (Ubuntu 20.04):

三、测试步骤：
1.下载tool代码
2.进入到路径ais-bench_workload/tool/ais_bench
3.pip3 wheel ./backend/ -v 报错


四、日志信息:
链接: https://pan.baidu.com/s/1OkVxh9T9UDt5uw-YWgK6bQ?pwd=tv72 提取码: tv72 复制这段内容后打开百度网盘手机App，操作更方便哦","open"
"https://gitee.com/ascend/tools/issues/I6BRB2","ascend","tools","pt溢出检测工具报错","一、问题现象（附报错日志上下文）：
""/usr/local/python3.7.5/lib/python3.7/site-packages/ptdbg_ascend/hooks/hooks.py""
![输入图片说明](https://foruda.gitee.com/images/1675066241563830314/5cf6753e_8537511.png ""屏幕截图"")

对于使用torch_npu.contrib.transfer_to_npu迁移的模型来说，torch.cuda.is_available实际上执行的是torch.npu.is_available，这里逻辑会有问题。建议使用if not torch.npu.is_available或捕获相关异常来控制，工具中有多处类似的问题

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/I6A88C","ascend","tools","vscode找不到acl.h","问题现象：已配置includepath，但依旧报错找不到acl/acl.h，如下所示：
可以看到c/c++ diagnostic下输出的路径下的确有acl.h头文件：
![可以看到c/c++ diagnostic下输出的路径下的确有acl.h头文件](https://foruda.gitee.com/images/1673507978283805084/5978b5db_10002788.png ""0.png"")
我甚至将acl的include添加到了.bashrc永久环境变量中
![我甚至将acl的include添加到了.bashrc永久环境变量中](https://foruda.gitee.com/images/1673508035546905644/7155942c_10002788.png ""企业微信截图_16734780053188.png"")
vscode中的配置如图
![vscode中的配置如图](https://foruda.gitee.com/images/1673508093468994029/4efce94b_10002788.png ""1.png"")
vscode中的配置如图
![vscode中的配置如图](https://foruda.gitee.com/images/1673508123838949618/906a9224_10002788.png ""2.png"")
acl头文件可以自动跳到所在位置
![acl头文件可以自动跳到所在位置](https://foruda.gitee.com/images/1673508148785095703/e18d88a9_10002788.png ""3.png"")
初步分析：由上图可知，鼠标放在acl.h处，明明是可以自动跳转打开此头文件的，可是为何编译时仍旧报错找不到acl.h呢？于是我又重新写了一个简单的helloworld程序，不引入acl相关的include，结果运行正常，如下图;
![helloworld运行正常](https://foruda.gitee.com/images/1673508281780618865/cec141a1_10002788.png ""_1.png"")

软件版本：应该不是软件版本的原因，因为我换成MindStudio IDE跑上面同样的程序，编译 运行与推理均正常！

所以为什么vscode偏偏找不到acl.h呢？
","open"
"https://gitee.com/ascend/tools/issues/I67ILI","ascend","tools","[应用开发]使用一键精度对比工具对Cognitive_Planning做精度对比报错","使用一键精度对比工具对Cognitive_Planning做精度对比报错

二、软件版本:
-- CANN 版本 (6.0.RC1.alpha003):
--Tensorflow 版本:1.15
--Python 版本 (e.g., Python 3.7.5):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
在910上精度对比

python3 main.py -m /home/disk/wubo/pb_om/Cognitive_Planning.pb -om  /home/disk/wubo/pb_om/Cognitive_Planning.om -s ""taskdata:8,20,64,64,90;taskdata_1:8,20,5;taskdata_2:8,20,8""  -c /usr/local/Ascend/ascend-toolkit/latest -o /home/disk/wubo/result

四、日志信息:

2022-12-26 15:15:00(0)-[INFO]=================================compare Node_output=================================
2022-12-26 15:15:00(0)-[INFO]start to compare the Node_output at now, compare result is:
2022-12-26 15:15:00(0)-[WARNING]The comparison of Node_output may be incorrect in certain scenarios. If the precision is abnormal, please check whether the mapping between the comparison data is correct.
2022-12-26 15:15:00(0)-[INFO]Execute command:['python3.7.5', '/usr/local/Ascend/ascend-toolkit/latest/toolkit/tools/operator_cmp/compare/msaccucmp.py', 'compare', '-m', '/home/disk/wubo/result/20221226150913/dump_data/npu/20221226_15_14_32_949262/Cognitive_Planning_output_0.npy', '-g', '/home/disk/wubo/result/20221226150913/dump_data/tf/policy_Reshape_3.0.1672038619573759.npy']
b'2022-12-26 15:15:01 (60372) - [INFO] The my output dump file is /home/disk/wubo/result/20221226150913/dump_data/npu/20221226_15_14_32_949262/Cognitive_Planning_output_0.npy.'
b'2022-12-26 15:15:01 (60372) - [INFO] The ground truth file is /home/disk/wubo/result/20221226150913/dump_data/tf/policy_Reshape_3.0.1672038619573759.npy.'
b""2022-12-26 15:15:01 (60372) - [ERROR] My output shape (8, 20, 140625) in '/home/disk/wubo/result/20221226150913/dump_data/npu/20221226_15_14_32_949262/Cognitive_Planning_output_0.npy' does not match the ground truth shape (8, 20, 7) in '/home/disk/wubo/result/20221226150913/dump_data/tf/policy_Reshape_3.0.1672038619573759.npy'.""
b'2022-12-26 15:15:01 (60372) - [INFO] The command was completed and took 0 seconds.'
2022-12-26 15:15:01(0)-[ERROR]Failed to execute command: python3.7.5 /usr/local/Ascend/ascend-toolkit/latest/toolkit/tools/operator_cmp/compare/msaccucmp.py compare -m /home/disk/wubo/result/20221226150913/dump_data/npu/20221226_15_14_32_949262/Cognitive_Planning_output_0.npy -g /home/disk/wubo/result/20221226150913/dump_data/tf/policy_Reshape_3.0.1672038619573759.npy
","open"
"https://gitee.com/ascend/tools/issues/I66DL2","ascend","tools","saved_model2om工具命令样例易用性差","**【测试环境】**
    硬件：1951 主线
**【问题现象】**
1、期望结果：
  tf_server客户端推理成功
2、实际结果：
  推理失败
3、详情
  在预处理工作中, 需要使用saved_model2om工具转换原始saved_model, 该脚本中 --method_name 参数是必选参数, 而不是可选参数, 因为如果不添加, 在客户端使用时会有问题, 必须加上些参数才能正常使用, 对于不熟悉该工具的人来说, 影响工作效率
![输入图片说明](https://foruda.gitee.com/images/1671102638738630250/9d812178_2104468.png ""191005.png"")

当前命令样例:
```
python3 saved_model2om.py --input_path=/xxx/xxx/saved_model --output_path=/xxx/output/model --input_shape ""input:16,224,224,3"" --soc_version Ascend310
```","open"
"https://gitee.com/ascend/tools/issues/I643QL","ascend","tools","msame/msquickcmp缺失保存bin数据的相关指导","这两个工具可以使用-i加载bin文件作为数据样本，但未提供bin文件生成方法。
可否补充python和C++进行数据捕获的样例","open"
"https://gitee.com/ascend/tools/issues/I62B83","ascend","tools","/usr/lib/ld:cannot find -lascendcl","![输入图片说明](https://foruda.gitee.com/images/1669031043806293221/65a70bf0_4952060.jpeg ""1489008995.jpg"")
","open"
"https://gitee.com/ascend/tools/issues/I61X6Z","ascend","tools","msquick工具运行报错","一、问题现象（附报错日志上下文）：
对比onnx和om模型之间运行报错
![](https://foruda.gitee.com/images/1668758840231668756/69a70445_5034039.png ""屏幕截图"")
二、软件版本:
-- CANN 版本: 5.1.RC2  
-- MindSpore 版本: 1.9.0
--Python 版本 :Python 3.7.5
-- MindStudio版本: None
--操作系统版本: CentOS Linux release 7.6.1810

三、测试步骤：
对比minspore /model下的yolov4导出的onnx和om模型
运行msquick工具

四、日志信息:
","open"
"https://gitee.com/ascend/tools/issues/I6102C","ascend","tools","dumps结果分析","yolov5从pytorch模型转为om模型，精度损失，用精度比较工具，结果如下，好像余弦距离都比较小，不知道我接过看对了没？","open"
"https://gitee.com/ascend/tools/issues/I604MC","ascend","tools","客户需要出一个定制化容灾复位小工具，上层应用厂商应用迁移完后通知我们小工具去执行设备健康检查和复位","客户需求：卡出问题了，上层应用把业务迁移到好的机器，然后通知小工具复位昇腾设备进行恢复，恢复后的设备又可以加入资源池使用
后续处理规划：在此issue发布代码+编译指导+使用Readme供用户自取->用户到此issue内自提使用","open"
"https://gitee.com/ascend/tools/issues/I5QW98","ascend","tools","推理时不指定output可以正常推理，指定output报”Segmentation fault“错误","om 和输入数据见附件；参考命令：
python3 /opt/npu/max/ais_infer/ais_infer.py --model=./ch_PP-OCRv2_rec_bs64.om --input=./pre_data/word_1.npy --dymHW=32,320 --output=./temp

报错：
![输入图片说明](https://foruda.gitee.com/images/1663076601011778246/73581018_7898025.png ""屏幕截图"")","open"
"https://gitee.com/ascend/samples/issues/IC8OGO","ascend","samples","w-内存未释放","一、需求场景&价值
![输入图片说明](https://foruda.gitee.com/images/1747622174722867114/45f25ea3_1215735.png ""屏幕截图"")
参考链接：
https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/appdevg/aclcppdevg/aclcppdevg_000063.html

我们这边在310B上面遇到一个问题，使用dvpp V2解码的时候，不送数据解码多次重启解码器内存使用正常。送入数据正常解码，解码结果、内存都正常。重启解码器6 7次后内存会突然增加很多，关闭解码器内存也不释放。这种情况一台盒子有，一台盒子没有，操作系统是ubuntu22，cann8.0.这一块要怎么排查呢？","open"
"https://gitee.com/ascend/tools/issues/I5FZPB","ascend","tools","ais_infer推理和使用benchmark推理输出bin不一致的问题","一、问题现象（附报错日志上下文）：
使用benchmark推理输出统计的精度是达标的，改用ais_infer后推理统计的精度大幅下降。经排查，确保了2者的输入bin和om是一致的，推理出的bin是不一致的
![输入图片说明](https://images.gitee.com/uploads/images/2022/0707/153215_eb37a9f9_9508512.png ""屏幕截图.png"")
![输入图片说明](https://images.gitee.com/uploads/images/2022/0707/153222_b9b9c438_9508512.png ""屏幕截图.png"")

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
python3 ais_infer.py --model /root/xxb/inference_tinybert/tinybert/TinyBERT_bs1.om --input /root/xxb/inference_tinybert/tinybert/bert_bin/input_ids,/root/xxb/inference_tinybert/tinybert/bert_bin/segment_ids,/root/xxb/inference_tinybert/tinybert/bert_bin/input_mask  --output /root/xxb/inference_tinybert/tinybert/result --outfmt BIN

四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/tools/issues/I5FTRV","ascend","tools","[msame]op 解析bug","1.报错：
![输入图片说明](https://images.gitee.com/uploads/images/2022/0706/172647_b16b08a6_7898025.png ""屏幕截图.png"")
2.在报错位置添加打印
![输入图片说明](https://images.gitee.com/uploads/images/2022/0706/172609_488542de_7898025.png ""屏幕截图.png"")
3.打印结果
![输入图片说明](https://images.gitee.com/uploads/images/2022/0706/172716_cc0b076e_7898025.png ""屏幕截图.png"")
4.分析
_input 的名字中有 ”onnx::Slice_1870_1156:0“， 代码中”_input_out_index = int(_input.split(':')[1])“明确不对。请修复
","open"
"https://gitee.com/ascend/tools/issues/I54XL8","ascend","tools","来来啦，先把img2bin中YUV关于transpose的操作删干净","一、问题现象（附报错日志上下文）：
来来啦，先把img2bin中YUV关于transpose的操作删干净
这个和你们华为的公式没半毛钱关系了把

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/samples/issues/IC8UKK","ascend","samples","Atlas 200I a2 dk 执行.h264格式视屏解码时报错","一、问题现象（附报错日志上下文）：
Atlas 200I a2 dk 执行.h264格式视屏解码时报错
执行samples样例：[yolov3_mask_detection_video]时报错(https://gitee.com/ascend/samples/tree/master/python/level2_simple_inference/2_object_detection/YOLOV3_mask_detection_video)
![报错](https://foruda.gitee.com/images/1747643621891916228/daf66873_13683452.png ""屏幕截图"")


二、软件版本:
-- CANN 版本 (CANN 7.0.rc1):  
--Pytorch 版本:1.13.0
--Python 版本 (e.g., Python 3.9.2):
--操作系统版本 (e.g., Ubuntu 22.04.5 LTS):

三、测试步骤：
按照：https://gitee.com/ascend/samples/tree/master/python/level2_simple_inference/2_object_detection/YOLOV3_mask_detection_video。
进行执行的。求助！！！！！！！！！！！！！！！！！！！！！！！


四、日志信息:
(base) root@davinci-mini:~/samples/python/level2_simple_inference/2_object_detection/YOLOV3_m                ask_detection_video/scripts# bash samples_run.sh
[INFO] The sample starts to run
Current environment valid ip list:
        127.0.0.1
        192.168.137.102
        192.168.137.53
        192.168.0.2
Please choose one to show the presenter in browser:192.168.137.102
[INFO]  init resource stage:
[INFO]  Init resource success
[INFO]  Init model resource start...
[INFO]  AclLiteModel create model output dataset:
[INFO]  malloc output 0, size 21120
[INFO]  malloc output 1, size 84480
[INFO]  malloc output 2, size 337920
[INFO]  Create model output dataset success
[INFO]  Init model resource success
[INFO]  presenter server ip 192.168.137.102, port 7006, channel name video, type 1
[INFO] The program runs successfully, Please visit http://192.168.137.102:7007 for display se                rver!
Enter any command to stop the application:[INFO]        Received open channel respone
[INFO]  Open status is 3 now
[INFO]  presenter agent change connect_status to 3
[INFO]  Get ../data/mask.h264 infomation: width 1920, height 1080, profile 100, codec h264, e                ntype 3
[INFO]  Ready to decode ../data/mask.h264...
[INFO]  Start decode ../data/mask.h264 frames
[INFO]  Pyav decode finish
[INFO]  No decode frame in queue anymore
read None image, break
Release yolov3 resource finished
[INFO]  acl resource release all resource
[INFO]  dvpp resource release success
[INFO]  AclLiteModel release source success
[INFO]  acl resource release stream
[INFO]  acl resource release context
[INFO]  Reset acl device 0
[INFO]  Release acl resource success
[INFO]  Presenter channel close...
[INFO]  Receive presenter agent exit notification, queue size 0
[ERROR] Heard beat thread exit
","open"
"https://gitee.com/ascend/samples/issues/IC8OTN","ascend","samples","w-盒子200DK报错peripheral_api.h","连接摄像头报错：
![输入图片说明](https://foruda.gitee.com/images/1747623214100100359/88e284ec_1215735.png ""屏幕截图"")
cann版本：6.0RC3","open"
"https://gitee.com/ascend/samples/issues/IC8OJZ","ascend","samples","w-最新cann版本在哪里","![输入图片说明](https://foruda.gitee.com/images/1747622363243328011/6407e8a4_1215735.png ""屏幕截图"")只有6.0的对应配套吗，最新8.0的在哪里 
https://gitee.com/ascend/samples/blob/master/docs/CHANGELOG.md","open"
"https://gitee.com/ascend/samples/issues/I8RLGC","ascend","samples","姚宝 你的小嘴儿水多吗","一、问题现象（附报错日志上下文）：
xxxx

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/samples/issues/I6XTTI","ascend","samples","桌子：请问你是谁啊？遇到什么问题了吗？","桌子，应该是洑阳成虎的桌子吧，貌似是姚宇背着小手，穿着牛仔裤，撅着个小屁股在那里闻味道吧？","open"
"https://gitee.com/ascend/samples/issues/I6XT0S","ascend","samples","@yao_yu 300个视频在300环境上测过没有，内存能不能挤爆",,"open"
"https://gitee.com/ascend/samples/issues/I6XSMS","ascend","samples","河小敏就是明宏全","何小敏就是明宏全，我艹，你好刘操宇已经告诉我了","open"
"https://gitee.com/ascend/samples/issues/I6XS06","ascend","samples","请验证300录视频在300卡环境上运行，每个视频2小时",,"open"
"https://gitee.com/ascend/samples/issues/I6XRZZ","ascend","samples","@yao_yu测试过300路视频在300环境上跑过没有，每个视频2小时，看看内存爆不爆",,"open"
"https://gitee.com/ascend/samples/issues/I6XRWY","ascend","samples","@yao_yu测试过300路视频在300环境上跑过没有，每个视频2小时，看看内存爆不爆",,"open"
"https://gitee.com/ascend/samples/issues/I6VRPS","ascend","samples","请问，dvpp版本的等比缩放，C++版本的代码在什么地方，请标出","问题现象：

初步分析：

软件版本：

日志信息：
--提供方式 
建议打包后上传到网盘中将下载链接粘贴回复内容

复现步骤：
","open"
"https://gitee.com/ascend/samples/issues/I6VJX5","ascend","samples","compile@vb_抑郁症林新华","compile@vb_抑郁症林新华","open"
"https://gitee.com/ascend/samples/issues/I5CWFU","ascend","samples","这是你们逼我自建ISSUE的","一、问题现象（附报错日志上下文）：
猫狗比，给老子滚出来。
狗比颜亚文呢？ 你他妈的会不会处理问题？？？？？？？？
让陈雨藏 好好教教你！！！！！！！！！！！！




二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/samples/issues/I55L33","ascend","samples","我又来啦， 安锐翔 爷爷不是跟你说了嘛，让你删了img2bin中YUV转换关于Transpose的操作","| name | about	                             | labels    |
| ---- | ----------------------------------- | --------- |
| Task | Use this template for task tracking | kind/task |

## Task Description

我又来啦， 安锐翔 爷爷不是跟你说了嘛，让你删了img2bin中YUV转换关于Transpose的操作
你是眼瞎没有看到嘛，还是哪个傻汪八蛋不让你删啊？ 
","open"
"https://gitee.com/ascend/samples/issues/I55I80","ascend","samples","我说耄红潮 ，你踏马的不重构start Run， 就连img2bin中的transpose代码块都不删？臭傻逼啊","一、问题现象（附报错日志上下文）：
我说耄红潮 ，你踏马的不重构start Run， 就连img2bin中的transpose代码块都不删？臭傻逼啊

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
我说耄红潮 ，你踏马的不重构start Run， 就连img2bin中的transpose代码块都不删？臭傻逼啊


四、日志信息:
我说耄红潮 ，你踏马的不重构start Run， 就连img2bin中的transpose代码块都不删？臭傻逼啊

请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
我说耄红潮 ，你踏马的不重构start Run， 就连img2bin中的transpose代码块都不删？臭傻逼啊","open"
"https://gitee.com/ascend/samples/issues/I55C47","ascend","samples","卧槽 你们tools仓里的img2bin的transpose操作还TMD没删呢？","一、问题现象（附报错日志上下文）：
xxxx

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/samples/issues/I54XGK","ascend","samples","XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX","一、问题现象（附报错日志上下文）：
xxxx

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"
"https://gitee.com/ascend/samples/issues/I54XAE","ascend","samples","PR??????            XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX","一、问题现象（附报错日志上下文）：
xxxx

二、软件版本:
-- CANN 版本 (e.g., CANN 3.0.x，5.x.x):  
--Tensorflow/Pytorch/MindSpore 版本:
--Python 版本 (e.g., Python 3.7.5):
-- MindStudio版本 (e.g., MindStudio 2.0.0 (beta3)):
--操作系统版本 (e.g., Ubuntu 18.04):

三、测试步骤：
xxxx


四、日志信息:
xxxx
请根据自己的运行环境参考以下方式搜集日志信息，如果涉及到算子开发相关的问题，建议也提供UT/ST测试和单算子集成测试相关的日志。

日志提供方式:
将日志打包后作为附件上传。若日志大小超出附件限制，则可上传至外部网盘后提供链接。

获取方法请参考wiki：
https://gitee.com/ascend/modelzoo/wikis/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%97%A5%E5%BF%97%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE?sort_id=4097825","open"